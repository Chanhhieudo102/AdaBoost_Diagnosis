{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13767258,"sourceType":"datasetVersion","datasetId":8761788},{"sourceId":649510,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":489996,"modelId":505429},{"sourceId":685825,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":520201,"modelId":534510}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyvi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T02:24:50.408409Z","iopub.execute_input":"2025-12-16T02:24:50.408662Z","iopub.status.idle":"2025-12-16T02:24:55.852820Z","shell.execute_reply.started":"2025-12-16T02:24:50.408622Z","shell.execute_reply":"2025-12-16T02:24:55.851817Z"}},"outputs":[{"name":"stdout","text":"Collecting pyvi\n  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pyvi) (1.2.2)\nCollecting sklearn-crfsuite (from pyvi)\n  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (3.6.0)\nCollecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\nRequirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->pyvi) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->pyvi) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->pyvi) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn->pyvi) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn->pyvi) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn->pyvi) (2024.2.0)\nDownloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\nDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\nSuccessfully installed python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import math\nimport random\nfrom collections import Counter, defaultdict\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.cuda.amp import autocast, GradScaler\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# -------------------------\n# Seed for reproducibility\n# -------------------------\ndef set_seed(seed=42):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\n# ==============================\n# LOAD DATA\n# ==============================\ntrain_en = open(\"/kaggle/input/en-vi-ds/data/train.en\", \"r\", encoding=\"utf-8\").read().splitlines()\ntrain_vi = open(\"/kaggle/input/en-vi-ds/data/train.vi\", \"r\", encoding=\"utf-8\").read().splitlines()\ntest_en  = open(\"/kaggle/input/en-vi-ds/data/tst2013.en\", \"r\", encoding=\"utf-8\").read().splitlines()\ntest_vi  = open(\"/kaggle/input/en-vi-ds/data/tst2013.vi\", \"r\", encoding=\"utf-8\").read().splitlines()\n\nprint(f\"Train: {len(train_en)}, Test: {len(test_en)}\")\n\n\n# ==============================\n# OPTIMIZED BPE TOKENIZER\n# ==============================\nclass BPETokenizer:\n    def __init__(self, texts, vocab_size=8000, min_freq=2, max_samples=50000):\n        \"\"\"Optimized BPE with caching and faster merging\"\"\"\n        print(f\"Initializing BPE Tokenizer (vocab_size={vocab_size})...\")\n        self.vocab_size = vocab_size\n        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n        self.idx2word = {v: k for k, v in self.word2idx.items()}\n        self.bpe_codes = {}\n        self.cache = {}  # Cache for encoded words\n        \n        if len(texts) > max_samples:\n            print(f\"Sampling {max_samples}/{len(texts)} for BPE training\")\n            texts = random.sample(texts, max_samples)\n        \n        self.build_bpe(texts, min_freq)\n    \n    def get_stats(self, vocab):\n        \"\"\"Optimized pair counting\"\"\"\n        pairs = defaultdict(int)\n        for word, freq in vocab.items():\n            symbols = word.split()\n            for i in range(len(symbols) - 1):\n                pairs[(symbols[i], symbols[i + 1])] += freq\n        return pairs\n    \n    def merge_vocab(self, pair, vocab):\n        \"\"\"Optimized vocabulary merging\"\"\"\n        new_vocab = {}\n        bigram = ' '.join(pair)\n        replacement = ''.join(pair)\n        \n        for word, freq in vocab.items():\n            new_word = word.replace(bigram, replacement)\n            new_vocab[new_word] = freq\n        return new_vocab\n    \n    def build_bpe(self, texts, min_freq):\n        \"\"\"Build BPE vocabulary with progress tracking\"\"\"\n        print(\"Step 1: Counting word frequencies...\")\n        word_freq = Counter()\n        \n        # Batch processing for efficiency\n        batch_size = 10000\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            for line in batch:\n                word_freq.update(line.strip().lower().split())\n            if (i // batch_size) % 5 == 0:\n                print(f\"  Processed {min(i + batch_size, len(texts))}/{len(texts)} lines\")\n        \n        print(f\"Step 2: Found {len(word_freq)} unique words\")\n        \n        # Filter and prepare vocab\n        vocab = {\n            ' '.join(list(word)) + ' </w>': freq\n            for word, freq in word_freq.items()\n            if freq >= min_freq\n        }\n        \n        print(f\"Step 3: After filtering (min_freq={min_freq}): {len(vocab)} words\")\n        \n        # Learn BPE merges\n        num_merges = min(self.vocab_size - len(self.word2idx), 5000)\n        print(f\"Step 4: Learning {num_merges} BPE merges...\")\n        \n        for i in range(num_merges):\n            if i % 200 == 0:\n                print(f\"  BPE merge {i}/{num_merges}\")\n            \n            pairs = self.get_stats(vocab)\n            if not pairs:\n                break\n            \n            best = max(pairs, key=pairs.get)\n            vocab = self.merge_vocab(best, vocab)\n            self.bpe_codes[best] = i\n        \n        # Build final vocabulary\n        print(\"Step 5: Building final vocabulary...\")\n        for word in vocab.keys():\n            for token in word.split():\n                if token not in self.word2idx:\n                    idx = len(self.word2idx)\n                    self.word2idx[token] = idx\n                    self.idx2word[idx] = token\n        \n        print(f\"‚úì Vocabulary size: {len(self.word2idx)}\\n\")\n    \n    def apply_bpe(self, word):\n        \"\"\"Apply BPE with caching\"\"\"\n        if word in self.cache:\n            return self.cache[word]\n        \n        word_tokens = ' '.join(list(word)) + ' </w>'\n        \n        while True:\n            symbols = word_tokens.split()\n            if len(symbols) == 1:\n                break\n            \n            # Find best pair to merge\n            pairs = [(symbols[i], symbols[i+1]) for i in range(len(symbols)-1)]\n            valid_pairs = [(self.bpe_codes.get(p, float('inf')), i, p) \n                          for i, p in enumerate(pairs) \n                          if p in self.bpe_codes]\n            \n            if not valid_pairs:\n                break\n            \n            # Merge the earliest learned pair\n            _, pos, pair = min(valid_pairs)\n            symbols[pos] = ''.join(pair)\n            del symbols[pos + 1]\n            word_tokens = ' '.join(symbols)\n        \n        result = word_tokens.split()\n        self.cache[word] = result\n        return result\n    \n    def encode(self, text):\n        \"\"\"Encode text to token IDs\"\"\"\n        tokens = []\n        for word in text.lower().split():\n            bpe_tokens = self.apply_bpe(word)\n            tokens.extend(self.word2idx.get(token, 3) for token in bpe_tokens)\n        return tokens\n    \n    def decode(self, ids):\n        \"\"\"Decode token IDs to text\"\"\"\n        words = []\n        current_word = \"\"\n        \n        for idx in ids:\n            if idx == 2:  # eos\n                break\n            if idx > 3:\n                token = self.idx2word.get(idx, \"<unk>\")\n                if token.endswith('</w>'):\n                    current_word += token[:-4]\n                    words.append(current_word)\n                    current_word = \"\"\n                else:\n                    current_word += token\n        \n        if current_word:\n            words.append(current_word)\n        \n        return \" \".join(words)\n\n\nprint(\"Building tokenizers...\")\ntok_src = BPETokenizer(train_en, vocab_size=8000, min_freq=2, max_samples=50000)\ntok_trg = BPETokenizer(train_vi, vocab_size=8000, min_freq=2, max_samples=50000)\n\n\n# ==============================\n# DATA AUGMENTATION (OPTIMIZED)\n# ==============================\nclass DataAugmentation:\n    @staticmethod\n    def random_swap(words: List[str], n=1) -> List[str]:\n        \"\"\"Randomly swap n words\"\"\"\n        if len(words) < 2:\n            return words\n        \n        words = words.copy()\n        for _ in range(min(n, len(words) // 2)):\n            idx1, idx2 = random.sample(range(len(words)), 2)\n            words[idx1], words[idx2] = words[idx2], words[idx1]\n        return words\n    \n    @staticmethod\n    def random_deletion(words: List[str], p=0.1) -> List[str]:\n        \"\"\"Randomly delete words with probability p\"\"\"\n        if len(words) == 1:\n            return words\n        \n        new_words = [word for word in words if random.random() > p]\n        return new_words if new_words else [random.choice(words)]\n\n\n# ==============================\n# DATASET (OPTIMIZED)\n# ==============================\nclass TranslationDataset(Dataset):\n    def __init__(self, src, trg, tok_src, tok_trg, augment=False, max_len=100):\n        self.tok_src = tok_src\n        self.tok_trg = tok_trg\n        self.augment = augment\n        self.max_len = max_len\n        \n        # Pre-encode all data for faster training\n        print(\"Pre-encoding dataset...\")\n        self.data = []\n        for i, (s, t) in enumerate(zip(src, trg)):\n            if i % 10000 == 0 and i > 0:\n                print(f\"  Encoded {i}/{len(src)} samples\")\n            \n            s_tokens = [1] + tok_src.encode(s) + [2]\n            t_tokens = [1] + tok_trg.encode(t) + [2]\n            \n            # Filter out very long sequences\n            if len(s_tokens) <= max_len and len(t_tokens) <= max_len:\n                self.data.append((s_tokens, t_tokens, s.split(), t.split()))\n        \n        print(f\"‚úì Encoded {len(self.data)} samples\\n\")\n\n    def __len__(self): \n        return len(self.data)\n\n    def __getitem__(self, idx):\n        s_tokens, t_tokens, s_words, t_words = self.data[idx]\n        \n        # Apply augmentation with 20% probability\n        if self.augment and random.random() < 0.2:\n            aug_s = DataAugmentation.random_swap(s_words, n=1)\n            aug_t = DataAugmentation.random_swap(t_words, n=1)\n            \n            s_tokens = [1] + self.tok_src.encode(' '.join(aug_s)) + [2]\n            t_tokens = [1] + self.tok_trg.encode(' '.join(aug_t)) + [2]\n        \n        return torch.tensor(s_tokens, dtype=torch.long), torch.tensor(t_tokens, dtype=torch.long)\n\n\ndef collate_fn(batch):\n    \"\"\"Optimized collate with proper padding\"\"\"\n    src, trg = zip(*batch)\n    src = nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=0)\n    trg = nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=0)\n    return src, trg\n\n\n# Create datasets\ndataset = TranslationDataset(train_en, train_vi, tok_src, tok_trg, augment=True, max_len=100)\ntrain_len = int(0.95 * len(dataset))\nval_len = len(dataset) - train_len\ntrain_set, val_set = random_split(dataset, [train_len, val_len])\n\n# Disable augmentation for validation\nval_set.dataset.augment = False\n\n# Optimized batch sizes\nBATCH_SIZE = 64 if torch.cuda.is_available() else 32\n\ntrain_loader = DataLoader(\n    train_set, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    collate_fn=collate_fn,\n    num_workers=2,\n    pin_memory=True if torch.cuda.is_available() else False\n)\nval_loader = DataLoader(\n    val_set, \n    batch_size=BATCH_SIZE * 2, \n    shuffle=False, \n    collate_fn=collate_fn,\n    num_workers=2,\n    pin_memory=True if torch.cuda.is_available() else False\n)\n\nprint(f\"Train: {train_len}, Val: {val_len}\")\nprint(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\\n\")\n\n\n# ==============================\n# OPTIMIZED POSITIONAL ENCODING\n# ==============================\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\n\n# ==============================\n# LABEL SMOOTHING LOSS\n# ==============================\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, num_classes, smoothing=0.1, ignore_index=0):\n        super().__init__()\n        self.smoothing = smoothing\n        self.num_classes = num_classes\n        self.ignore_index = ignore_index\n        self.confidence = 1.0 - smoothing\n    \n    def forward(self, pred, target):\n        \"\"\"\n        pred: (batch_size * seq_len, num_classes)\n        target: (batch_size * seq_len)\n        \"\"\"\n        pred = pred.log_softmax(dim=-1)\n        \n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.num_classes - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n            \n            # Mask padding\n            mask = target == self.ignore_index\n            true_dist[mask] = 0\n        \n        return torch.mean(torch.sum(-true_dist * pred, dim=-1))\n\n\n# ==============================\n# IMPROVED TRANSFORMER MODEL\n# ==============================\nclass TransformerModel(nn.Module):\n    def __init__(self, src_vocab, trg_vocab, d_model=512, nhead=8, \n                 num_layers=6, dim_feedforward=2048, dropout=0.1, pad_idx=0):\n        super().__init__()\n        self.pad_idx = pad_idx\n        self.d_model = d_model\n\n        # Embeddings with scaling\n        self.src_emb = nn.Embedding(src_vocab, d_model, padding_idx=pad_idx)\n        self.trg_emb = nn.Embedding(trg_vocab, d_model, padding_idx=pad_idx)\n        self.pos = PositionalEncoding(d_model, dropout=dropout)\n        \n        # Scale factor for embeddings\n        self.scale = math.sqrt(d_model)\n\n        # Transformer\n        self.transformer = nn.Transformer(\n            d_model=d_model, \n            nhead=nhead,\n            num_encoder_layers=num_layers,\n            num_decoder_layers=num_layers,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True,\n            norm_first=True  # Pre-LN for better training stability\n        )\n\n        # Output projection with weight tying\n        self.fc = nn.Linear(d_model, trg_vocab)\n        self.fc.weight = self.trg_emb.weight\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n        \"\"\"Xavier initialization for better convergence\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, src, trg):\n        # Create masks\n        src_mask = (src == self.pad_idx)\n        trg_mask = (trg == self.pad_idx)\n        seq_len = trg.size(1)\n        \n        # Causal mask for decoder\n        causal_mask = nn.Transformer.generate_square_subsequent_mask(\n            seq_len, device=trg.device\n        )\n\n        # Embeddings with scaling + positional encoding\n        src_emb = self.pos(self.src_emb(src) * self.scale)\n        trg_emb = self.pos(self.trg_emb(trg) * self.scale)\n\n        # Transformer forward\n        out = self.transformer(\n            src_emb, trg_emb,\n            tgt_mask=causal_mask,\n            src_key_padding_mask=src_mask,\n            tgt_key_padding_mask=trg_mask,\n            memory_key_padding_mask=src_mask\n        )\n        \n        return self.fc(out)\n\n\n# ==============================\n# EARLY STOPPING\n# ==============================\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=0.0005, mode='min'):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.mode = mode\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        \n    def __call__(self, val_metric):\n        score = -val_metric if self.mode == 'min' else val_metric\n        \n        if self.best_score is None:\n            self.best_score = score\n        elif score < self.best_score + self.min_delta:\n            self.counter += 1\n            print(f\"     ‚ö† EarlyStopping: {self.counter}/{self.patience}\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.counter = 0\n\n\n# ==============================\n# OPTIMIZED TRAINING WITH MIXED PRECISION\n# ==============================\ndef train_model(model, train_loader, val_loader, device, epochs=25, lr=5e-4, \n                patience=7, warmup_epochs=2, use_amp=True):\n    model.to(device)\n    print(f\"Device: {device}\")\n    print(f\"Mixed Precision: {use_amp and torch.cuda.is_available()}\")\n    \n    # Optimizer with weight decay\n    opt = torch.optim.AdamW(\n        model.parameters(), \n        lr=lr, \n        betas=(0.9, 0.98), \n        eps=1e-9,\n        weight_decay=0.0001\n    )\n    \n    # Learning rate scheduler with warmup\n    warmup_steps = warmup_epochs * len(train_loader)\n    total_steps = epochs * len(train_loader)\n    \n    def lr_lambda(step):\n        if step < warmup_steps:\n            return float(step) / float(max(1, warmup_steps))\n        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n        return max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n    \n    scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n    \n    # Loss function\n    loss_fn = LabelSmoothingLoss(\n        num_classes=len(tok_trg.word2idx), \n        smoothing=0.1, \n        ignore_index=0\n    )\n    \n    # Mixed precision scaler\n    scaler = GradScaler() if use_amp and torch.cuda.is_available() else None\n    \n    # Early stopping\n    early_stopping = EarlyStopping(patience=patience, min_delta=0.0005)\n    \n    print(f\"\\nTraining Config:\")\n    print(f\"  Epochs: {epochs}\")\n    print(f\"  Learning Rate: {lr}\")\n    print(f\"  Warmup Epochs: {warmup_epochs}\")\n    print(f\"  Batch Size: {BATCH_SIZE}\")\n    print(f\"  Early Stopping Patience: {patience}\")\n    print()\n\n    best_val = float('inf')\n    train_losses, val_losses = [], []\n\n    for ep in range(1, epochs + 1):\n        print(f\"{'='*60}\")\n        print(f\"Epoch {ep}/{epochs}\")\n        print(f\"{'='*60}\")\n        \n        # Training\n        model.train()\n        total_loss = 0\n        \n        for batch_idx, (src, trg) in enumerate(train_loader):\n            src, trg = src.to(device), trg.to(device)\n            opt.zero_grad()\n            \n            # Mixed precision training\n            if scaler:\n                with autocast():\n                    out = model(src, trg[:, :-1])\n                    loss = loss_fn(\n                        out.reshape(-1, out.size(-1)), \n                        trg[:, 1:].reshape(-1)\n                    )\n                \n                scaler.scale(loss).backward()\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(opt)\n                scaler.update()\n            else:\n                out = model(src, trg[:, :-1])\n                loss = loss_fn(\n                    out.reshape(-1, out.size(-1)), \n                    trg[:, 1:].reshape(-1)\n                )\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                opt.step()\n            \n            scheduler.step()\n            total_loss += loss.item()\n            \n            # Progress update\n            if (batch_idx + 1) % 100 == 0:\n                avg_loss = total_loss / (batch_idx + 1)\n                lr_current = opt.param_groups[0]['lr']\n                print(f\"  Batch {batch_idx+1}/{len(train_loader)} | \"\n                      f\"Loss: {avg_loss:.4f} | LR: {lr_current:.6f}\")\n\n        # Validation\n        print(f\"\\n  Validating...\")\n        model.eval()\n        val_loss = 0\n        \n        with torch.no_grad():\n            for src, trg in val_loader:\n                src, trg = src.to(device), trg.to(device)\n                \n                if scaler:\n                    with autocast():\n                        out = model(src, trg[:, :-1])\n                        loss = loss_fn(\n                            out.reshape(-1, out.size(-1)), \n                            trg[:, 1:].reshape(-1)\n                        )\n                else:\n                    out = model(src, trg[:, :-1])\n                    loss = loss_fn(\n                        out.reshape(-1, out.size(-1)), \n                        trg[:, 1:].reshape(-1)\n                    )\n                \n                val_loss += loss.item()\n\n        # Epoch summary\n        avg_train = total_loss / len(train_loader)\n        avg_val = val_loss / len(val_loader)\n        lr_current = opt.param_groups[0]['lr']\n        \n        train_losses.append(avg_train)\n        val_losses.append(avg_val)\n\n        print(f\"\\n  üìä Summary:\")\n        print(f\"     Train Loss: {avg_train:.4f}\")\n        print(f\"     Val Loss:   {avg_val:.4f}\")\n        print(f\"     LR:         {lr_current:.6f}\")\n\n        # Save best model\n        if avg_val < best_val:\n            best_val = avg_val\n            torch.save({\n                'epoch': ep,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': opt.state_dict(),\n                'val_loss': best_val,\n            }, \"best_model.pt\")\n            print(f\"     ‚úÖ Best model saved! (Val Loss: {best_val:.4f})\")\n        \n        # Early stopping\n        early_stopping(avg_val)\n        if early_stopping.early_stop:\n            print(f\"\\nüõë Early stopping at epoch {ep}\")\n            print(f\"   Best Val Loss: {best_val:.4f}\")\n            break\n        \n        print()\n    \n    return train_losses, val_losses\n\n\n# ==============================\n# BEAM SEARCH DECODING\n# ==============================\ndef beam_search_decode(model, src, tok_trg, device, beam_size=5, max_len=80):\n    model.eval()\n    sos, eos = 1, 2\n    \n    src = src.to(device)\n    \n    # Encode source\n    with torch.no_grad():\n        src_mask = (src == 0)\n        src_emb = model.pos(model.src_emb(src) * model.scale)\n        memory = model.transformer.encoder(src_emb, src_key_padding_mask=src_mask)\n    \n    # Initialize beam\n    sequences = [[sos]]\n    scores = [0.0]\n    \n    for _ in range(max_len):\n        all_candidates = []\n        \n        for seq, score in zip(sequences, scores):\n            if seq[-1] == eos:\n                all_candidates.append((seq, score))\n                continue\n            \n            trg = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n            tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n                trg.size(1), device=device\n            )\n            \n            with torch.no_grad():\n                trg_emb = model.pos(model.trg_emb(trg) * model.scale)\n                out = model.transformer.decoder(\n                    trg_emb, memory,\n                    tgt_mask=tgt_mask,\n                    memory_key_padding_mask=src_mask\n                )\n                logits = model.fc(out[:, -1])\n                log_probs = F.log_softmax(logits, dim=-1)\n            \n            # Get top-k candidates\n            topk_probs, topk_ids = torch.topk(log_probs, beam_size)\n            \n            for k in range(beam_size):\n                candidate_seq = seq + [topk_ids[0, k].item()]\n                candidate_score = score + topk_probs[0, k].item()\n                all_candidates.append((candidate_seq, candidate_score))\n        \n        # Select top beam_size candidates\n        ordered = sorted(all_candidates, key=lambda x: x[1] / len(x[0]), reverse=True)\n        sequences = [seq for seq, _ in ordered[:beam_size]]\n        scores = [score for _, score in ordered[:beam_size]]\n        \n        # Early stopping if all beams end with EOS\n        if all(seq[-1] == eos for seq in sequences):\n            break\n    \n    best_seq = sequences[0]\n    return tok_trg.decode(best_seq[1:])\n\n\n# ==============================\n# TRANSLATION FUNCTION\n# ==============================\ndef translate(model, text, tok_src, tok_trg, device, beam_size=5):\n    src = [1] + tok_src.encode(text) + [2]\n    src = torch.tensor(src, dtype=torch.long).unsqueeze(0)\n    return beam_search_decode(model, src, tok_trg, device, beam_size=beam_size)\n\n\n# ==============================\n# EVALUATION\n# ==============================\ndef evaluate_test_set(model, test_en, test_vi, tok_src, tok_trg, device, n=100):\n    model.eval()\n    smooth = SmoothingFunction().method1\n    \n    total_bleu = 0\n    n = min(n, len(test_en))\n    \n    print(f\"Evaluating {n} test samples...\")\n    \n    for i in range(n):\n        pred = translate(model, test_en[i], tok_src, tok_trg, device, beam_size=5)\n        bleu = sentence_bleu(\n            [test_vi[i].split()], \n            pred.split(), \n            smoothing_function=smooth\n        )\n        total_bleu += bleu\n        \n        # Show first 5 examples\n        if i < 5:\n            print(f\"\\n--- Example {i+1} ---\")\n            print(f\"EN: {test_en[i]}\")\n            print(f\"GT: {test_vi[i]}\")\n            print(f\"PR: {pred}\")\n            print(f\"BLEU: {bleu:.4f}\")\n    \n    avg_bleu = total_bleu / n\n    print(f\"\\n{'='*60}\")\n    print(f\"AVERAGE BLEU SCORE: {avg_bleu:.4f}\")\n    print(f\"{'='*60}\")\n    \n    return avg_bleu\n\n\n# ==============================\n# MAIN EXECUTION\n# ==============================\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"\\n{'='*60}\")\n    print(f\"DEVICE: {device}\")\n    print(f\"{'='*60}\\n\")\n    \n    # Initialize model\n    model = TransformerModel(\n        src_vocab=len(tok_src.word2idx),\n        trg_vocab=len(tok_trg.word2idx),\n        d_model=512,\n        nhead=8,\n        num_layers=6,\n        dim_feedforward=2048,\n        dropout=0.1,\n        pad_idx=0\n    )\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Model Parameters: {total_params:,}\")\n    print(f\"Trainable Parameters: {trainable_params:,}\\n\")\n    \n    # Train\n    print(\"=\"*60)\n    print(\"TRAINING\")\n    print(\"=\"*60)\n    train_losses, val_losses = train_model(\n        model, train_loader, val_loader, device,\n        epochs=25,\n        lr=5e-4,\n        patience=7,\n        warmup_epochs=2,\n        use_amp=True\n    )\n    \n    # Load best model\n    print(\"\\n\" + \"=\"*60)\n    print(\"LOADING BEST MODEL\")\n    print(\"=\"*60)\n    checkpoint = torch.load(\"best_model.pt\")\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\"‚úì Loaded model from epoch {checkpoint['epoch']}\")\n    print(f\"  Best Val Loss: {checkpoint['val_loss']:.4f}\\n\")\n    \n    # Evaluate on test set\n    print(\"=\"*60)\n    print(\"EVALUATION ON TEST SET\")\n    print(\"=\"*60)\n    avg_bleu = evaluate_test_set(model, test_en, test_vi, tok_src, tok_trg, device, n=100)\n    \n    # Interactive translation\n    print(\"\\n\" + \"=\"*60)\n    print(\"INTERACTIVE TRANSLATION\")\n    print(\"=\"*60)\n    print(\"Enter English sentences to translate (or 'quit' to exit):\\n\")\n    \n    while True:\n        try:\n            text = input(\"EN: \").strip()\n            if text.lower() in ['quit', 'exit', 'q']:\n                print(\"Goodbye!\")\n                break\n            if not text:\n                continue\n                \n            translation = translate(model, text, tok_src, tok_trg, device, beam_size=5)\n            print(f\"VI: {translation}\\n\")\n            \n        except KeyboardInterrupt:\n            print(\"\\nGoodbye!\")\n            break\n        except Exception as e:\n            print(f\"Error: {e}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T02:24:55.857629Z","iopub.execute_input":"2025-12-16T02:24:55.857870Z"}},"outputs":[{"name":"stdout","text":"Train: 133317, Test: 1268\nBuilding tokenizers...\nInitializing BPE Tokenizer (vocab_size=8000)...\nSampling 50000/133317 for BPE training\nStep 1: Counting word frequencies...\n  Processed 10000/50000 lines\nStep 2: Found 31792 unique words\nStep 3: After filtering (min_freq=2): 17929 words\nStep 4: Learning 5000 BPE merges...\n  BPE merge 0/5000\n  BPE merge 200/5000\n  BPE merge 400/5000\n  BPE merge 600/5000\n  BPE merge 800/5000\n  BPE merge 1000/5000\n  BPE merge 1200/5000\n  BPE merge 1400/5000\n  BPE merge 1600/5000\n  BPE merge 1800/5000\n  BPE merge 2000/5000\n  BPE merge 2200/5000\n  BPE merge 2400/5000\n  BPE merge 2600/5000\n  BPE merge 2800/5000\n  BPE merge 3000/5000\n  BPE merge 3200/5000\n  BPE merge 3400/5000\n  BPE merge 3600/5000\n  BPE merge 3800/5000\n  BPE merge 4000/5000\n  BPE merge 4200/5000\n  BPE merge 4400/5000\n  BPE merge 4600/5000\n  BPE merge 4800/5000\nStep 5: Building final vocabulary...\n‚úì Vocabulary size: 15346\n\nInitializing BPE Tokenizer (vocab_size=8000)...\nSampling 50000/133317 for BPE training\nStep 1: Counting word frequencies...\n  Processed 10000/50000 lines\nStep 2: Found 14280 unique words\nStep 3: After filtering (min_freq=2): 7543 words\nStep 4: Learning 5000 BPE merges...\n  BPE merge 0/5000\n  BPE merge 200/5000\n  BPE merge 400/5000\n  BPE merge 600/5000\n  BPE merge 800/5000\n  BPE merge 1000/5000\n  BPE merge 1200/5000\n  BPE merge 1400/5000\n  BPE merge 1600/5000\n  BPE merge 1800/5000\n  BPE merge 2000/5000\n  BPE merge 2200/5000\n  BPE merge 2400/5000\n  BPE merge 2600/5000\n  BPE merge 2800/5000\n  BPE merge 3000/5000\n  BPE merge 3200/5000\n  BPE merge 3400/5000\n  BPE merge 3600/5000\n  BPE merge 3800/5000\n  BPE merge 4000/5000\n  BPE merge 4200/5000\n  BPE merge 4400/5000\n  BPE merge 4600/5000\n  BPE merge 4800/5000\nStep 5: Building final vocabulary...\n‚úì Vocabulary size: 7038\n\nPre-encoding dataset...\n  Encoded 10000/133317 samples\n  Encoded 20000/133317 samples\n  Encoded 30000/133317 samples\n  Encoded 40000/133317 samples\n  Encoded 50000/133317 samples\n  Encoded 60000/133317 samples\n  Encoded 70000/133317 samples\n  Encoded 80000/133317 samples\n  Encoded 90000/133317 samples\n  Encoded 100000/133317 samples\n  Encoded 110000/133317 samples\n  Encoded 120000/133317 samples\n  Encoded 130000/133317 samples\n‚úì Encoded 128090 samples\n\nTrain: 121685, Val: 6405\nTrain batches: 1902, Val batches: 51\n\n\n============================================================\nDEVICE: cuda\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model Parameters: 55,608,190\nTrainable Parameters: 55,608,190\n\n============================================================\nTRAINING\n============================================================\nDevice: cuda\nMixed Precision: True\n\nTraining Config:\n  Epochs: 25\n  Learning Rate: 0.0005\n  Warmup Epochs: 2\n  Batch Size: 64\n  Early Stopping Patience: 7\n\n============================================================\nEpoch 1/25\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/4120245670.py:492: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler() if use_amp and torch.cuda.is_available() else None\n/tmp/ipykernel_47/4120245670.py:523: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Batch 100/1902 | Loss: 3.1693 | LR: 0.000013\n  Batch 200/1902 | Loss: 3.0361 | LR: 0.000026\n  Batch 300/1902 | Loss: 2.8676 | LR: 0.000039\n  Batch 400/1902 | Loss: 2.7547 | LR: 0.000053\n  Batch 500/1902 | Loss: 2.6666 | LR: 0.000066\n  Batch 600/1902 | Loss: 2.5985 | LR: 0.000079\n  Batch 700/1902 | Loss: 2.5338 | LR: 0.000092\n  Batch 800/1902 | Loss: 2.4754 | LR: 0.000105\n  Batch 900/1902 | Loss: 2.4252 | LR: 0.000118\n  Batch 1000/1902 | Loss: 2.3835 | LR: 0.000131\n  Batch 1100/1902 | Loss: 2.3398 | LR: 0.000145\n  Batch 1200/1902 | Loss: 2.3018 | LR: 0.000158\n  Batch 1300/1902 | Loss: 2.2672 | LR: 0.000171\n  Batch 1400/1902 | Loss: 2.2384 | LR: 0.000184\n  Batch 1500/1902 | Loss: 2.2083 | LR: 0.000197\n  Batch 1600/1902 | Loss: 2.1796 | LR: 0.000210\n  Batch 1700/1902 | Loss: 2.1534 | LR: 0.000223\n  Batch 1800/1902 | Loss: 2.1303 | LR: 0.000237\n  Batch 1900/1902 | Loss: 2.1074 | LR: 0.000250\n\n  Validating...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/4120245670.py:565: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"\n  üìä Summary:\n     Train Loss: 2.1072\n     Val Loss:   1.5934\n     LR:         0.000250\n     ‚úÖ Best model saved! (Val Loss: 1.5934)\n\n============================================================\nEpoch 2/25\n============================================================\n  Batch 100/1902 | Loss: 1.6319 | LR: 0.000263\n  Batch 200/1902 | Loss: 1.6299 | LR: 0.000276\n  Batch 300/1902 | Loss: 1.6142 | LR: 0.000289\n  Batch 400/1902 | Loss: 1.6177 | LR: 0.000303\n  Batch 500/1902 | Loss: 1.6133 | LR: 0.000316\n  Batch 600/1902 | Loss: 1.6123 | LR: 0.000329\n  Batch 700/1902 | Loss: 1.6010 | LR: 0.000342\n  Batch 800/1902 | Loss: 1.5991 | LR: 0.000355\n  Batch 900/1902 | Loss: 1.5927 | LR: 0.000368\n  Batch 1000/1902 | Loss: 1.5907 | LR: 0.000381\n  Batch 1100/1902 | Loss: 1.5836 | LR: 0.000395\n  Batch 1200/1902 | Loss: 1.5746 | LR: 0.000408\n  Batch 1300/1902 | Loss: 1.5675 | LR: 0.000421\n  Batch 1400/1902 | Loss: 1.5604 | LR: 0.000434\n  Batch 1500/1902 | Loss: 1.5551 | LR: 0.000447\n  Batch 1600/1902 | Loss: 1.5494 | LR: 0.000460\n  Batch 1700/1902 | Loss: 1.5430 | LR: 0.000473\n  Batch 1800/1902 | Loss: 1.5379 | LR: 0.000487\n  Batch 1900/1902 | Loss: 1.5323 | LR: 0.000500\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 1.5320\n     Val Loss:   1.3361\n     LR:         0.000500\n     ‚úÖ Best model saved! (Val Loss: 1.3361)\n\n============================================================\nEpoch 3/25\n============================================================\n  Batch 100/1902 | Loss: 1.3888 | LR: 0.000500\n  Batch 200/1902 | Loss: 1.3854 | LR: 0.000500\n  Batch 300/1902 | Loss: 1.3882 | LR: 0.000500\n  Batch 400/1902 | Loss: 1.3837 | LR: 0.000500\n  Batch 500/1902 | Loss: 1.3833 | LR: 0.000500\n  Batch 600/1902 | Loss: 1.3798 | LR: 0.000500\n  Batch 700/1902 | Loss: 1.3770 | LR: 0.000500\n  Batch 800/1902 | Loss: 1.3730 | LR: 0.000500\n  Batch 900/1902 | Loss: 1.3685 | LR: 0.000499\n  Batch 1000/1902 | Loss: 1.3650 | LR: 0.000499\n  Batch 1100/1902 | Loss: 1.3641 | LR: 0.000499\n  Batch 1200/1902 | Loss: 1.3636 | LR: 0.000499\n  Batch 1300/1902 | Loss: 1.3594 | LR: 0.000499\n  Batch 1400/1902 | Loss: 1.3579 | LR: 0.000499\n  Batch 1500/1902 | Loss: 1.3558 | LR: 0.000499\n  Batch 1600/1902 | Loss: 1.3508 | LR: 0.000498\n  Batch 1700/1902 | Loss: 1.3482 | LR: 0.000498\n  Batch 1800/1902 | Loss: 1.3445 | LR: 0.000498\n  Batch 1900/1902 | Loss: 1.3422 | LR: 0.000498\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 1.3422\n     Val Loss:   1.2086\n     LR:         0.000498\n     ‚úÖ Best model saved! (Val Loss: 1.2086)\n\n============================================================\nEpoch 4/25\n============================================================\n  Batch 100/1902 | Loss: 1.2563 | LR: 0.000497\n  Batch 200/1902 | Loss: 1.2677 | LR: 0.000497\n  Batch 300/1902 | Loss: 1.2669 | LR: 0.000497\n  Batch 400/1902 | Loss: 1.2538 | LR: 0.000497\n  Batch 500/1902 | Loss: 1.2544 | LR: 0.000496\n  Batch 600/1902 | Loss: 1.2517 | LR: 0.000496\n  Batch 700/1902 | Loss: 1.2474 | LR: 0.000496\n  Batch 800/1902 | Loss: 1.2446 | LR: 0.000495\n  Batch 900/1902 | Loss: 1.2467 | LR: 0.000495\n  Batch 1000/1902 | Loss: 1.2461 | LR: 0.000495\n  Batch 1100/1902 | Loss: 1.2462 | LR: 0.000494\n  Batch 1200/1902 | Loss: 1.2438 | LR: 0.000494\n  Batch 1300/1902 | Loss: 1.2424 | LR: 0.000493\n  Batch 1400/1902 | Loss: 1.2395 | LR: 0.000493\n  Batch 1500/1902 | Loss: 1.2379 | LR: 0.000493\n  Batch 1600/1902 | Loss: 1.2366 | LR: 0.000492\n  Batch 1700/1902 | Loss: 1.2360 | LR: 0.000492\n  Batch 1800/1902 | Loss: 1.2334 | LR: 0.000491\n  Batch 1900/1902 | Loss: 1.2339 | LR: 0.000491\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 1.2337\n     Val Loss:   1.1417\n     LR:         0.000491\n     ‚úÖ Best model saved! (Val Loss: 1.1417)\n\n============================================================\nEpoch 5/25\n============================================================\n  Batch 100/1902 | Loss: 1.1541 | LR: 0.000490\n  Batch 200/1902 | Loss: 1.1511 | LR: 0.000490\n  Batch 300/1902 | Loss: 1.1584 | LR: 0.000489\n  Batch 400/1902 | Loss: 1.1603 | LR: 0.000489\n  Batch 500/1902 | Loss: 1.1597 | LR: 0.000488\n  Batch 600/1902 | Loss: 1.1587 | LR: 0.000488\n  Batch 700/1902 | Loss: 1.1608 | LR: 0.000487\n  Batch 800/1902 | Loss: 1.1576 | LR: 0.000486\n  Batch 900/1902 | Loss: 1.1594 | LR: 0.000486\n  Batch 1000/1902 | Loss: 1.1588 | LR: 0.000485\n  Batch 1100/1902 | Loss: 1.1585 | LR: 0.000485\n  Batch 1200/1902 | Loss: 1.1587 | LR: 0.000484\n  Batch 1300/1902 | Loss: 1.1574 | LR: 0.000483\n  Batch 1400/1902 | Loss: 1.1569 | LR: 0.000483\n  Batch 1500/1902 | Loss: 1.1578 | LR: 0.000482\n  Batch 1600/1902 | Loss: 1.1564 | LR: 0.000481\n  Batch 1700/1902 | Loss: 1.1580 | LR: 0.000481\n  Batch 1800/1902 | Loss: 1.1572 | LR: 0.000480\n  Batch 1900/1902 | Loss: 1.1571 | LR: 0.000479\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 1.1571\n     Val Loss:   1.0992\n     LR:         0.000479\n     ‚úÖ Best model saved! (Val Loss: 1.0992)\n\n============================================================\nEpoch 6/25\n============================================================\n  Batch 100/1902 | Loss: 1.0990 | LR: 0.000479\n  Batch 200/1902 | Loss: 1.1083 | LR: 0.000478\n  Batch 300/1902 | Loss: 1.1125 | LR: 0.000477\n  Batch 400/1902 | Loss: 1.1125 | LR: 0.000476\n  Batch 500/1902 | Loss: 1.1150 | LR: 0.000476\n  Batch 600/1902 | Loss: 1.1127 | LR: 0.000475\n  Batch 700/1902 | Loss: 1.1130 | LR: 0.000474\n  Batch 800/1902 | Loss: 1.1123 | LR: 0.000473\n  Batch 900/1902 | Loss: 1.1113 | LR: 0.000472\n  Batch 1000/1902 | Loss: 1.1110 | LR: 0.000472\n  Batch 1100/1902 | Loss: 1.1111 | LR: 0.000471\n  Batch 1200/1902 | Loss: 1.1112 | LR: 0.000470\n  Batch 1300/1902 | Loss: 1.1095 | LR: 0.000469\n  Batch 1400/1902 | Loss: 1.1097 | LR: 0.000468\n  Batch 1500/1902 | Loss: 1.1097 | LR: 0.000467\n  Batch 1600/1902 | Loss: 1.1088 | LR: 0.000466\n  Batch 1700/1902 | Loss: 1.1079 | LR: 0.000465\n  Batch 1800/1902 | Loss: 1.1071 | LR: 0.000465\n  Batch 1900/1902 | Loss: 1.1065 | LR: 0.000464\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 1.1069\n     Val Loss:   1.0723\n     LR:         0.000464\n     ‚úÖ Best model saved! (Val Loss: 1.0723)\n\n============================================================\nEpoch 7/25\n============================================================\n  Batch 100/1902 | Loss: 1.0656 | LR: 0.000463\n  Batch 200/1902 | Loss: 1.0579 | LR: 0.000462\n  Batch 300/1902 | Loss: 1.0520 | LR: 0.000461\n  Batch 400/1902 | Loss: 1.0512 | LR: 0.000460\n  Batch 500/1902 | Loss: 1.0568 | LR: 0.000459\n  Batch 600/1902 | Loss: 1.0614 | LR: 0.000458\n  Batch 700/1902 | Loss: 1.0610 | LR: 0.000457\n  Batch 800/1902 | Loss: 1.0590 | LR: 0.000456\n  Batch 900/1902 | Loss: 1.0599 | LR: 0.000455\n  Batch 1000/1902 | Loss: 1.0606 | LR: 0.000454\n  Batch 1100/1902 | Loss: 1.0609 | LR: 0.000453\n  Batch 1200/1902 | Loss: 1.0609 | LR: 0.000452\n  Batch 1300/1902 | Loss: 1.0627 | LR: 0.000451\n  Batch 1400/1902 | Loss: 1.0625 | LR: 0.000449\n  Batch 1500/1902 | Loss: 1.0628 | LR: 0.000448\n  Batch 1600/1902 | Loss: 1.0636 | LR: 0.000447\n  Batch 1700/1902 | Loss: 1.0636 | LR: 0.000446\n  Batch 1800/1902 | Loss: 1.0641 | LR: 0.000445\n  Batch 1900/1902 | Loss: 1.0658 | LR: 0.000444\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 1.0661\n     Val Loss:   1.0531\n     LR:         0.000444\n     ‚úÖ Best model saved! (Val Loss: 1.0531)\n\n============================================================\nEpoch 8/25\n============================================================\n  Batch 100/1902 | Loss: 1.0127 | LR: 0.000443\n  Batch 200/1902 | Loss: 1.0068 | LR: 0.000442\n  Batch 300/1902 | Loss: 1.0074 | LR: 0.000440\n  Batch 400/1902 | Loss: 1.0119 | LR: 0.000439\n  Batch 500/1902 | Loss: 1.0177 | LR: 0.000438\n  Batch 600/1902 | Loss: 1.0210 | LR: 0.000437\n  Batch 700/1902 | Loss: 1.0210 | LR: 0.000436\n  Batch 800/1902 | Loss: 1.0224 | LR: 0.000435\n  Batch 900/1902 | Loss: 1.0229 | LR: 0.000433\n  Batch 1000/1902 | Loss: 1.0238 | LR: 0.000432\n  Batch 1100/1902 | Loss: 1.0262 | LR: 0.000431\n  Batch 1200/1902 | Loss: 1.0265 | LR: 0.000430\n  Batch 1300/1902 | Loss: 1.0266 | LR: 0.000428\n  Batch 1400/1902 | Loss: 1.0270 | LR: 0.000427\n  Batch 1500/1902 | Loss: 1.0272 | LR: 0.000426\n  Batch 1600/1902 | Loss: 1.0288 | LR: 0.000425\n  Batch 1700/1902 | Loss: 1.0296 | LR: 0.000423\n  Batch 1800/1902 | Loss: 1.0296 | LR: 0.000422\n  Batch 1900/1902 | Loss: 1.0299 | LR: 0.000421\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 1.0300\n     Val Loss:   1.0387\n     LR:         0.000421\n     ‚úÖ Best model saved! (Val Loss: 1.0387)\n\n============================================================\nEpoch 9/25\n============================================================\n  Batch 100/1902 | Loss: 0.9771 | LR: 0.000419\n  Batch 200/1902 | Loss: 0.9783 | LR: 0.000418\n  Batch 300/1902 | Loss: 0.9770 | LR: 0.000417\n  Batch 400/1902 | Loss: 0.9812 | LR: 0.000415\n  Batch 500/1902 | Loss: 0.9812 | LR: 0.000414\n  Batch 600/1902 | Loss: 0.9838 | LR: 0.000413\n  Batch 700/1902 | Loss: 0.9828 | LR: 0.000411\n  Batch 800/1902 | Loss: 0.9856 | LR: 0.000410\n  Batch 900/1902 | Loss: 0.9880 | LR: 0.000408\n  Batch 1000/1902 | Loss: 0.9882 | LR: 0.000407\n  Batch 1100/1902 | Loss: 0.9913 | LR: 0.000406\n  Batch 1200/1902 | Loss: 0.9909 | LR: 0.000404\n  Batch 1300/1902 | Loss: 0.9919 | LR: 0.000403\n  Batch 1400/1902 | Loss: 0.9922 | LR: 0.000401\n  Batch 1500/1902 | Loss: 0.9932 | LR: 0.000400\n  Batch 1600/1902 | Loss: 0.9946 | LR: 0.000399\n  Batch 1700/1902 | Loss: 0.9957 | LR: 0.000397\n  Batch 1800/1902 | Loss: 0.9960 | LR: 0.000396\n  Batch 1900/1902 | Loss: 0.9972 | LR: 0.000394\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 0.9973\n     Val Loss:   1.0279\n     LR:         0.000394\n     ‚úÖ Best model saved! (Val Loss: 1.0279)\n\n============================================================\nEpoch 10/25\n============================================================\n  Batch 100/1902 | Loss: 0.9340 | LR: 0.000393\n  Batch 200/1902 | Loss: 0.9476 | LR: 0.000391\n  Batch 300/1902 | Loss: 0.9456 | LR: 0.000390\n  Batch 400/1902 | Loss: 0.9475 | LR: 0.000388\n  Batch 500/1902 | Loss: 0.9483 | LR: 0.000387\n  Batch 600/1902 | Loss: 0.9513 | LR: 0.000385\n  Batch 700/1902 | Loss: 0.9522 | LR: 0.000384\n  Batch 800/1902 | Loss: 0.9538 | LR: 0.000382\n  Batch 900/1902 | Loss: 0.9564 | LR: 0.000381\n  Batch 1000/1902 | Loss: 0.9594 | LR: 0.000379\n  Batch 1100/1902 | Loss: 0.9589 | LR: 0.000378\n  Batch 1200/1902 | Loss: 0.9607 | LR: 0.000376\n  Batch 1300/1902 | Loss: 0.9611 | LR: 0.000375\n  Batch 1400/1902 | Loss: 0.9613 | LR: 0.000373\n  Batch 1500/1902 | Loss: 0.9625 | LR: 0.000371\n  Batch 1600/1902 | Loss: 0.9645 | LR: 0.000370\n  Batch 1700/1902 | Loss: 0.9663 | LR: 0.000368\n  Batch 1800/1902 | Loss: 0.9675 | LR: 0.000367\n  Batch 1900/1902 | Loss: 0.9683 | LR: 0.000365\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 0.9684\n     Val Loss:   1.0234\n     LR:         0.000365\n     ‚úÖ Best model saved! (Val Loss: 1.0234)\n\n============================================================\nEpoch 11/25\n============================================================\n  Batch 100/1902 | Loss: 0.9162 | LR: 0.000363\n  Batch 200/1902 | Loss: 0.9174 | LR: 0.000362\n  Batch 300/1902 | Loss: 0.9203 | LR: 0.000360\n  Batch 400/1902 | Loss: 0.9187 | LR: 0.000359\n  Batch 500/1902 | Loss: 0.9228 | LR: 0.000357\n  Batch 600/1902 | Loss: 0.9283 | LR: 0.000355\n  Batch 700/1902 | Loss: 0.9308 | LR: 0.000354\n  Batch 800/1902 | Loss: 0.9305 | LR: 0.000352\n  Batch 900/1902 | Loss: 0.9321 | LR: 0.000350\n  Batch 1000/1902 | Loss: 0.9328 | LR: 0.000349\n  Batch 1100/1902 | Loss: 0.9353 | LR: 0.000347\n  Batch 1200/1902 | Loss: 0.9369 | LR: 0.000345\n  Batch 1300/1902 | Loss: 0.9374 | LR: 0.000344\n  Batch 1400/1902 | Loss: 0.9384 | LR: 0.000342\n  Batch 1500/1902 | Loss: 0.9383 | LR: 0.000340\n  Batch 1600/1902 | Loss: 0.9372 | LR: 0.000339\n  Batch 1700/1902 | Loss: 0.9385 | LR: 0.000337\n  Batch 1800/1902 | Loss: 0.9389 | LR: 0.000335\n  Batch 1900/1902 | Loss: 0.9399 | LR: 0.000334\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 0.9400\n     Val Loss:   1.0186\n     LR:         0.000334\n     ‚úÖ Best model saved! (Val Loss: 1.0186)\n\n============================================================\nEpoch 12/25\n============================================================\n  Batch 100/1902 | Loss: 0.8923 | LR: 0.000332\n  Batch 200/1902 | Loss: 0.8920 | LR: 0.000330\n  Batch 300/1902 | Loss: 0.8940 | LR: 0.000329\n  Batch 400/1902 | Loss: 0.8941 | LR: 0.000327\n  Batch 500/1902 | Loss: 0.8953 | LR: 0.000325\n  Batch 600/1902 | Loss: 0.8980 | LR: 0.000323\n  Batch 700/1902 | Loss: 0.9004 | LR: 0.000322\n  Batch 800/1902 | Loss: 0.9033 | LR: 0.000320\n  Batch 900/1902 | Loss: 0.9041 | LR: 0.000318\n  Batch 1000/1902 | Loss: 0.9063 | LR: 0.000317\n  Batch 1100/1902 | Loss: 0.9076 | LR: 0.000315\n  Batch 1200/1902 | Loss: 0.9085 | LR: 0.000313\n  Batch 1300/1902 | Loss: 0.9100 | LR: 0.000311\n  Batch 1400/1902 | Loss: 0.9111 | LR: 0.000310\n  Batch 1500/1902 | Loss: 0.9121 | LR: 0.000308\n  Batch 1600/1902 | Loss: 0.9124 | LR: 0.000306\n  Batch 1700/1902 | Loss: 0.9130 | LR: 0.000304\n  Batch 1800/1902 | Loss: 0.9143 | LR: 0.000303\n  Batch 1900/1902 | Loss: 0.9162 | LR: 0.000301\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 0.9164\n     Val Loss:   1.0185\n     LR:         0.000301\n     ‚úÖ Best model saved! (Val Loss: 1.0185)\n     ‚ö† EarlyStopping: 1/7\n\n============================================================\nEpoch 13/25\n============================================================\n  Batch 100/1902 | Loss: 0.8732 | LR: 0.000299\n  Batch 200/1902 | Loss: 0.8718 | LR: 0.000297\n  Batch 300/1902 | Loss: 0.8769 | LR: 0.000296\n  Batch 400/1902 | Loss: 0.8793 | LR: 0.000294\n  Batch 500/1902 | Loss: 0.8753 | LR: 0.000292\n  Batch 600/1902 | Loss: 0.8783 | LR: 0.000290\n  Batch 700/1902 | Loss: 0.8783 | LR: 0.000289\n  Batch 800/1902 | Loss: 0.8793 | LR: 0.000287\n  Batch 900/1902 | Loss: 0.8800 | LR: 0.000285\n  Batch 1000/1902 | Loss: 0.8806 | LR: 0.000283\n  Batch 1100/1902 | Loss: 0.8822 | LR: 0.000281\n  Batch 1200/1902 | Loss: 0.8835 | LR: 0.000280\n  Batch 1300/1902 | Loss: 0.8857 | LR: 0.000278\n  Batch 1400/1902 | Loss: 0.8877 | LR: 0.000276\n  Batch 1500/1902 | Loss: 0.8895 | LR: 0.000274\n  Batch 1600/1902 | Loss: 0.8901 | LR: 0.000272\n  Batch 1700/1902 | Loss: 0.8907 | LR: 0.000271\n  Batch 1800/1902 | Loss: 0.8911 | LR: 0.000269\n  Batch 1900/1902 | Loss: 0.8912 | LR: 0.000267\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 0.8913\n     Val Loss:   1.0178\n     LR:         0.000267\n     ‚úÖ Best model saved! (Val Loss: 1.0178)\n\n============================================================\nEpoch 14/25\n============================================================\n  Batch 100/1902 | Loss: 0.8628 | LR: 0.000265\n  Batch 200/1902 | Loss: 0.8542 | LR: 0.000263\n  Batch 300/1902 | Loss: 0.8530 | LR: 0.000262\n  Batch 400/1902 | Loss: 0.8556 | LR: 0.000260\n  Batch 500/1902 | Loss: 0.8598 | LR: 0.000258\n  Batch 600/1902 | Loss: 0.8593 | LR: 0.000256\n  Batch 700/1902 | Loss: 0.8614 | LR: 0.000255\n  Batch 800/1902 | Loss: 0.8627 | LR: 0.000253\n  Batch 900/1902 | Loss: 0.8649 | LR: 0.000251\n  Batch 1000/1902 | Loss: 0.8651 | LR: 0.000249\n  Batch 1100/1902 | Loss: 0.8643 | LR: 0.000247\n  Batch 1200/1902 | Loss: 0.8659 | LR: 0.000246\n  Batch 1300/1902 | Loss: 0.8658 | LR: 0.000244\n  Batch 1400/1902 | Loss: 0.8651 | LR: 0.000242\n  Batch 1500/1902 | Loss: 0.8658 | LR: 0.000240\n  Batch 1600/1902 | Loss: 0.8655 | LR: 0.000238\n  Batch 1700/1902 | Loss: 0.8671 | LR: 0.000237\n  Batch 1800/1902 | Loss: 0.8679 | LR: 0.000235\n  Batch 1900/1902 | Loss: 0.8681 | LR: 0.000233\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 0.8683\n     Val Loss:   1.0199\n     LR:         0.000233\n     ‚ö† EarlyStopping: 1/7\n\n============================================================\nEpoch 15/25\n============================================================\n  Batch 100/1902 | Loss: 0.8310 | LR: 0.000231\n  Batch 200/1902 | Loss: 0.8276 | LR: 0.000229\n  Batch 300/1902 | Loss: 0.8299 | LR: 0.000228\n  Batch 400/1902 | Loss: 0.8296 | LR: 0.000226\n  Batch 500/1902 | Loss: 0.8327 | LR: 0.000224\n  Batch 600/1902 | Loss: 0.8342 | LR: 0.000222\n  Batch 700/1902 | Loss: 0.8360 | LR: 0.000220\n  Batch 800/1902 | Loss: 0.8378 | LR: 0.000219\n  Batch 900/1902 | Loss: 0.8382 | LR: 0.000217\n  Batch 1000/1902 | Loss: 0.8379 | LR: 0.000215\n  Batch 1100/1902 | Loss: 0.8406 | LR: 0.000213\n  Batch 1200/1902 | Loss: 0.8414 | LR: 0.000212\n  Batch 1300/1902 | Loss: 0.8414 | LR: 0.000210\n  Batch 1400/1902 | Loss: 0.8425 | LR: 0.000208\n  Batch 1500/1902 | Loss: 0.8438 | LR: 0.000206\n  Batch 1600/1902 | Loss: 0.8440 | LR: 0.000204\n  Batch 1700/1902 | Loss: 0.8443 | LR: 0.000203\n  Batch 1800/1902 | Loss: 0.8456 | LR: 0.000201\n  Batch 1900/1902 | Loss: 0.8464 | LR: 0.000199\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 0.8463\n     Val Loss:   1.0222\n     LR:         0.000199\n     ‚ö† EarlyStopping: 2/7\n\n============================================================\nEpoch 16/25\n============================================================\n  Batch 100/1902 | Loss: 0.8041 | LR: 0.000197\n  Batch 200/1902 | Loss: 0.8015 | LR: 0.000196\n  Batch 300/1902 | Loss: 0.8132 | LR: 0.000194\n  Batch 400/1902 | Loss: 0.8137 | LR: 0.000192\n  Batch 500/1902 | Loss: 0.8136 | LR: 0.000190\n  Batch 600/1902 | Loss: 0.8170 | LR: 0.000189\n  Batch 700/1902 | Loss: 0.8180 | LR: 0.000187\n  Batch 800/1902 | Loss: 0.8188 | LR: 0.000185\n  Batch 900/1902 | Loss: 0.8198 | LR: 0.000183\n  Batch 1000/1902 | Loss: 0.8200 | LR: 0.000182\n  Batch 1100/1902 | Loss: 0.8223 | LR: 0.000180\n  Batch 1200/1902 | Loss: 0.8235 | LR: 0.000178\n  Batch 1300/1902 | Loss: 0.8245 | LR: 0.000177\n  Batch 1400/1902 | Loss: 0.8246 | LR: 0.000175\n  Batch 1500/1902 | Loss: 0.8248 | LR: 0.000173\n  Batch 1600/1902 | Loss: 0.8261 | LR: 0.000171\n  Batch 1700/1902 | Loss: 0.8263 | LR: 0.000170\n  Batch 1800/1902 | Loss: 0.8265 | LR: 0.000168\n  Batch 1900/1902 | Loss: 0.8263 | LR: 0.000166\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 0.8261\n     Val Loss:   1.0266\n     LR:         0.000166\n     ‚ö† EarlyStopping: 3/7\n\n============================================================\nEpoch 17/25\n============================================================\n  Batch 100/1902 | Loss: 0.7780 | LR: 0.000165\n  Batch 200/1902 | Loss: 0.7898 | LR: 0.000163\n  Batch 300/1902 | Loss: 0.7911 | LR: 0.000161\n  Batch 400/1902 | Loss: 0.7970 | LR: 0.000160\n  Batch 500/1902 | Loss: 0.8014 | LR: 0.000158\n  Batch 600/1902 | Loss: 0.8027 | LR: 0.000156\n  Batch 700/1902 | Loss: 0.8046 | LR: 0.000155\n  Batch 800/1902 | Loss: 0.8039 | LR: 0.000153\n  Batch 900/1902 | Loss: 0.8039 | LR: 0.000151\n  Batch 1000/1902 | Loss: 0.8030 | LR: 0.000150\n  Batch 1100/1902 | Loss: 0.8038 | LR: 0.000148\n  Batch 1200/1902 | Loss: 0.8053 | LR: 0.000146\n  Batch 1300/1902 | Loss: 0.8049 | LR: 0.000145\n  Batch 1400/1902 | Loss: 0.8053 | LR: 0.000143\n  Batch 1500/1902 | Loss: 0.8050 | LR: 0.000141\n  Batch 1600/1902 | Loss: 0.8057 | LR: 0.000140\n  Batch 1700/1902 | Loss: 0.8062 | LR: 0.000138\n  Batch 1800/1902 | Loss: 0.8067 | LR: 0.000137\n  Batch 1900/1902 | Loss: 0.8066 | LR: 0.000135\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 0.8067\n     Val Loss:   1.0304\n     LR:         0.000135\n     ‚ö† EarlyStopping: 4/7\n\n============================================================\nEpoch 18/25\n============================================================\n  Batch 100/1902 | Loss: 0.7862 | LR: 0.000133\n  Batch 200/1902 | Loss: 0.7816 | LR: 0.000132\n  Batch 300/1902 | Loss: 0.7811 | LR: 0.000130\n  Batch 400/1902 | Loss: 0.7833 | LR: 0.000129\n  Batch 500/1902 | Loss: 0.7821 | LR: 0.000127\n  Batch 600/1902 | Loss: 0.7795 | LR: 0.000126\n  Batch 700/1902 | Loss: 0.7786 | LR: 0.000124\n  Batch 800/1902 | Loss: 0.7808 | LR: 0.000122\n  Batch 900/1902 | Loss: 0.7809 | LR: 0.000121\n  Batch 1000/1902 | Loss: 0.7808 | LR: 0.000119\n  Batch 1100/1902 | Loss: 0.7810 | LR: 0.000118\n  Batch 1200/1902 | Loss: 0.7815 | LR: 0.000116\n  Batch 1300/1902 | Loss: 0.7835 | LR: 0.000115\n  Batch 1400/1902 | Loss: 0.7857 | LR: 0.000113\n  Batch 1500/1902 | Loss: 0.7854 | LR: 0.000112\n  Batch 1600/1902 | Loss: 0.7863 | LR: 0.000110\n  Batch 1700/1902 | Loss: 0.7865 | LR: 0.000109\n  Batch 1800/1902 | Loss: 0.7872 | LR: 0.000107\n  Batch 1900/1902 | Loss: 0.7881 | LR: 0.000106\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 0.7881\n     Val Loss:   1.0359\n     LR:         0.000106\n     ‚ö† EarlyStopping: 5/7\n\n============================================================\nEpoch 19/25\n============================================================\n  Batch 100/1902 | Loss: 0.7766 | LR: 0.000104\n  Batch 200/1902 | Loss: 0.7744 | LR: 0.000103\n  Batch 300/1902 | Loss: 0.7638 | LR: 0.000101\n  Batch 400/1902 | Loss: 0.7631 | LR: 0.000100\n  Batch 500/1902 | Loss: 0.7639 | LR: 0.000099\n  Batch 600/1902 | Loss: 0.7674 | LR: 0.000097\n  Batch 700/1902 | Loss: 0.7668 | LR: 0.000096\n  Batch 800/1902 | Loss: 0.7676 | LR: 0.000094\n  Batch 900/1902 | Loss: 0.7667 | LR: 0.000093\n  Batch 1000/1902 | Loss: 0.7678 | LR: 0.000092\n  Batch 1100/1902 | Loss: 0.7690 | LR: 0.000090\n  Batch 1200/1902 | Loss: 0.7700 | LR: 0.000089\n  Batch 1300/1902 | Loss: 0.7702 | LR: 0.000087\n  Batch 1400/1902 | Loss: 0.7716 | LR: 0.000086\n  Batch 1500/1902 | Loss: 0.7722 | LR: 0.000085\n  Batch 1600/1902 | Loss: 0.7719 | LR: 0.000083\n  Batch 1700/1902 | Loss: 0.7726 | LR: 0.000082\n  Batch 1800/1902 | Loss: 0.7734 | LR: 0.000081\n  Batch 1900/1902 | Loss: 0.7735 | LR: 0.000079\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 0.7735\n     Val Loss:   1.0396\n     LR:         0.000079\n     ‚ö† EarlyStopping: 6/7\n\n============================================================\nEpoch 20/25\n============================================================\n  Batch 100/1902 | Loss: 0.7533 | LR: 0.000078\n  Batch 200/1902 | Loss: 0.7515 | LR: 0.000077\n  Batch 300/1902 | Loss: 0.7518 | LR: 0.000075\n  Batch 400/1902 | Loss: 0.7543 | LR: 0.000074\n  Batch 500/1902 | Loss: 0.7530 | LR: 0.000073\n  Batch 600/1902 | Loss: 0.7557 | LR: 0.000072\n  Batch 700/1902 | Loss: 0.7578 | LR: 0.000070\n  Batch 800/1902 | Loss: 0.7581 | LR: 0.000069\n  Batch 900/1902 | Loss: 0.7576 | LR: 0.000068\n  Batch 1000/1902 | Loss: 0.7570 | LR: 0.000067\n  Batch 1100/1902 | Loss: 0.7582 | LR: 0.000065\n  Batch 1200/1902 | Loss: 0.7580 | LR: 0.000064\n  Batch 1300/1902 | Loss: 0.7584 | LR: 0.000063\n  Batch 1400/1902 | Loss: 0.7589 | LR: 0.000062\n  Batch 1500/1902 | Loss: 0.7589 | LR: 0.000061\n  Batch 1600/1902 | Loss: 0.7589 | LR: 0.000060\n  Batch 1700/1902 | Loss: 0.7598 | LR: 0.000058\n  Batch 1800/1902 | Loss: 0.7590 | LR: 0.000057\n  Batch 1900/1902 | Loss: 0.7595 | LR: 0.000056\n\n  Validating...\n\n  üìä Summary:\n     Train Loss: 0.7594\n     Val Loss:   1.0430\n     LR:         0.000056\n     ‚ö† EarlyStopping: 7/7\n\nüõë Early stopping at epoch 20\n   Best Val Loss: 1.0178\n\n============================================================\nLOADING BEST MODEL\n============================================================\n‚úì Loaded model from epoch 13\n  Best Val Loss: 1.0178\n\n============================================================\nEVALUATION ON TEST SET\n============================================================\nEvaluating 100 test samples...\n\n--- Example 1 ---\nEN: When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy . &quot;\nGT: Khi t√¥i c√≤n nh·ªè , T√¥i nghƒ© r·∫±ng B·∫ØcTri·ªÅu Ti√™n l√† ƒë·∫•t n∆∞·ªõc t·ªët nh·∫•t tr√™n th·∫ø gi·ªõi v√† t√¥i th∆∞·ªùng h√°t b√†i &quot; Ch√∫ng ta ch·∫≥ng c√≥ g√¨ ph·∫£i ghen t·ªã . &quot;\nPR: khi t√¥i c√≤n nh·ªè , t√¥i nghƒ© ƒë·∫•t n∆∞·ªõc t√¥i l√† ng∆∞·ªùi gi·ªèi nh·∫•t tr√™n h√†nh t, v√† t√¥i l·ªõn l√™n h√°t m·ªôt b√†i h√°t c√≥ t√™n l√† &quot; kh√¥ng g√¨ l√† ghen t·ªã . &quot;\nBLEU: 0.1808\n\n--- Example 2 ---\nEN: And I was very proud .\nGT: T√¥i ƒë√£ r·∫•t t·ª± h√†o v·ªÅ ƒë·∫•t n∆∞·ªõc t√¥i .\nPR: v√† t√¥i r·∫•t t·ª± h√†o .\nBLEU: 0.1179\n\n--- Example 3 ---\nEN: In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .\nGT: ·ªû tr∆∞·ªùng , ch√∫ng t√¥i d√†nh r·∫•t nhi·ªÅu th·ªùi gian ƒë·ªÉ h·ªçc v·ªÅ cu·ªôc ƒë·ªùi c·ªßa ch·ªß t·ªãch Kim II- Sung , nh∆∞ng l·∫°i kh√¥ng h·ªçc nhi·ªÅu v·ªÅ th·∫ø gi·ªõi b√™n ngo√†i , ngo·∫°i tr·ª´ vi·ªác Hoa K·ª≥ , H√†n Qu·ªëc v√† Nh·∫≠t B·∫£n l√† k·∫ª th√π c·ªßa ch√∫ng t√¥i .\nPR: ·ªü tr∆∞·ªùng , ch√∫ng t√¥i ƒë√£ d√†nh r·∫•t nhi·ªÅu th·ªùi gian nghi√™n c·ª©u v·ªÅ l·ªãch s·ª≠ c·ªßa kim sun , nh∆∞ng ch√∫ng t√¥i ch∆∞a bao gi·ªù h·ªçc ƒë∆∞·ª£c nhi·ªÅu v·ªÅ th·∫ø gi·ªõi b√™n ngo√†i , ngo·∫°i tr·ª´ n∆∞·ªõc m·ªπ , nam h√†n , nh·∫≠t b·∫£n l√† nh·ªØng k·∫ª th√π .\nBLEU: 0.3142\n\n--- Example 4 ---\nEN: Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .\nGT: M·∫∑c d√π t√¥i ƒë√£ t·ª´ng t·ª± h·ªèi kh√¥ng bi·∫øt th·∫ø gi·ªõi b√™n ngo√†i kia nh∆∞ th·∫ø n√†o , nh∆∞ng t√¥i v·∫´n nghƒ© r·∫±ng m√¨nh s·∫Ω s·ªëng c·∫£ cu·ªôc ƒë·ªùi ·ªü B·∫ØcTri·ªÅu Ti√™n , cho t·ªõi khi t·∫•t c·∫£ m·ªçi th·ª© ƒë·ªôt nhi√™n thay ƒë·ªïi .\nPR: m·∫∑c d√π t√¥i th∆∞·ªùng th·∫Øc m·∫Øc v·ªÅ th·∫ø gi·ªõi b√™n ngo√†i , t√¥i nghƒ© t√¥i s·∫Ω d√†nh c·∫£ ƒë·ªùi m√¨nh ·ªü b·∫Øc trtcho ƒë·∫øn khi m·ªçi th·ª© thay ƒë·ªïi ƒë·ªôt ng·ªôt .\nBLEU: 0.0871\n\n--- Example 5 ---\nEN: When I was seven years old , I saw my first public execution , but I thought my life in North Korea was normal .\nGT: Khi t√¥i l√™n 7 , t√¥i ch·ª©ng ki·∫øn c·∫£nh ng∆∞·ªùi ta x·ª≠ b·∫Øn c√¥ng khai l·∫ßn ƒë·∫ßu ti√™n trong ƒë·ªùi , nh∆∞ng t√¥i v·∫´n nghƒ© cu·ªôc s·ªëng c·ªßa m√¨nh ·ªü ƒë√¢y l√† ho√†n to√†n b√¨nh th∆∞·ªùng .\nPR: khi t√¥i 7 tu·ªïi , t√¥i ƒë√£ ch·ª©ng kn l·∫ßn ƒë·∫ßu tc·ªßa m√¨nh , nh∆∞ng t√¥i nghƒ© cu·ªôc ƒë·ªùi t√¥i ·ªü b·∫Øc trtl√† b√¨nh th∆∞·ªùng .\nBLEU: 0.0600\n\n============================================================\nAVERAGE BLEU SCORE: 0.1805\n============================================================\n\n============================================================\nINTERACTIVE TRANSLATION\n============================================================\nEnter English sentences to translate (or 'quit' to exit):\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"EN:  When I was seven years old , I saw my first public execution , but I thought my life in North Korea was normal\n"}],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# 0. IMPORTS + SEED\n# =========================================================\nimport math\nimport random\nfrom collections import Counter, defaultdict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"DEVICE:\", device)\n\n# =========================================================\n# 1. BPE TOKENIZER (GI·ªêNG L√öC TRAIN)\n# =========================================================\nclass BPETokenizer:\n    def __init__(self, texts, vocab_size=8000, min_freq=2, max_samples=50000):\n        self.vocab_size = vocab_size\n        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n        self.idx2word = {v: k for k, v in self.word2idx.items()}\n        self.bpe_codes = {}\n        self.cache = {}\n\n        if len(texts) > max_samples:\n            texts = random.sample(texts, max_samples)\n\n        self.build_bpe(texts, min_freq)\n\n    def get_stats(self, vocab):\n        pairs = defaultdict(int)\n        for word, freq in vocab.items():\n            symbols = word.split()\n            for i in range(len(symbols) - 1):\n                pairs[(symbols[i], symbols[i + 1])] += freq\n        return pairs\n\n    def merge_vocab(self, pair, vocab):\n        new_vocab = {}\n        bigram = \" \".join(pair)\n        replacement = \"\".join(pair)\n        for word, freq in vocab.items():\n            new_vocab[word.replace(bigram, replacement)] = freq\n        return new_vocab\n\n    def build_bpe(self, texts, min_freq):\n        word_freq = Counter()\n        for line in texts:\n            word_freq.update(line.lower().split())\n\n        vocab = {\n            \" \".join(list(word)) + \" </w>\": freq\n            for word, freq in word_freq.items()\n            if freq >= min_freq\n        }\n\n        num_merges = min(self.vocab_size - len(self.word2idx), 5000)\n\n        for i in range(num_merges):\n            pairs = self.get_stats(vocab)\n            if not pairs:\n                break\n            best = max(pairs, key=pairs.get)\n            vocab = self.merge_vocab(best, vocab)\n            self.bpe_codes[best] = i\n\n        for word in vocab.keys():\n            for token in word.split():\n                if token not in self.word2idx:\n                    idx = len(self.word2idx)\n                    self.word2idx[token] = idx\n                    self.idx2word[idx] = token\n\n    def apply_bpe(self, word):\n        if word in self.cache:\n            return self.cache[word]\n\n        tokens = \" \".join(list(word)) + \" </w>\"\n        while True:\n            symbols = tokens.split()\n            pairs = [(symbols[i], symbols[i + 1]) for i in range(len(symbols) - 1)]\n            valid = [(self.bpe_codes.get(p, 1e9), i, p) for i, p in enumerate(pairs) if p in self.bpe_codes]\n            if not valid:\n                break\n            _, pos, pair = min(valid)\n            symbols[pos] = \"\".join(pair)\n            del symbols[pos + 1]\n            tokens = \" \".join(symbols)\n\n        self.cache[word] = tokens.split()\n        return self.cache[word]\n\n    def encode(self, text):\n        ids = []\n        for word in text.lower().split():\n            for t in self.apply_bpe(word):\n                ids.append(self.word2idx.get(t, 3))\n        return ids\n\n    def decode(self, ids):\n        words, cur = [], \"\"\n        for idx in ids:\n            if idx == 2:\n                break\n            if idx > 3:\n                tok = self.idx2word.get(idx, \"<unk>\")\n                if tok.endswith(\"</w>\"):\n                    cur += tok[:-4]\n                    words.append(cur)\n                    cur = \"\"\n                else:\n                    cur += tok\n        if cur:\n            words.append(cur)\n        return \" \".join(words)\n\n# =========================================================\n# 2. LOAD DATA ‚Üí BUILD TOKENIZER\n# =========================================================\ntrain_en = open(\"/kaggle/input/en-vi-ds/data/train.en\", encoding=\"utf-8\").read().splitlines()\ntrain_vi = open(\"/kaggle/input/en-vi-ds/data/train.vi\", encoding=\"utf-8\").read().splitlines()\n\nprint(\"Building tokenizers...\")\ntok_src = BPETokenizer(train_en)\ntok_trg = BPETokenizer(train_vi)\n\n# =========================================================\n# 3. TRANSFORMER MODEL (GI·ªêNG L√öC TRAIN)\n# =========================================================\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\nclass TransformerModel(nn.Module):\n    def __init__(self, src_vocab, trg_vocab, d_model=512):\n        super().__init__()\n        self.scale = math.sqrt(d_model)\n        self.src_emb = nn.Embedding(src_vocab, d_model, padding_idx=0)\n        self.trg_emb = nn.Embedding(trg_vocab, d_model, padding_idx=0)\n        self.pos = PositionalEncoding(d_model)\n\n        self.transformer = nn.Transformer(\n            d_model=d_model,\n            nhead=8,\n            num_encoder_layers=6,\n            num_decoder_layers=6,\n            batch_first=True,\n            norm_first=True\n        )\n\n        self.fc = nn.Linear(d_model, trg_vocab)\n        self.fc.weight = self.trg_emb.weight\n\n# =========================================================\n# 4. LOAD CHECKPOINT (FIX VOCAB MISMATCH)\n# =========================================================\nckpt = torch.load(\n    \"/kaggle/input/envi-final/pytorch/default/1/best_model (11).pt\",\n    map_location=device\n)\n\nsrc_vocab_ckpt = ckpt[\"model_state_dict\"][\"src_emb.weight\"].shape[0]\ntrg_vocab_ckpt = ckpt[\"model_state_dict\"][\"trg_emb.weight\"].shape[0]\n\nprint(\"Checkpoint vocab:\", src_vocab_ckpt, trg_vocab_ckpt)\n\nmodel = TransformerModel(\n    src_vocab=src_vocab_ckpt,\n    trg_vocab=trg_vocab_ckpt\n).to(device)\n\nmodel.load_state_dict(ckpt[\"model_state_dict\"])\nmodel.eval()\nprint(\"‚úÖ Model loaded\")\n\n# =========================================================\n# 5. SAFE ENCODE (KH√ìA TOKEN ID)\n# =========================================================\ndef safe_encode(tok, text, max_id):\n    return [i if i < max_id else 3 for i in tok.encode(text)]\n\n# =========================================================\n# 6. BEAM SEARCH TRANSLATION\n# =========================================================\ndef translate(text, beam_size=5, max_len=80):\n    sos, eos = 1, 2\n\n    src_ids = safe_encode(tok_src, text, src_vocab_ckpt)\n    src = torch.tensor([sos] + src_ids + [eos]).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        src_mask = (src == 0)\n        memory = model.transformer.encoder(\n            model.pos(model.src_emb(src) * model.scale),\n            src_key_padding_mask=src_mask\n        )\n\n    beams = [([sos], 0.0)]\n\n    for _ in range(max_len):\n        new_beams = []\n        for seq, score in beams:\n            if seq[-1] == eos:\n                new_beams.append((seq, score))\n                continue\n\n            trg = torch.tensor(seq).unsqueeze(0).to(device)\n            tgt_mask = nn.Transformer.generate_square_subsequent_mask(trg.size(1)).to(device)\n\n            with torch.no_grad():\n                out = model.transformer.decoder(\n                    model.pos(model.trg_emb(trg) * model.scale),\n                    memory,\n                    tgt_mask=tgt_mask,\n                    memory_key_padding_mask=src_mask\n                )\n                logp = F.log_softmax(model.fc(out[:, -1]), dim=-1)\n\n            topk = torch.topk(logp, beam_size)\n            for i in range(beam_size):\n                new_beams.append(\n                    (seq + [topk.indices[0, i].item()],\n                     score + topk.values[0, i].item())\n                )\n\n        beams = sorted(new_beams, key=lambda x: x[1] / len(x[0]), reverse=True)[:beam_size]\n        if all(b[0][-1] == eos for b in beams):\n            break\n\n    return tok_trg.decode(beams[0][0][1:])\n\n# =========================================================\n# 7. TEST\n# =========================================================\nprint(translate(\"I love machine learning\"))\nprint(translate(\"This model was trained using transformer architecture\"))\nprint(translate(\"How are you today?\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T12:51:23.104172Z","iopub.execute_input":"2025-12-16T12:51:23.104813Z","iopub.status.idle":"2025-12-16T12:53:18.880543Z","shell.execute_reply.started":"2025-12-16T12:51:23.104789Z","shell.execute_reply":"2025-12-16T12:53:18.879921Z"}},"outputs":[{"name":"stdout","text":"DEVICE: cuda\nBuilding tokenizers...\nCheckpoint vocab: 15346 7038\n‚úÖ Model loaded\nt√¥i y√™u m√°y h·ªçc .\nm√¥ h√¨nh n√†y ƒë∆∞·ª£c hu·∫•n luy·ªán b·∫±ng c√°ch s·ª≠ d·ª•ng kn tr√∫c chuy·ªÉn ho√° .\nng√†y nay b·∫°n l√† ai ?\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(translate(\"Secondly various projects, researches, assignments and practical scenarios are conducted in universities or colleges from where students get exposure and experience to various problems which they might have to face in their real life while practicing. Like in dentistry the students have to work on tooth for scaling, wiring etc from which they get practical exposure.\"))\nprint(translate(\"Moreover universities have huge libraries carrying thousands of books of different subjects and other study material like fictional, non-fictional, journals, newspapers, reports which are huge sources of information for the students and teachers.\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T13:23:42.197855Z","iopub.execute_input":"2025-12-16T13:23:42.198155Z","iopub.status.idle":"2025-12-16T13:23:46.521556Z","shell.execute_reply.started":"2025-12-16T13:23:42.198135Z","shell.execute_reply":"2025-12-16T13:23:46.520772Z"}},"outputs":[{"name":"stdout","text":"th·ª© hai l√† nh·ªØng nh√† nghi√™n c·ª©u , nh·ªØng b√†i lu·∫≠n lu·∫≠n v√† th·ª±c t·∫ø ƒë∆∞·ª£c tn h√†nh t·∫°i c√°c tr∆∞·ªùng ƒë·∫°i h·ªçc ho·∫∑c ƒë·∫°i h·ªçc t·ª´ nh·ªØng n∆°i h·ªçc sinh c√≥ ƒë∆∞·ª£c trti√™u v√† kinh nghi·ªám ƒë·ªÉ gi·∫£i quy·∫øt nhi·ªÅu v·∫•n ƒë·ªÅ kh√°c nhau m√† h·ªç c√≥ th·ªÉ gi·∫£i quy·∫øt ƒë∆∞·ª£c b·∫±ng c√°ch √°p d·ª•ng trong cu·ªôc s·ªëng .\nnhi·ªÅu tr∆∞·ªùng ƒë·∫°i h·ªçc h∆°n c√≥ nh·ªØng th∆∞ vkh·ªïng l·ªì mang theo h√†ng ng√†n nh·ªØng ƒë·ªÅ t√†i kh√°c nhau v√† nh·ªØng nghi√™n c·ª©u kh√°c nh∆∞ nh·ªØng t√†i li·ªáu h∆∞ c·∫•u , kh√¥ng ph·∫£i t·∫°p ch√≠ , b√°o ch√≠ , b√°o c√°o , b√°o c√°o l√† nh·ªØng m·∫©u tin l·ªõn cho nh·ªØng th√¥ng tin l·ªõn .\n","output_type":"stream"}],"execution_count":18}]}