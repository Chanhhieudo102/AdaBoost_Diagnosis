{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13767258,"sourceType":"datasetVersion","datasetId":8761788},{"sourceId":649510,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":489996,"modelId":505429}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport random\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# -------------------------\n# Seed (deterministic for reproducibility)\n# -------------------------\ndef set_seed(seed=42):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\n# ==============================\n# 2️⃣ LOAD DỮ LIỆU\n# ==============================\ntrain_en = open(\"/kaggle/input/en-vi-ds/data/train.en\", \"r\", encoding=\"utf-8\").read().splitlines()\ntrain_vi = open(\"/kaggle/input/en-vi-ds/data/train.vi\", \"r\", encoding=\"utf-8\").read().splitlines()\ntest_en  = open(\"/kaggle/input/en-vi-ds/data/tst2013.en\", \"r\", encoding=\"utf-8\").read().splitlines()\ntest_vi  = open(\"/kaggle/input/en-vi-ds/data/tst2013.vi\", \"r\", encoding=\"utf-8\").read().splitlines()\n\nprint(\"Train:\", len(train_en), \"Test:\", len(test_en))\n\n# ==============================\n# 3️⃣ TOKENIZER (min_freq=1 to avoid too many <unk>)\n# ==============================\nclass Tokenizer:\n    def __init__(self, texts, min_freq=1):\n        self.word2idx = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n        self.idx2word = {v:k for k,v in self.word2idx.items()}\n        self.build_vocab(texts, min_freq)\n\n    def build_vocab(self, texts, min_freq):\n        counter = Counter()\n        for line in texts:\n            counter.update(line.strip().lower().split())\n        for word, freq in counter.items():\n            if freq >= min_freq and word not in self.word2idx:\n                idx = len(self.word2idx)\n                self.word2idx[word] = idx\n                self.idx2word[idx] = word\n\n    def encode(self, text):\n        # returns list of token ids (no <sos>/<eos> here)\n        return [self.word2idx.get(tok, self.word2idx[\"<unk>\"]) for tok in text.strip().lower().split()]\n\n    def decode(self, ids):\n        words = []\n        for i in ids:\n            if i == self.word2idx[\"<eos>\"]:\n                break\n            if i <= 3:  # 0-3 are special tokens or pad/unk/sos/eos: skip except unk? keep readable\n                if i == self.word2idx[\"<unk>\"]:\n                    words.append(\"<unk>\")\n                continue\n            words.append(self.idx2word.get(i, \"<unk>\"))\n        return \" \".join(words)\n\ntok_src = Tokenizer(train_en, min_freq=1)\ntok_trg = Tokenizer(train_vi, min_freq=1)\n\n# ==============================\n# 4️⃣ DATASET + collate\n# ==============================\nclass TranslationDataset(Dataset):\n    def __init__(self, src, trg, tok_src, tok_trg):\n        self.src = src\n        self.trg = trg\n        self.tok_src = tok_src\n        self.tok_trg = tok_trg\n\n    def __len__(self): return len(self.src)\n\n    def __getitem__(self, idx):\n        s = [self.tok_src.word2idx[\"<sos>\"]] + self.tok_src.encode(self.src[idx]) + [self.tok_src.word2idx[\"<eos>\"]]\n        t = [self.tok_trg.word2idx[\"<sos>\"]] + self.tok_trg.encode(self.trg[idx]) + [self.tok_trg.word2idx[\"<eos>\"]]\n        return torch.tensor(s, dtype=torch.long), torch.tensor(t, dtype=torch.long)\n\ndef collate_fn(batch, pad_idx=0):\n    src, trg = zip(*batch)\n    src = nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=pad_idx)\n    trg = nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=pad_idx)\n    return src, trg\n\ndataset = TranslationDataset(train_en, train_vi, tok_src, tok_trg)\ntrain_len = int(0.9 * len(dataset))\nval_len = len(dataset) - train_len\ntrain_set, val_set = random_split(dataset, [train_len, val_len])\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader   = DataLoader(val_set, batch_size=32, shuffle=False, collate_fn=collate_fn)\n\n# ==============================\n# 5️⃣ POS ENCODING\n# ==============================\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer('pe', pe.unsqueeze(0))  # registered buffer -> moves with .to(device)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\n# ==============================\n# 6️⃣ TRANSFORMER MODEL (with weight tying & device-safety)\n# ==============================\nclass TransformerModel(nn.Module):\n    def __init__(self, src_vocab, trg_vocab, d_model=256, nhead=4, num_layers=3, pad_idx=0, dropout=0.1):\n        super().__init__()\n        self.pad_idx = pad_idx\n        self.src_emb = nn.Embedding(src_vocab, d_model, padding_idx=pad_idx)\n        self.trg_emb = nn.Embedding(trg_vocab, d_model, padding_idx=pad_idx)\n        self.pos = PositionalEncoding(d_model)\n        self.transformer = nn.Transformer(\n            d_model=d_model, nhead=nhead,\n            num_encoder_layers=num_layers,\n            num_decoder_layers=num_layers,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.fc = nn.Linear(d_model, trg_vocab)\n        # weight tying (output projection shares embedding weight)\n        self.fc.weight = self.trg_emb.weight\n\n    def forward(self, src, trg):\n        # src: (B, S), trg: (B, T)\n        device = src.device\n        src_key_padding_mask = (src == self.pad_idx)  # (B, S)\n        trg_key_padding_mask = (trg == self.pad_idx)  # (B, T)\n        # subsequent mask for target (T, T)\n        T = trg.size(1)\n        trg_mask = nn.Transformer.generate_square_subsequent_mask(T).to(device)\n\n        src_emb = self.pos(self.src_emb(src))  # (B, S, d_model)\n        trg_emb = self.pos(self.trg_emb(trg))  # (B, T, d_model)\n\n        out = self.transformer(\n            src_emb, trg_emb,\n            tgt_mask=trg_mask,\n            src_key_padding_mask=src_key_padding_mask,\n            tgt_key_padding_mask=trg_key_padding_mask,\n            memory_key_padding_mask=src_key_padding_mask\n        )  # (B, T, d_model)\n        return self.fc(out)  # (B, T, trg_vocab)\n\n# ==============================\n# 7️⃣ TRAINING FUNCTION (improvements: lr scheduler, AdamW, save best)\n# ==============================\ndef train_model(model, train_loader, val_loader, device, epochs=10, lr=3e-4, pad_idx=0):\n    model.to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_idx)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', patience=1, factor=0.5, verbose=True)\n\n    best_val = float('inf')\n    for ep in range(1, epochs+1):\n        model.train()\n        total_loss = 0.0\n        for src, trg in train_loader:\n            src, trg = src.to(device), trg.to(device)\n            opt.zero_grad()\n            # teacher forcing: feed trg tokens except last as input\n            out = model(src, trg[:, :-1])  # predict next token for each pos\n            # out: (B, T-1, V) -> flatten\n            loss = loss_fn(out.reshape(-1, out.size(-1)), trg[:, 1:].reshape(-1))\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            opt.step()\n            total_loss += loss.item()\n\n        avg_train = total_loss / len(train_loader)\n\n        # validation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for src, trg in val_loader:\n                src, trg = src.to(device), trg.to(device)\n                out = model(src, trg[:, :-1])\n                loss = loss_fn(out.reshape(-1, out.size(-1)), trg[:, 1:].reshape(-1))\n                val_loss += loss.item()\n        avg_val = val_loss / len(val_loader)\n\n        scheduler.step(avg_val)\n\n        print(f\"Epoch {ep}/{epochs} | Train {avg_train:.4f} | Val {avg_val:.4f}\")\n\n        if avg_val < best_val:\n            best_val = avg_val\n            torch.save({\n                'model_state': model.state_dict(),\n                'tok_src': tok_src.word2idx,\n                'tok_trg': tok_trg.word2idx\n            }, \"best_model.pt\")\n            print(\"✔ Saved best model!\")\n\n# ==============================\n# 8️⃣ TRANSLATE (greedy) - improved (stop when <eos>, limit length)\n# ==============================\ndef translate(model, text, tok_src, tok_trg, device, max_len=60):\n    model.eval()\n    src = [tok_src.word2idx[\"<sos>\"]] + tok_src.encode(text) + [tok_src.word2idx[\"<eos>\"]]\n    src = torch.tensor(src, dtype=torch.long).unsqueeze(0).to(device)  # (1, S)\n    trg = torch.tensor([[tok_trg.word2idx[\"<sos>\"]]], dtype=torch.long).to(device)  # (1,1)\n    with torch.no_grad():\n        for _ in range(max_len):\n            out = model(src, trg)  # (1, T, V)\n            next_tok = out[0, -1].argmax().item()\n            trg = torch.cat([trg, torch.tensor([[next_tok]], device=device)], dim=1)\n            if next_tok == tok_trg.word2idx[\"<eos>\"]:\n                break\n    # strip leading <sos> when decoding\n    return tok_trg.decode(trg[0].tolist()[1:])\n\n# ==============================\n# 9️⃣ EVALUATE BLEU (use smoothing)\n# ==============================\ndef evaluate_test_set(model, test_en, test_vi, tok_src, tok_trg, device, n=50):\n    model.eval()\n    total_bleu = 0\n    smooth = SmoothingFunction().method1\n    n = min(n, len(test_en))\n    for i in range(n):\n        pred = translate(model, test_en[i], tok_src, tok_trg, device)\n        bleu = sentence_bleu([test_vi[i].split()], pred.split(), smoothing_function=smooth)\n        total_bleu += bleu\n        if i < 10:  # print a few examples\n            print(f\"\\nEN: {test_en[i]}\")\n            print(f\"GT: {test_vi[i]}\")\n            print(f\"PR: {pred}\")\n            print(f\"BLEU: {bleu:.4f}\")\n\n    print(\"\\nAVERAGE BLEU =\", total_bleu / n)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================\n# RUN (example)\n# ==============================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TransformerModel(len(tok_src.word2idx), len(tok_trg.word2idx), d_model=256, nhead=4, num_layers=3, pad_idx=tok_src.word2idx[\"<pad>\"])\ntrain_model(model, train_loader, val_loader, device, epochs=10, lr=3e-4, pad_idx=tok_src.word2idx[\"<pad>\"])\n# then evaluate\n# evaluate_test_set(model, test_en, test_vi, tok_src, tok_trg, device, n=50)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nevaluate_test_set(model, test_en, test_vi, tok_src, tok_trg, device, n=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sent = \"See you again\"\nprint(translate(model, sent, tok_src, tok_trg, device))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# full_transformer_nmt.py\nimport math\nimport random\nfrom collections import Counter\nimport os\nimport sys\nimport time\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# -------------------------\n# CẤU HÌNH / SEED\n# -------------------------\ndef set_seed(seed=42):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\nDATA_DIR = \"/kaggle/input/en-vi-ds/data\"\nTRAIN_EN = os.path.join(DATA_DIR, \"train.en\")\nTRAIN_VI = os.path.join(DATA_DIR, \"train.vi\")\nTEST_EN  = os.path.join(DATA_DIR, \"tst2013.en\")\nTEST_VI  = os.path.join(DATA_DIR, \"tst2013.vi\")\n\n# Hyperparams (chỉnh nếu cần)\nBATCH_SIZE = 32\nD_MODEL = 256\nNHEAD = 4\nNUM_LAYERS = 3\nEPOCHS = 20\nLR = 3e-4\nWARMUP_STEPS = 4000\nBEAM_SIZE = 5\nMAX_LEN = 60\nMIN_FREQ = 1   # word-level vocab threshold\nPAD_IDX = 0\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# ==============================\n# 1️⃣ LOAD DỮ LIỆU\n# ==============================\ndef load_lines(path):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        return f.read().splitlines()\n\ntrain_en = load_lines(TRAIN_EN)\ntrain_vi = load_lines(TRAIN_VI)\ntest_en = load_lines(TEST_EN)\ntest_vi = load_lines(TEST_VI)\nprint(\"Train pairs:\", len(train_en), \"Test pairs:\", len(test_en))\n\n# ==============================\n# 2️⃣ TOKENIZER (word-level) - compatible with your pipeline\n# ==============================\nclass Tokenizer:\n    def __init__(self, texts, min_freq=1):\n        self.word2idx = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n        self.idx2word = {v:k for k,v in self.word2idx.items()}\n        self.build_vocab(texts, min_freq)\n\n    def build_vocab(self, texts, min_freq):\n        counter = Counter()\n        for line in texts:\n            counter.update(line.strip().lower().split())\n        for word, freq in counter.items():\n            if freq >= min_freq and word not in self.word2idx:\n                idx = len(self.word2idx)\n                self.word2idx[word] = idx\n                self.idx2word[idx] = word\n\n    def encode(self, text):\n        return [self.word2idx.get(tok, self.word2idx[\"<unk>\"]) for tok in text.strip().lower().split()]\n\n    def decode(self, ids):\n        words = []\n        for i in ids:\n            if i == self.word2idx[\"<eos>\"]:\n                break\n            if i <= 3:\n                if i == self.word2idx[\"<unk>\"]:\n                    words.append(\"<unk>\")\n                continue\n            words.append(self.idx2word.get(i, \"<unk>\"))\n        return \" \".join(words)\n\ntok_src = Tokenizer(train_en, min_freq=MIN_FREQ)\ntok_trg = Tokenizer(train_vi, min_freq=MIN_FREQ)\nprint(\"Src vocab:\", len(tok_src.word2idx), \"Trg vocab:\", len(tok_trg.word2idx))\n\n# ==============================\n# 3️⃣ DATASET\n# ==============================\nclass TranslationDataset(Dataset):\n    def __init__(self, src_lines, trg_lines, tok_src, tok_trg):\n        self.src = src_lines\n        self.trg = trg_lines\n        self.tok_src = tok_src\n        self.tok_trg = tok_trg\n\n    def __len__(self):\n        return len(self.src)\n\n    def __getitem__(self, idx):\n        s = [self.tok_src.word2idx[\"<sos>\"]] + self.tok_src.encode(self.src[idx]) + [self.tok_src.word2idx[\"<eos>\"]]\n        t = [self.tok_trg.word2idx[\"<sos>\"]] + self.tok_trg.encode(self.trg[idx]) + [self.tok_trg.word2idx[\"<eos>\"]]\n        return torch.tensor(s, dtype=torch.long), torch.tensor(t, dtype=torch.long)\n\ndef collate_fn(batch, pad_idx=0):\n    src, trg = zip(*batch)\n    src = nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=pad_idx)\n    trg = nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=pad_idx)\n    return src, trg\n\ndataset = TranslationDataset(train_en, train_vi, tok_src, tok_trg)\ntrain_len = int(0.9 * len(dataset))\nval_len = len(dataset) - train_len\ntrain_set, val_set = random_split(dataset, [train_len, val_len])\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n# ==============================\n# 4️⃣ POS ENCODING\n# ==============================\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\n# ==============================\n# 5️⃣ TRANSFORMER MODEL\n# ==============================\nclass TransformerModel(nn.Module):\n    def __init__(self, src_vocab, trg_vocab, d_model=256, nhead=4, num_layers=3, pad_idx=0, dropout=0.1):\n        super().__init__()\n        self.pad_idx = pad_idx\n        self.src_emb = nn.Embedding(src_vocab, d_model, padding_idx=pad_idx)\n        self.trg_emb = nn.Embedding(trg_vocab, d_model, padding_idx=pad_idx)\n        self.pos = PositionalEncoding(d_model)\n        self.transformer = nn.Transformer(\n            d_model=d_model, nhead=nhead,\n            num_encoder_layers=num_layers,\n            num_decoder_layers=num_layers,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.fc = nn.Linear(d_model, trg_vocab)\n        # tie weights\n        try:\n            self.fc.weight = self.trg_emb.weight\n        except Exception:\n            pass\n\n    def forward(self, src, trg):\n        device = src.device\n        src_key_padding_mask = (src == self.pad_idx)\n        trg_key_padding_mask = (trg == self.pad_idx)\n        T = trg.size(1)\n        trg_mask = nn.Transformer.generate_square_subsequent_mask(T).to(device)\n\n        src_emb = self.pos(self.src_emb(src))\n        trg_emb = self.pos(self.trg_emb(trg))\n\n        out = self.transformer(\n            src_emb, trg_emb,\n            tgt_mask=trg_mask,\n            src_key_padding_mask=src_key_padding_mask,\n            tgt_key_padding_mask=trg_key_padding_mask,\n            memory_key_padding_mask=src_key_padding_mask\n        )\n        return self.fc(out)\n\n# ==============================\n# 6️⃣ LABEL SMOOTHING LOSS\n# ==============================\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.1, ignore_index=0):\n        super().__init__()\n        self.ignore_index = ignore_index\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.classes = classes\n\n    def forward(self, pred, target):\n        # pred: (N, C) logits, target: (N,)\n        pred = pred.log_softmax(dim=-1)  # (N, C)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.classes - 1))\n            mask = (target != self.ignore_index)\n            # scatter target positions\n            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n            # zero out pad positions\n            true_dist = true_dist * mask.unsqueeze(1)\n        loss = torch.mean(torch.sum(-true_dist * pred, dim=1))\n        return loss\n\n# ==============================\n# 7️⃣ LR SCHEDULER (warmup style)\n# ==============================\ndef get_warmup_scheduler(optimizer, d_model=D_MODEL, warmup=WARMUP_STEPS):\n    def lr_lambda(step):\n        step = max(1, step)\n        return (d_model ** -0.5) * min(step ** -0.5, step * (warmup ** -1.5))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n# ==============================\n# 8️⃣ BEAM SEARCH (translate)\n# ==============================\ndef translate_beam(model, text, tok_src, tok_trg, device, beam_size=5, max_len=60):\n    model.eval()\n    sos = tok_trg.word2idx[\"<sos>\"]\n    eos = tok_trg.word2idx[\"<eos>\"]\n\n    src = [tok_src.word2idx[\"<sos>\"]] + tok_src.encode(text) + [tok_src.word2idx[\"<eos>\"]]\n    src = torch.tensor(src, dtype=torch.long).unsqueeze(0).to(device)\n\n    # beams: list of (sequence_tensor, score)\n    beams = [(torch.tensor([[sos]], device=device, dtype=torch.long), 0.0)]\n    completed = []\n\n    with torch.no_grad():\n        for _step in range(max_len):\n            new_beams = []\n            for seq, score in beams:\n                if seq[0, -1].item() == eos:\n                    # already ended, keep in completed\n                    completed.append((seq, score))\n                    continue\n\n                out = model(src, seq)  # (1, T, V)\n                logits = out[0, -1]    # (V,)\n                log_probs = torch.log_softmax(logits, dim=-1)  # (V,)\n                topk = torch.topk(log_probs, beam_size)\n\n                for k in range(topk.indices.size(0)):\n                    tok = topk.indices[k].item()\n                    lp = topk.values[k].item()\n                    new_seq = torch.cat([seq, torch.tensor([[tok]], device=device)], dim=1)\n                    new_score = score + lp\n                    new_beams.append((new_seq, new_score))\n\n            # keep top beam_size\n            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n\n            # stop if we have enough completed and best beam ended\n            if len(completed) >= beam_size:\n                break\n\n        # add remaining beams to completed (if none ended)\n        completed.extend(beams)\n        # sort completed by score\n        completed = sorted(completed, key=lambda x: x[1], reverse=True)\n        best_seq = completed[0][0][0].tolist()[1:]  # remove <sos>\n        return tok_trg.decode(best_seq)\n\n# ==============================\n# 9️⃣ TRAIN + VALIDATION\n# ==============================\ndef train_model(model, train_loader, val_loader, device, epochs=EPOCHS, lr=LR, pad_idx=PAD_IDX):\n    model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n    scheduler = get_warmup_scheduler(optimizer, d_model=D_MODEL, warmup=WARMUP_STEPS)\n    loss_fn = LabelSmoothingLoss(classes=len(tok_trg.word2idx), smoothing=0.1, ignore_index=pad_idx)\n\n    best_val = float('inf')\n    global_step = 0\n\n    for ep in range(1, epochs+1):\n        model.train()\n        total_loss = 0.0\n        t0 = time.time()\n\n        for src, trg in train_loader:\n            src, trg = src.to(device), trg.to(device)\n            optimizer.zero_grad()\n            out = model(src, trg[:, :-1])  # predict for positions 1..T-1\n            N, T, V = out.shape\n            loss = loss_fn(out.reshape(-1, V), trg[:, 1:].reshape(-1))\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            global_step += 1\n            total_loss += loss.item()\n\n        avg_train = total_loss / len(train_loader)\n\n        # validation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for src, trg in val_loader:\n                src, trg = src.to(device), trg.to(device)\n                out = model(src, trg[:, :-1])\n                N, T, V = out.shape\n                loss = loss_fn(out.reshape(-1, V), trg[:, 1:].reshape(-1))\n                val_loss += loss.item()\n        avg_val = val_loss / len(val_loader)\n        t1 = time.time()\n\n        print(f\"Epoch {ep}/{epochs} | Train {avg_train:.4f} | Val {avg_val:.4f} | Time {t1-t0:.1f}s\")\n\n        if avg_val < best_val:\n            best_val = avg_val\n            torch.save({\n                'model_state': model.state_dict(),\n                'tok_src': tok_src.word2idx,\n                'tok_trg': tok_trg.word2idx\n            }, \"best_model.pt\")\n            print(\"✔ Saved best model!\")\n\n# ==============================\n# 10️⃣ EVALUATE BLEU\n# ==============================\ndef evaluate_test_set(model, test_en, test_vi, tok_src, tok_trg, device, n=50, use_beam=True):\n    model.to(device)\n    model.eval()\n    total_bleu = 0.0\n    smooth = SmoothingFunction().method1\n    n = min(n, len(test_en))\n    for i in range(n):\n        sent = test_en[i]\n        if use_beam:\n            pred = translate_beam(model, sent, tok_src, tok_trg, device, beam_size=BEAM_SIZE, max_len=MAX_LEN)\n        else:\n            pred = translate_greedy(model, sent, tok_src, tok_trg, device, max_len=MAX_LEN)\n        bleu = sentence_bleu([test_vi[i].split()], pred.split(), smoothing_function=smooth)\n        total_bleu += bleu\n        if i < 10:\n            print(\"\\nEN:\", sent)\n            print(\"GT:\", test_vi[i])\n            print(\"PR:\", pred)\n            print(f\"BLEU: {bleu:.4f}\")\n\n    print(\"\\nAVERAGE BLEU =\", total_bleu / n)\n\n# greedy translate for fallback printing\ndef translate_greedy(model, text, tok_src, tok_trg, device, max_len=60):\n    model.eval()\n    src = [tok_src.word2idx[\"<sos>\"]] + tok_src.encode(text) + [tok_src.word2idx[\"<eos>\"]]\n    src = torch.tensor(src, dtype=torch.long).unsqueeze(0).to(device)\n    trg = torch.tensor([[tok_trg.word2idx[\"<sos>\"]]], dtype=torch.long).to(device)\n    with torch.no_grad():\n        for _ in range(max_len):\n            out = model(src, trg)\n            next_tok = out[0, -1].argmax().item()\n            trg = torch.cat([trg, torch.tensor([[next_tok]], device=device)], dim=1)\n            if next_tok == tok_trg.word2idx[\"<eos>\"]:\n                break\n    return tok_trg.decode(trg[0].tolist()[1:])\n\n# ==============================\n# 11️⃣ RUN: tạo model, train, evaluate, interactive\n# ==============================\nif __name__ == \"__main__\":\n    model = TransformerModel(\n        src_vocab=len(tok_src.word2idx),\n        trg_vocab=len(tok_trg.word2idx),\n        d_model=D_MODEL,\n        nhead=NHEAD,\n        num_layers=NUM_LAYERS,\n        pad_idx=PAD_IDX,\n        dropout=0.1\n    )\n\n    print(\"Start training ...\")\n    train_model(model, train_loader, val_loader, device, epochs=EPOCHS, lr=LR, pad_idx=PAD_IDX)\n\n    # load best model\n    if os.path.exists(\"best_model.pt\"):\n        ckpt = torch.load(\"best_model.pt\", map_location=device)\n        model.load_state_dict(ckpt['model_state'])\n        model.to(device)\n        print(\"Loaded best_model.pt\")\n\n    # evaluate\n    evaluate_test_set(model, test_en, test_vi, tok_src, tok_trg, device, n=50, use_beam=True)\n\n    # interactive\n    print(\"\\nNhập câu tiếng Anh để dịch (gõ 'exit' để thoát):\")\n    while True:\n        text = input(\"EN> \").strip()\n        if text.lower() in (\"exit\", \"quit\"): break\n        out = translate_beam(model, text, tok_src, tok_trg, device, beam_size=BEAM_SIZE, max_len=MAX_LEN)\n        print(\"VI>\", out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T04:39:52.594063Z","iopub.execute_input":"2025-11-25T04:39:52.594377Z","iopub.status.idle":"2025-11-25T04:40:45.701419Z","shell.execute_reply.started":"2025-11-25T04:39:52.594349Z","shell.execute_reply":"2025-11-25T04:40:45.700207Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nTrain pairs: 133317 Test pairs: 1268\nSrc vocab: 47861 Trg vocab: 22443\nStart training ...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_95/543871215.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start training ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPAD_IDX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;31m# load best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_95/543871215.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, epochs, lr, pad_idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# predict for positions 1..T-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_95/543871215.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pred, target)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;31m# zero out pad positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mtrue_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_dist\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrue_dist\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.73 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.61 GiB is free. Process 35505 has 13.13 GiB memory in use. Of the allocated memory 11.54 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.73 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.61 GiB is free. Process 35505 has 13.13 GiB memory in use. Of the allocated memory 11.54 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":1}]}