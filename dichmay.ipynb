{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa87fc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:28:05.876730Z",
     "iopub.status.busy": "2025-12-14T17:28:05.876483Z",
     "iopub.status.idle": "2025-12-14T17:28:11.100745Z",
     "shell.execute_reply": "2025-12-14T17:28:11.100031Z"
    },
    "papermill": {
     "duration": 5.228894,
     "end_time": "2025-12-14T17:28:11.102132",
     "exception": false,
     "start_time": "2025-12-14T17:28:05.873238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyvi\r\n",
      "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pyvi) (1.2.2)\r\n",
      "Collecting sklearn-crfsuite (from pyvi)\r\n",
      "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (3.6.0)\r\n",
      "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\r\n",
      "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\r\n",
      "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\r\n",
      "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (4.67.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->pyvi) (2.4.1)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->pyvi) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->pyvi) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->pyvi) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn->pyvi) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn->pyvi) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn->pyvi) (2024.2.0)\r\n",
      "Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\r\n",
      "Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\r\n",
      "Successfully installed python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4af8900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:28:11.109674Z",
     "iopub.status.busy": "2025-12-14T17:28:11.109444Z",
     "iopub.status.idle": "2025-12-14T17:29:44.223648Z",
     "shell.execute_reply": "2025-12-14T17:29:44.222809Z"
    },
    "papermill": {
     "duration": 93.124748,
     "end_time": "2025-12-14T17:29:44.229725",
     "exception": false,
     "start_time": "2025-12-14T17:28:11.104977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 133317 Test: 1268\n",
      "Building source BPE tokenizer...\n",
      "Initializing BPE Tokenizer (vocab_size=5000)...\n",
      "Using 50000/133317 samples for BPE training\n",
      "Step 1: Counting word frequencies...\n",
      "  Processed 10000/50000 lines\n",
      "  Processed 20000/50000 lines\n",
      "  Processed 30000/50000 lines\n",
      "  Processed 40000/50000 lines\n",
      "Step 2: Found 31792 unique words\n",
      "Step 3: After filtering (min_freq=2): 17929 words\n",
      "Step 4: Learning 3000 BPE merges...\n",
      "  BPE merge 0/3000\n",
      "  BPE merge 100/3000\n",
      "  BPE merge 200/3000\n",
      "  BPE merge 300/3000\n",
      "  BPE merge 400/3000\n",
      "  BPE merge 500/3000\n",
      "  BPE merge 600/3000\n",
      "  BPE merge 700/3000\n",
      "  BPE merge 800/3000\n",
      "  BPE merge 900/3000\n",
      "  BPE merge 1000/3000\n",
      "  BPE merge 1100/3000\n",
      "  BPE merge 1200/3000\n",
      "  BPE merge 1300/3000\n",
      "  BPE merge 1400/3000\n",
      "  BPE merge 1500/3000\n",
      "  BPE merge 1600/3000\n",
      "  BPE merge 1700/3000\n",
      "  BPE merge 1800/3000\n",
      "  BPE merge 1900/3000\n",
      "  BPE merge 2000/3000\n",
      "  BPE merge 2100/3000\n",
      "  BPE merge 2200/3000\n",
      "  BPE merge 2300/3000\n",
      "  BPE merge 2400/3000\n",
      "  BPE merge 2500/3000\n",
      "  BPE merge 2600/3000\n",
      "  BPE merge 2700/3000\n",
      "  BPE merge 2800/3000\n",
      "  BPE merge 2900/3000\n",
      "Step 5: Building final vocabulary...\n",
      "‚úì BPE Tokenizer ready! Vocabulary size: 13040\n",
      "\n",
      "Building target BPE tokenizer...\n",
      "Initializing BPE Tokenizer (vocab_size=5000)...\n",
      "Using 50000/133317 samples for BPE training\n",
      "Step 1: Counting word frequencies...\n",
      "  Processed 10000/50000 lines\n",
      "  Processed 20000/50000 lines\n",
      "  Processed 30000/50000 lines\n",
      "  Processed 40000/50000 lines\n",
      "Step 2: Found 14280 unique words\n",
      "Step 3: After filtering (min_freq=2): 7543 words\n",
      "Step 4: Learning 3000 BPE merges...\n",
      "  BPE merge 0/3000\n",
      "  BPE merge 100/3000\n",
      "  BPE merge 200/3000\n",
      "  BPE merge 300/3000\n",
      "  BPE merge 400/3000\n",
      "  BPE merge 500/3000\n",
      "  BPE merge 600/3000\n",
      "  BPE merge 700/3000\n",
      "  BPE merge 800/3000\n",
      "  BPE merge 900/3000\n",
      "  BPE merge 1000/3000\n",
      "  BPE merge 1100/3000\n",
      "  BPE merge 1200/3000\n",
      "  BPE merge 1300/3000\n",
      "  BPE merge 1400/3000\n",
      "  BPE merge 1500/3000\n",
      "  BPE merge 1600/3000\n",
      "  BPE merge 1700/3000\n",
      "  BPE merge 1800/3000\n",
      "  BPE merge 1900/3000\n",
      "  BPE merge 2000/3000\n",
      "  BPE merge 2100/3000\n",
      "  BPE merge 2200/3000\n",
      "  BPE merge 2300/3000\n",
      "  BPE merge 2400/3000\n",
      "  BPE merge 2500/3000\n",
      "  BPE merge 2600/3000\n",
      "  BPE merge 2700/3000\n",
      "  BPE merge 2800/3000\n",
      "  BPE merge 2900/3000\n",
      "Step 5: Building final vocabulary...\n",
      "‚úì BPE Tokenizer ready! Vocabulary size: 5231\n",
      "\n",
      "Total dataset size: 133317\n",
      "Train: 119985, Val: 13332\n",
      "Train batches: 3750, Val batches: 417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# -------------------------\n",
    "# Seed (deterministic for reproducibility)\n",
    "# -------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ==============================\n",
    "# 2Ô∏è‚É£ LOAD D·ªÆ LI·ªÜU\n",
    "# ==============================\n",
    "train_en = open(\"/kaggle/input/en-vi-ds/data/train.en\", \"r\", encoding=\"utf-8\").read().splitlines()\n",
    "train_vi = open(\"/kaggle/input/en-vi-ds/data/train.vi\", \"r\", encoding=\"utf-8\").read().splitlines()\n",
    "test_en  = open(\"/kaggle/input/en-vi-ds/data/tst2013.en\", \"r\", encoding=\"utf-8\").read().splitlines()\n",
    "test_vi  = open(\"/kaggle/input/en-vi-ds/data/tst2013.vi\", \"r\", encoding=\"utf-8\").read().splitlines()\n",
    "\n",
    "print(\"Train:\", len(train_en), \"Test:\", len(test_en))\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 3Ô∏è‚É£ BPE TOKENIZER\n",
    "# ==============================\n",
    "class BPETokenizer:\n",
    "    def __init__(self, texts, vocab_size=5000, min_freq=2, max_samples=50000):\n",
    "        \"\"\"\n",
    "        texts: list of sentences\n",
    "        vocab_size: target vocabulary size\n",
    "        min_freq: minimum word frequency\n",
    "        max_samples: limit number of sentences for faster training\n",
    "        \"\"\"\n",
    "        print(f\"Initializing BPE Tokenizer (vocab_size={vocab_size})...\")\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "        self.bpe_codes = {}\n",
    "        \n",
    "        # Limit data for faster BPE training\n",
    "        if len(texts) > max_samples:\n",
    "            print(f\"Using {max_samples}/{len(texts)} samples for BPE training\")\n",
    "            texts = random.sample(texts, max_samples)\n",
    "        \n",
    "        self.build_bpe(texts, min_freq)\n",
    "    \n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"Count frequency of adjacent symbol pairs\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"Merge the most frequent pair in vocabulary\"\"\"\n",
    "        new_vocab = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        \n",
    "        for word in vocab:\n",
    "            new_word = word.replace(bigram, replacement)\n",
    "            new_vocab[new_word] = vocab[word]\n",
    "        return new_vocab\n",
    "    \n",
    "    def build_bpe(self, texts, min_freq):\n",
    "        \"\"\"Build BPE vocabulary - optimized version\"\"\"\n",
    "        print(\"Step 1: Counting word frequencies...\")\n",
    "        # Count word frequencies\n",
    "        word_freq = Counter()\n",
    "        for i, line in enumerate(texts):\n",
    "            if i % 10000 == 0 and i > 0:\n",
    "                print(f\"  Processed {i}/{len(texts)} lines\")\n",
    "            words = line.strip().lower().split()\n",
    "            word_freq.update(words)\n",
    "        \n",
    "        print(f\"Step 2: Found {len(word_freq)} unique words\")\n",
    "        \n",
    "        # Filter by min_freq and prepare vocab\n",
    "        vocab = {}\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= min_freq:\n",
    "                vocab[' '.join(list(word)) + ' </w>'] = freq\n",
    "        \n",
    "        print(f\"Step 3: After filtering (min_freq={min_freq}): {len(vocab)} words\")\n",
    "        \n",
    "        # Learn BPE merges\n",
    "        num_merges = min(self.vocab_size - len(self.word2idx), 3000)  # Limit merges\n",
    "        print(f\"Step 4: Learning {num_merges} BPE merges...\")\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"  BPE merge {i}/{num_merges}\")\n",
    "            \n",
    "            pairs = self.get_stats(vocab)\n",
    "            if not pairs:\n",
    "                print(f\"  No more pairs to merge at iteration {i}\")\n",
    "                break\n",
    "            \n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = self.merge_vocab(best, vocab)\n",
    "            self.bpe_codes[best] = i\n",
    "        \n",
    "        # Build final vocabulary\n",
    "        print(\"Step 5: Building final vocabulary...\")\n",
    "        for word in vocab.keys():\n",
    "            for token in word.split():\n",
    "                if token not in self.word2idx:\n",
    "                    idx = len(self.word2idx)\n",
    "                    self.word2idx[token] = idx\n",
    "                    self.idx2word[idx] = token\n",
    "        \n",
    "        print(f\"‚úì BPE Tokenizer ready! Vocabulary size: {len(self.word2idx)}\")\n",
    "        print()\n",
    "    \n",
    "    def apply_bpe(self, word):\n",
    "        \"\"\"Apply BPE codes to a word\"\"\"\n",
    "        word = ' '.join(list(word)) + ' </w>'\n",
    "        \n",
    "        while True:\n",
    "            pairs = [(word[i:i+2], i) for i in range(len(word.split())-1)]\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Find the pair with lowest merge order\n",
    "            bigrams = [(' '.join([word.split()[i], word.split()[i+1]]), i) \n",
    "                      for i in range(len(word.split())-1)]\n",
    "            \n",
    "            valid_bigrams = [(self.bpe_codes.get(tuple(bg.split())), bg, pos) \n",
    "                           for bg, pos in bigrams \n",
    "                           if tuple(bg.split()) in self.bpe_codes]\n",
    "            \n",
    "            if not valid_bigrams:\n",
    "                break\n",
    "            \n",
    "            # Merge the pair with lowest index (learned earliest)\n",
    "            _, bigram, pos = min(valid_bigrams)\n",
    "            word_list = word.split()\n",
    "            word_list[pos] = ''.join(bigram.split())\n",
    "            del word_list[pos + 1]\n",
    "            word = ' '.join(word_list)\n",
    "        \n",
    "        return word.split()\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to token IDs\"\"\"\n",
    "        tokens = []\n",
    "        for word in text.lower().split():\n",
    "            bpe_tokens = self.apply_bpe(word)\n",
    "            for token in bpe_tokens:\n",
    "                tokens.append(self.word2idx.get(token, 3))\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Decode token IDs to text\"\"\"\n",
    "        words = []\n",
    "        current_word = \"\"\n",
    "        \n",
    "        for i in ids:\n",
    "            if i == 2:  # eos\n",
    "                break\n",
    "            if i > 3:\n",
    "                token = self.idx2word.get(i, \"<unk>\")\n",
    "                if token.endswith('</w>'):\n",
    "                    current_word += token[:-4]\n",
    "                    words.append(current_word)\n",
    "                    current_word = \"\"\n",
    "                else:\n",
    "                    current_word += token\n",
    "        \n",
    "        if current_word:\n",
    "            words.append(current_word)\n",
    "        \n",
    "        return \" \".join(words)\n",
    "\n",
    "\n",
    "print(\"Building source BPE tokenizer...\")\n",
    "tok_src = BPETokenizer(train_en, vocab_size=5000, min_freq=2, max_samples=50000)\n",
    "\n",
    "print(\"Building target BPE tokenizer...\")\n",
    "tok_trg = BPETokenizer(train_vi, vocab_size=5000, min_freq=2, max_samples=50000)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 4Ô∏è‚É£ DATA AUGMENTATION\n",
    "# ==============================\n",
    "class DataAugmentation:\n",
    "    @staticmethod\n",
    "    def random_swap(text, n=1):\n",
    "        \"\"\"Randomly swap n words in the text\"\"\"\n",
    "        words = text.split()\n",
    "        if len(words) < 2:\n",
    "            return text\n",
    "        \n",
    "        for _ in range(n):\n",
    "            idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "            words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "        \n",
    "        return ' '.join(words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_deletion(text, p=0.1):\n",
    "        \"\"\"Randomly delete words with probability p\"\"\"\n",
    "        words = text.split()\n",
    "        if len(words) == 1:\n",
    "            return text\n",
    "        \n",
    "        new_words = [word for word in words if random.random() > p]\n",
    "        \n",
    "        if len(new_words) == 0:\n",
    "            return random.choice(words)\n",
    "        \n",
    "        return ' '.join(new_words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def augment(text, method='swap'):\n",
    "        \"\"\"Apply augmentation method\"\"\"\n",
    "        if method == 'swap':\n",
    "            return DataAugmentation.random_swap(text, n=1)\n",
    "        elif method == 'delete':\n",
    "            return DataAugmentation.random_deletion(text, p=0.1)\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 5Ô∏è‚É£ DATASET + collate (with augmentation)\n",
    "# ==============================\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src, trg, tok_src, tok_trg, augment=False):\n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.tok_src = tok_src\n",
    "        self.tok_trg = tok_trg\n",
    "        self.augment = augment\n",
    "        self.aug = DataAugmentation()\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src[idx]\n",
    "        trg_text = self.trg[idx]\n",
    "        \n",
    "        # Apply augmentation with 30% probability during training\n",
    "        if self.augment and random.random() < 0.3:\n",
    "            method = random.choice(['swap', 'delete'])\n",
    "            src_text = self.aug.augment(src_text, method)\n",
    "            trg_text = self.aug.augment(trg_text, method)\n",
    "        \n",
    "        s = [1] + self.tok_src.encode(src_text) + [2]\n",
    "        t = [1] + self.tok_trg.encode(trg_text) + [2]\n",
    "        return torch.tensor(s), torch.tensor(t)\n",
    "\n",
    "def collate_fn(batch, pad_idx=0):\n",
    "    src, trg = zip(*batch)\n",
    "    src = nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=0)\n",
    "    trg = nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=0)\n",
    "    return src, trg\n",
    "\n",
    "\n",
    "dataset = TranslationDataset(train_en, train_vi, tok_src, tok_trg, augment=True)\n",
    "print(f\"Total dataset size: {len(dataset)}\")\n",
    "\n",
    "train_len = int(0.9 * len(dataset))\n",
    "val_len = len(dataset) - train_len\n",
    "train_set, val_set = random_split(dataset, [train_len, val_len])\n",
    "print(f\"Train: {train_len}, Val: {val_len}\")\n",
    "\n",
    "# Disable augmentation for validation\n",
    "val_set.dataset.augment = False\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_set, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 6Ô∏è‚É£ POSITIONAL ENCODING\n",
    "# ==============================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 7Ô∏è‚É£ LABEL SMOOTHING LOSS\n",
    "# ==============================\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, num_classes, smoothing=0.1, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_index = ignore_index\n",
    "        self.confidence = 1.0 - smoothing\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        pred: (batch_size * seq_len, num_classes)\n",
    "        target: (batch_size * seq_len)\n",
    "        \"\"\"\n",
    "        pred = pred.log_softmax(dim=-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.num_classes - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "            true_dist[:, self.ignore_index] = 0\n",
    "            \n",
    "            mask = torch.nonzero(target == self.ignore_index, as_tuple=False)\n",
    "            if mask.dim() > 0 and mask.size(0) > 0:\n",
    "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=-1))\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 8Ô∏è‚É£ TRANSFORMER MODEL\n",
    "# ==============================\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model=256, nhead=4, num_layers=3, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        self.src_emb = nn.Embedding(src_vocab, d_model, padding_idx=pad_idx)\n",
    "        self.trg_emb = nn.Embedding(trg_vocab, d_model, padding_idx=pad_idx)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(d_model, trg_vocab)\n",
    "        self.fc.weight = self.trg_emb.weight   # weight tying\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        device = src.device\n",
    "        src_mask = (src == self.pad_idx)\n",
    "        trg_mask = (trg == self.pad_idx)\n",
    "\n",
    "        seq_len = trg.size(1)\n",
    "        subsequent_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(device)\n",
    "\n",
    "        src_emb = self.pos(self.src_emb(src))\n",
    "        trg_emb = self.pos(self.trg_emb(trg))\n",
    "\n",
    "        out = self.transformer(\n",
    "            src_emb, trg_emb,\n",
    "            tgt_mask=subsequent_mask,\n",
    "            src_key_padding_mask=src_mask,\n",
    "            tgt_key_padding_mask=trg_mask,\n",
    "            memory_key_padding_mask=src_mask\n",
    "        )\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 9Ô∏è‚É£ EARLY STOPPING\n",
    "# ==============================\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001, mode='min'):\n",
    "        \"\"\"\n",
    "        patience: number of epochs to wait before stopping\n",
    "        min_delta: minimum change to qualify as improvement\n",
    "        mode: 'min' for loss, 'max' for accuracy\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_metric):\n",
    "        score = -val_metric if self.mode == 'min' else val_metric\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"     ‚ö† EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# üîü TRAINING (with AdamW + Scheduler + Early Stopping)\n",
    "# ==============================\n",
    "def train_model(model, train_loader, val_loader, device, epochs=20, lr=3e-4, pad_idx=0, \n",
    "                patience=5, warmup_epochs=2):\n",
    "    model.to(device)\n",
    "    print(f\"Model moved to {device}\")\n",
    "    \n",
    "    # AdamW optimizer with weight decay\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01, betas=(0.9, 0.98))\n",
    "    \n",
    "    # Multi-step learning rate scheduler with warmup\n",
    "    warmup_steps = warmup_epochs * len(train_loader)\n",
    "    total_steps = epochs * len(train_loader)\n",
    "    \n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            # Linear warmup\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        # Cosine decay after warmup\n",
    "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "    \n",
    "    # Label smoothing loss\n",
    "    loss_fn = LabelSmoothingLoss(\n",
    "        num_classes=len(tok_trg.word2idx), \n",
    "        smoothing=0.1, \n",
    "        ignore_index=pad_idx\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=0.001, mode='min')\n",
    "    \n",
    "    print(f\"Optimizer: AdamW (lr={lr}, weight_decay=0.01)\")\n",
    "    print(f\"Scheduler: Warmup + Cosine Decay (warmup={warmup_epochs} epochs)\")\n",
    "    print(f\"Loss: LabelSmoothing(0.1)\")\n",
    "    print(f\"Early Stopping: patience={patience}\")\n",
    "    print()\n",
    "\n",
    "    best_val = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Epoch {ep}/{epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        for batch_idx, (src, trg) in enumerate(train_loader):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            opt.zero_grad()\n",
    "\n",
    "            out = model(src, trg[:, :-1])\n",
    "            loss = loss_fn(out.reshape(-1, out.size(-1)), trg[:, 1:].reshape(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            scheduler.step()  # Update learning rate every batch\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Print progress every 100 batches\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                avg_loss = total_loss / batch_count\n",
    "                current_lr = opt.param_groups[0]['lr']\n",
    "                print(f\"  Batch {batch_idx+1}/{len(train_loader)} | Loss: {avg_loss:.4f} | LR: {current_lr:.6f}\")\n",
    "\n",
    "        # validation\n",
    "        print(f\"\\n  Running validation...\")\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for src, trg in val_loader:\n",
    "                src, trg = src.to(device), trg.to(device)\n",
    "                out = model(src, trg[:, :-1])\n",
    "                loss = loss_fn(out.reshape(-1, out.size(-1)), trg[:, 1:].reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_train = total_loss / len(train_loader)\n",
    "        avg_val = val_loss / len(val_loader)\n",
    "        current_lr = opt.param_groups[0]['lr']\n",
    "        \n",
    "        train_losses.append(avg_train)\n",
    "        val_losses.append(avg_val)\n",
    "\n",
    "        print(f\"\\n  üìä Epoch {ep} Summary:\")\n",
    "        print(f\"     Train Loss: {avg_train:.4f}\")\n",
    "        print(f\"     Val Loss:   {avg_val:.4f}\")\n",
    "        print(f\"     LR:         {current_lr:.6f}\")\n",
    "\n",
    "        if avg_val < best_val:\n",
    "            best_val = avg_val\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(f\"     ‚úî New best model saved! (Val Loss: {best_val:.4f})\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(avg_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"\\nüõë Early stopping triggered at epoch {ep}!\")\n",
    "            print(f\"   Best Val Loss: {best_val:.4f}\")\n",
    "            break\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Plot training history\n",
    "    print(\"\\nüìà Training History:\")\n",
    "    print(f\"Best Val Loss: {best_val:.4f} at epoch {val_losses.index(min(val_losses)) + 1}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# üîü BEAM SEARCH DECODING\n",
    "# ==============================\n",
    "def beam_search_decode(model, src, tok_trg, device, beam_size=5, max_len=60):\n",
    "    model.eval()\n",
    "\n",
    "    sos = 1\n",
    "    eos = 2\n",
    "\n",
    "    memory_src = src.to(device)\n",
    "\n",
    "    # Encode\n",
    "    with torch.no_grad():\n",
    "        src_mask = (memory_src == 0)\n",
    "        src_emb = model.pos(model.src_emb(memory_src))\n",
    "        memory = model.transformer.encoder(src_emb, src_key_padding_mask=src_mask)\n",
    "\n",
    "    sequences = [[sos]]\n",
    "    scores = [0]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "\n",
    "        for i in range(len(sequences)):\n",
    "            seq = sequences[i]\n",
    "            score = scores[i]\n",
    "\n",
    "            if seq[-1] == eos:\n",
    "                all_candidates.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            trg = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(trg.size(1)).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                trg_emb = model.pos(model.trg_emb(trg))\n",
    "                out = model.transformer.decoder(\n",
    "                    trg_emb, memory,\n",
    "                    tgt_mask=tgt_mask,\n",
    "                    memory_key_padding_mask=src_mask\n",
    "                )\n",
    "                logits = model.fc(out[:, -1])  # last token\n",
    "                log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "            topk = torch.topk(log_probs, beam_size)\n",
    "            next_tokens = topk.indices[0]\n",
    "            next_scores = topk.values[0]\n",
    "\n",
    "            for k in range(beam_size):\n",
    "                candidate = seq + [next_tokens[k].item()]\n",
    "                candidate_score = score + next_scores[k].item()\n",
    "                all_candidates.append((candidate, candidate_score))\n",
    "\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        sequences = []\n",
    "        scores = []\n",
    "        for i in range(beam_size):\n",
    "            sequences.append(ordered[i][0])\n",
    "            scores.append(ordered[i][1])\n",
    "\n",
    "    best_seq = sequences[0]\n",
    "    return tok_trg.decode(best_seq[1:])\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ TRANSLATE (using BEAM SEARCH)\n",
    "# ==============================\n",
    "def translate(model, text, tok_src, tok_trg, device, beam_size=5):\n",
    "    src = [1] + tok_src.encode(text) + [2]\n",
    "    src = torch.tensor(src, dtype=torch.long).unsqueeze(0)\n",
    "    return beam_search_decode(model, src, tok_trg, device, beam_size=beam_size)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ EVALUATE BLEU\n",
    "# ==============================\n",
    "def evaluate_test_set(model, test_en, test_vi, tok_src, tok_trg, device, n=50):\n",
    "    model.eval()\n",
    "    smooth = SmoothingFunction().method1\n",
    "\n",
    "    total_bleu = 0\n",
    "    n = min(n, len(test_en))\n",
    "\n",
    "    for i in range(n):\n",
    "        pred = translate(model, test_en[i], tok_src, tok_trg, device, beam_size=5)\n",
    "        bleu = sentence_bleu([test_vi[i].split()], pred.split(), smoothing_function=smooth)\n",
    "        total_bleu += bleu\n",
    "\n",
    "        if i < 10:\n",
    "            print(\"\\nEN:\", test_en[i])\n",
    "            print(\"GT:\", test_vi[i])\n",
    "            print(\"PR:\", pred)\n",
    "            print(\"BLEU:\", bleu)\n",
    "\n",
    "    print(\"\\nAVERAGE BLEU =\", total_bleu / n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef12946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:29:44.240830Z",
     "iopub.status.busy": "2025-12-14T17:29:44.240498Z",
     "iopub.status.idle": "2025-12-14T20:14:43.309419Z",
     "shell.execute_reply": "2025-12-14T20:14:43.308693Z"
    },
    "papermill": {
     "duration": 9899.076687,
     "end_time": "2025-12-14T20:14:43.310986",
     "exception": false,
     "start_time": "2025-12-14T17:29:44.234299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Model parameters: 13,365,103\n",
      "\n",
      "==================================================\n",
      "TRAINING\n",
      "==================================================\n",
      "Model moved to cuda\n",
      "Optimizer: AdamW (lr=0.0003, weight_decay=0.01)\n",
      "Scheduler: Warmup + Cosine Decay (warmup=2 epochs)\n",
      "Loss: LabelSmoothing(0.1)\n",
      "Early Stopping: patience=5\n",
      "\n",
      "============================================================\n",
      "Epoch 1/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100/3750 | Loss: 20.0097 | LR: 0.000004\n",
      "  Batch 200/3750 | Loss: 16.3349 | LR: 0.000008\n",
      "  Batch 300/3750 | Loss: 14.2925 | LR: 0.000012\n",
      "  Batch 400/3750 | Loss: 12.8346 | LR: 0.000016\n",
      "  Batch 500/3750 | Loss: 11.9500 | LR: 0.000020\n",
      "  Batch 600/3750 | Loss: 11.2478 | LR: 0.000024\n",
      "  Batch 700/3750 | Loss: 10.7148 | LR: 0.000028\n",
      "  Batch 800/3750 | Loss: 10.2561 | LR: 0.000032\n",
      "  Batch 900/3750 | Loss: 9.8490 | LR: 0.000036\n",
      "  Batch 1000/3750 | Loss: 9.4868 | LR: 0.000040\n",
      "  Batch 1100/3750 | Loss: 9.1487 | LR: 0.000044\n",
      "  Batch 1200/3750 | Loss: 8.8646 | LR: 0.000048\n",
      "  Batch 1300/3750 | Loss: 8.5964 | LR: 0.000052\n",
      "  Batch 1400/3750 | Loss: 8.3433 | LR: 0.000056\n",
      "  Batch 1500/3750 | Loss: 8.0958 | LR: 0.000060\n",
      "  Batch 1600/3750 | Loss: 7.8559 | LR: 0.000064\n",
      "  Batch 1700/3750 | Loss: 7.6421 | LR: 0.000068\n",
      "  Batch 1800/3750 | Loss: 7.4383 | LR: 0.000072\n",
      "  Batch 1900/3750 | Loss: 7.2412 | LR: 0.000076\n",
      "  Batch 2000/3750 | Loss: 7.0528 | LR: 0.000080\n",
      "  Batch 2100/3750 | Loss: 6.8766 | LR: 0.000084\n",
      "  Batch 2200/3750 | Loss: 6.7070 | LR: 0.000088\n",
      "  Batch 2300/3750 | Loss: 6.5469 | LR: 0.000092\n",
      "  Batch 2400/3750 | Loss: 6.3895 | LR: 0.000096\n",
      "  Batch 2500/3750 | Loss: 6.2418 | LR: 0.000100\n",
      "  Batch 2600/3750 | Loss: 6.1051 | LR: 0.000104\n",
      "  Batch 2700/3750 | Loss: 5.9702 | LR: 0.000108\n",
      "  Batch 2800/3750 | Loss: 5.8508 | LR: 0.000112\n",
      "  Batch 2900/3750 | Loss: 5.7317 | LR: 0.000116\n",
      "  Batch 3000/3750 | Loss: 5.6149 | LR: 0.000120\n",
      "  Batch 3100/3750 | Loss: 5.5085 | LR: 0.000124\n",
      "  Batch 3200/3750 | Loss: 5.4108 | LR: 0.000128\n",
      "  Batch 3300/3750 | Loss: 5.3137 | LR: 0.000132\n",
      "  Batch 3400/3750 | Loss: 5.2206 | LR: 0.000136\n",
      "  Batch 3500/3750 | Loss: 5.1338 | LR: 0.000140\n",
      "  Batch 3600/3750 | Loss: 5.0503 | LR: 0.000144\n",
      "  Batch 3700/3750 | Loss: 4.9699 | LR: 0.000148\n",
      "\n",
      "  Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  üìä Epoch 1 Summary:\n",
      "     Train Loss: 4.9331\n",
      "     Val Loss:   1.9641\n",
      "     LR:         0.000150\n",
      "     ‚úî New best model saved! (Val Loss: 1.9641)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 2.1444 | LR: 0.000154\n",
      "  Batch 200/3750 | Loss: 2.1018 | LR: 0.000158\n",
      "  Batch 300/3750 | Loss: 2.0671 | LR: 0.000162\n",
      "  Batch 400/3750 | Loss: 2.0653 | LR: 0.000166\n",
      "  Batch 500/3750 | Loss: 2.0628 | LR: 0.000170\n",
      "  Batch 600/3750 | Loss: 2.0582 | LR: 0.000174\n",
      "  Batch 700/3750 | Loss: 2.0498 | LR: 0.000178\n",
      "  Batch 800/3750 | Loss: 2.0304 | LR: 0.000182\n",
      "  Batch 900/3750 | Loss: 2.0262 | LR: 0.000186\n",
      "  Batch 1000/3750 | Loss: 2.0164 | LR: 0.000190\n",
      "  Batch 1100/3750 | Loss: 2.0096 | LR: 0.000194\n",
      "  Batch 1200/3750 | Loss: 2.0052 | LR: 0.000198\n",
      "  Batch 1300/3750 | Loss: 1.9988 | LR: 0.000202\n",
      "  Batch 1400/3750 | Loss: 1.9885 | LR: 0.000206\n",
      "  Batch 1500/3750 | Loss: 1.9807 | LR: 0.000210\n",
      "  Batch 1600/3750 | Loss: 1.9779 | LR: 0.000214\n",
      "  Batch 1700/3750 | Loss: 1.9705 | LR: 0.000218\n",
      "  Batch 1800/3750 | Loss: 1.9671 | LR: 0.000222\n",
      "  Batch 1900/3750 | Loss: 1.9617 | LR: 0.000226\n",
      "  Batch 2000/3750 | Loss: 1.9540 | LR: 0.000230\n",
      "  Batch 2100/3750 | Loss: 1.9499 | LR: 0.000234\n",
      "  Batch 2200/3750 | Loss: 1.9459 | LR: 0.000238\n",
      "  Batch 2300/3750 | Loss: 1.9413 | LR: 0.000242\n",
      "  Batch 2400/3750 | Loss: 1.9386 | LR: 0.000246\n",
      "  Batch 2500/3750 | Loss: 1.9358 | LR: 0.000250\n",
      "  Batch 2600/3750 | Loss: 1.9307 | LR: 0.000254\n",
      "  Batch 2700/3750 | Loss: 1.9258 | LR: 0.000258\n",
      "  Batch 2800/3750 | Loss: 1.9240 | LR: 0.000262\n",
      "  Batch 2900/3750 | Loss: 1.9189 | LR: 0.000266\n",
      "  Batch 3000/3750 | Loss: 1.9159 | LR: 0.000270\n",
      "  Batch 3100/3750 | Loss: 1.9114 | LR: 0.000274\n",
      "  Batch 3200/3750 | Loss: 1.9089 | LR: 0.000278\n",
      "  Batch 3300/3750 | Loss: 1.9046 | LR: 0.000282\n",
      "  Batch 3400/3750 | Loss: 1.9022 | LR: 0.000286\n",
      "  Batch 3500/3750 | Loss: 1.8988 | LR: 0.000290\n",
      "  Batch 3600/3750 | Loss: 1.8949 | LR: 0.000294\n",
      "  Batch 3700/3750 | Loss: 1.8895 | LR: 0.000298\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 2 Summary:\n",
      "     Train Loss: 1.8875\n",
      "     Val Loss:   1.6731\n",
      "     LR:         0.000300\n",
      "     ‚úî New best model saved! (Val Loss: 1.6731)\n",
      "\n",
      "============================================================\n",
      "Epoch 3/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.7072 | LR: 0.000300\n",
      "  Batch 200/3750 | Loss: 1.7085 | LR: 0.000300\n",
      "  Batch 300/3750 | Loss: 1.6907 | LR: 0.000300\n",
      "  Batch 400/3750 | Loss: 1.6843 | LR: 0.000300\n",
      "  Batch 500/3750 | Loss: 1.6755 | LR: 0.000300\n",
      "  Batch 600/3750 | Loss: 1.6764 | LR: 0.000300\n",
      "  Batch 700/3750 | Loss: 1.6710 | LR: 0.000300\n",
      "  Batch 800/3750 | Loss: 1.6647 | LR: 0.000300\n",
      "  Batch 900/3750 | Loss: 1.6670 | LR: 0.000300\n",
      "  Batch 1000/3750 | Loss: 1.6672 | LR: 0.000300\n",
      "  Batch 1100/3750 | Loss: 1.6678 | LR: 0.000300\n",
      "  Batch 1200/3750 | Loss: 1.6659 | LR: 0.000300\n",
      "  Batch 1300/3750 | Loss: 1.6644 | LR: 0.000300\n",
      "  Batch 1400/3750 | Loss: 1.6623 | LR: 0.000300\n",
      "  Batch 1500/3750 | Loss: 1.6619 | LR: 0.000300\n",
      "  Batch 1600/3750 | Loss: 1.6599 | LR: 0.000300\n",
      "  Batch 1700/3750 | Loss: 1.6583 | LR: 0.000300\n",
      "  Batch 1800/3750 | Loss: 1.6563 | LR: 0.000299\n",
      "  Batch 1900/3750 | Loss: 1.6511 | LR: 0.000299\n",
      "  Batch 2000/3750 | Loss: 1.6471 | LR: 0.000299\n",
      "  Batch 2100/3750 | Loss: 1.6445 | LR: 0.000299\n",
      "  Batch 2200/3750 | Loss: 1.6399 | LR: 0.000299\n",
      "  Batch 2300/3750 | Loss: 1.6362 | LR: 0.000299\n",
      "  Batch 2400/3750 | Loss: 1.6345 | LR: 0.000299\n",
      "  Batch 2500/3750 | Loss: 1.6327 | LR: 0.000299\n",
      "  Batch 2600/3750 | Loss: 1.6297 | LR: 0.000299\n",
      "  Batch 2700/3750 | Loss: 1.6271 | LR: 0.000299\n",
      "  Batch 2800/3750 | Loss: 1.6261 | LR: 0.000299\n",
      "  Batch 2900/3750 | Loss: 1.6240 | LR: 0.000299\n",
      "  Batch 3000/3750 | Loss: 1.6227 | LR: 0.000299\n",
      "  Batch 3100/3750 | Loss: 1.6206 | LR: 0.000298\n",
      "  Batch 3200/3750 | Loss: 1.6174 | LR: 0.000298\n",
      "  Batch 3300/3750 | Loss: 1.6149 | LR: 0.000298\n",
      "  Batch 3400/3750 | Loss: 1.6130 | LR: 0.000298\n",
      "  Batch 3500/3750 | Loss: 1.6100 | LR: 0.000298\n",
      "  Batch 3600/3750 | Loss: 1.6086 | LR: 0.000298\n",
      "  Batch 3700/3750 | Loss: 1.6057 | LR: 0.000298\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 3 Summary:\n",
      "     Train Loss: 1.6051\n",
      "     Val Loss:   1.4778\n",
      "     LR:         0.000298\n",
      "     ‚úî New best model saved! (Val Loss: 1.4778)\n",
      "\n",
      "============================================================\n",
      "Epoch 4/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.4894 | LR: 0.000298\n",
      "  Batch 200/3750 | Loss: 1.5183 | LR: 0.000297\n",
      "  Batch 300/3750 | Loss: 1.4878 | LR: 0.000297\n",
      "  Batch 400/3750 | Loss: 1.4869 | LR: 0.000297\n",
      "  Batch 500/3750 | Loss: 1.4946 | LR: 0.000297\n",
      "  Batch 600/3750 | Loss: 1.4919 | LR: 0.000297\n",
      "  Batch 700/3750 | Loss: 1.4887 | LR: 0.000297\n",
      "  Batch 800/3750 | Loss: 1.4886 | LR: 0.000297\n",
      "  Batch 900/3750 | Loss: 1.4872 | LR: 0.000297\n",
      "  Batch 1000/3750 | Loss: 1.4928 | LR: 0.000296\n",
      "  Batch 1100/3750 | Loss: 1.4906 | LR: 0.000296\n",
      "  Batch 1200/3750 | Loss: 1.4926 | LR: 0.000296\n",
      "  Batch 1300/3750 | Loss: 1.4922 | LR: 0.000296\n",
      "  Batch 1400/3750 | Loss: 1.4921 | LR: 0.000296\n",
      "  Batch 1500/3750 | Loss: 1.4884 | LR: 0.000296\n",
      "  Batch 1600/3750 | Loss: 1.4851 | LR: 0.000295\n",
      "  Batch 1700/3750 | Loss: 1.4864 | LR: 0.000295\n",
      "  Batch 1800/3750 | Loss: 1.4818 | LR: 0.000295\n",
      "  Batch 1900/3750 | Loss: 1.4817 | LR: 0.000295\n",
      "  Batch 2000/3750 | Loss: 1.4814 | LR: 0.000295\n",
      "  Batch 2100/3750 | Loss: 1.4781 | LR: 0.000294\n",
      "  Batch 2200/3750 | Loss: 1.4746 | LR: 0.000294\n",
      "  Batch 2300/3750 | Loss: 1.4750 | LR: 0.000294\n",
      "  Batch 2400/3750 | Loss: 1.4748 | LR: 0.000294\n",
      "  Batch 2500/3750 | Loss: 1.4735 | LR: 0.000294\n",
      "  Batch 2600/3750 | Loss: 1.4713 | LR: 0.000293\n",
      "  Batch 2700/3750 | Loss: 1.4702 | LR: 0.000293\n",
      "  Batch 2800/3750 | Loss: 1.4683 | LR: 0.000293\n",
      "  Batch 2900/3750 | Loss: 1.4663 | LR: 0.000293\n",
      "  Batch 3000/3750 | Loss: 1.4649 | LR: 0.000293\n",
      "  Batch 3100/3750 | Loss: 1.4622 | LR: 0.000292\n",
      "  Batch 3200/3750 | Loss: 1.4603 | LR: 0.000292\n",
      "  Batch 3300/3750 | Loss: 1.4591 | LR: 0.000292\n",
      "  Batch 3400/3750 | Loss: 1.4589 | LR: 0.000292\n",
      "  Batch 3500/3750 | Loss: 1.4576 | LR: 0.000292\n",
      "  Batch 3600/3750 | Loss: 1.4561 | LR: 0.000291\n",
      "  Batch 3700/3750 | Loss: 1.4552 | LR: 0.000291\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 4 Summary:\n",
      "     Train Loss: 1.4552\n",
      "     Val Loss:   1.3634\n",
      "     LR:         0.000291\n",
      "     ‚úî New best model saved! (Val Loss: 1.3634)\n",
      "\n",
      "============================================================\n",
      "Epoch 5/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.3440 | LR: 0.000291\n",
      "  Batch 200/3750 | Loss: 1.3975 | LR: 0.000290\n",
      "  Batch 300/3750 | Loss: 1.3902 | LR: 0.000290\n",
      "  Batch 400/3750 | Loss: 1.3762 | LR: 0.000290\n",
      "  Batch 500/3750 | Loss: 1.3779 | LR: 0.000290\n",
      "  Batch 600/3750 | Loss: 1.3716 | LR: 0.000289\n",
      "  Batch 700/3750 | Loss: 1.3733 | LR: 0.000289\n",
      "  Batch 800/3750 | Loss: 1.3783 | LR: 0.000289\n",
      "  Batch 900/3750 | Loss: 1.3774 | LR: 0.000289\n",
      "  Batch 1000/3750 | Loss: 1.3820 | LR: 0.000288\n",
      "  Batch 1100/3750 | Loss: 1.3825 | LR: 0.000288\n",
      "  Batch 1200/3750 | Loss: 1.3837 | LR: 0.000288\n",
      "  Batch 1300/3750 | Loss: 1.3859 | LR: 0.000288\n",
      "  Batch 1400/3750 | Loss: 1.3823 | LR: 0.000287\n",
      "  Batch 1500/3750 | Loss: 1.3783 | LR: 0.000287\n",
      "  Batch 1600/3750 | Loss: 1.3791 | LR: 0.000287\n",
      "  Batch 1700/3750 | Loss: 1.3743 | LR: 0.000286\n",
      "  Batch 1800/3750 | Loss: 1.3747 | LR: 0.000286\n",
      "  Batch 1900/3750 | Loss: 1.3740 | LR: 0.000286\n",
      "  Batch 2000/3750 | Loss: 1.3738 | LR: 0.000286\n",
      "  Batch 2100/3750 | Loss: 1.3716 | LR: 0.000285\n",
      "  Batch 2200/3750 | Loss: 1.3708 | LR: 0.000285\n",
      "  Batch 2300/3750 | Loss: 1.3687 | LR: 0.000285\n",
      "  Batch 2400/3750 | Loss: 1.3671 | LR: 0.000284\n",
      "  Batch 2500/3750 | Loss: 1.3671 | LR: 0.000284\n",
      "  Batch 2600/3750 | Loss: 1.3673 | LR: 0.000284\n",
      "  Batch 2700/3750 | Loss: 1.3683 | LR: 0.000283\n",
      "  Batch 2800/3750 | Loss: 1.3694 | LR: 0.000283\n",
      "  Batch 2900/3750 | Loss: 1.3667 | LR: 0.000283\n",
      "  Batch 3000/3750 | Loss: 1.3641 | LR: 0.000282\n",
      "  Batch 3100/3750 | Loss: 1.3624 | LR: 0.000282\n",
      "  Batch 3200/3750 | Loss: 1.3617 | LR: 0.000282\n",
      "  Batch 3300/3750 | Loss: 1.3609 | LR: 0.000281\n",
      "  Batch 3400/3750 | Loss: 1.3597 | LR: 0.000281\n",
      "  Batch 3500/3750 | Loss: 1.3601 | LR: 0.000281\n",
      "  Batch 3600/3750 | Loss: 1.3588 | LR: 0.000280\n",
      "  Batch 3700/3750 | Loss: 1.3577 | LR: 0.000280\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 5 Summary:\n",
      "     Train Loss: 1.3573\n",
      "     Val Loss:   1.2905\n",
      "     LR:         0.000280\n",
      "     ‚úî New best model saved! (Val Loss: 1.2905)\n",
      "\n",
      "============================================================\n",
      "Epoch 6/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.2597 | LR: 0.000280\n",
      "  Batch 200/3750 | Loss: 1.2910 | LR: 0.000279\n",
      "  Batch 300/3750 | Loss: 1.2863 | LR: 0.000279\n",
      "  Batch 400/3750 | Loss: 1.2865 | LR: 0.000278\n",
      "  Batch 500/3750 | Loss: 1.2857 | LR: 0.000278\n",
      "  Batch 600/3750 | Loss: 1.2835 | LR: 0.000278\n",
      "  Batch 700/3750 | Loss: 1.2822 | LR: 0.000277\n",
      "  Batch 800/3750 | Loss: 1.2869 | LR: 0.000277\n",
      "  Batch 900/3750 | Loss: 1.2909 | LR: 0.000277\n",
      "  Batch 1000/3750 | Loss: 1.2941 | LR: 0.000276\n",
      "  Batch 1100/3750 | Loss: 1.2913 | LR: 0.000276\n",
      "  Batch 1200/3750 | Loss: 1.2908 | LR: 0.000276\n",
      "  Batch 1300/3750 | Loss: 1.2922 | LR: 0.000275\n",
      "  Batch 1400/3750 | Loss: 1.2912 | LR: 0.000275\n",
      "  Batch 1500/3750 | Loss: 1.2928 | LR: 0.000274\n",
      "  Batch 1600/3750 | Loss: 1.2890 | LR: 0.000274\n",
      "  Batch 1700/3750 | Loss: 1.2907 | LR: 0.000274\n",
      "  Batch 1800/3750 | Loss: 1.2900 | LR: 0.000273\n",
      "  Batch 1900/3750 | Loss: 1.2909 | LR: 0.000273\n",
      "  Batch 2000/3750 | Loss: 1.2905 | LR: 0.000272\n",
      "  Batch 2100/3750 | Loss: 1.2903 | LR: 0.000272\n",
      "  Batch 2200/3750 | Loss: 1.2921 | LR: 0.000272\n",
      "  Batch 2300/3750 | Loss: 1.2922 | LR: 0.000271\n",
      "  Batch 2400/3750 | Loss: 1.2919 | LR: 0.000271\n",
      "  Batch 2500/3750 | Loss: 1.2930 | LR: 0.000270\n",
      "  Batch 2600/3750 | Loss: 1.2948 | LR: 0.000270\n",
      "  Batch 2700/3750 | Loss: 1.2939 | LR: 0.000269\n",
      "  Batch 2800/3750 | Loss: 1.2932 | LR: 0.000269\n",
      "  Batch 2900/3750 | Loss: 1.2932 | LR: 0.000269\n",
      "  Batch 3000/3750 | Loss: 1.2936 | LR: 0.000268\n",
      "  Batch 3100/3750 | Loss: 1.2947 | LR: 0.000268\n",
      "  Batch 3200/3750 | Loss: 1.2945 | LR: 0.000267\n",
      "  Batch 3300/3750 | Loss: 1.2936 | LR: 0.000267\n",
      "  Batch 3400/3750 | Loss: 1.2926 | LR: 0.000266\n",
      "  Batch 3500/3750 | Loss: 1.2905 | LR: 0.000266\n",
      "  Batch 3600/3750 | Loss: 1.2904 | LR: 0.000266\n",
      "  Batch 3700/3750 | Loss: 1.2901 | LR: 0.000265\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 6 Summary:\n",
      "     Train Loss: 1.2894\n",
      "     Val Loss:   1.2355\n",
      "     LR:         0.000265\n",
      "     ‚úî New best model saved! (Val Loss: 1.2355)\n",
      "\n",
      "============================================================\n",
      "Epoch 7/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.2172 | LR: 0.000264\n",
      "  Batch 200/3750 | Loss: 1.2272 | LR: 0.000264\n",
      "  Batch 300/3750 | Loss: 1.2399 | LR: 0.000264\n",
      "  Batch 400/3750 | Loss: 1.2402 | LR: 0.000263\n",
      "  Batch 500/3750 | Loss: 1.2434 | LR: 0.000263\n",
      "  Batch 600/3750 | Loss: 1.2418 | LR: 0.000262\n",
      "  Batch 700/3750 | Loss: 1.2448 | LR: 0.000262\n",
      "  Batch 800/3750 | Loss: 1.2465 | LR: 0.000261\n",
      "  Batch 900/3750 | Loss: 1.2488 | LR: 0.000261\n",
      "  Batch 1000/3750 | Loss: 1.2506 | LR: 0.000260\n",
      "  Batch 1100/3750 | Loss: 1.2480 | LR: 0.000260\n",
      "  Batch 1200/3750 | Loss: 1.2461 | LR: 0.000259\n",
      "  Batch 1300/3750 | Loss: 1.2487 | LR: 0.000259\n",
      "  Batch 1400/3750 | Loss: 1.2473 | LR: 0.000258\n",
      "  Batch 1500/3750 | Loss: 1.2502 | LR: 0.000258\n",
      "  Batch 1600/3750 | Loss: 1.2494 | LR: 0.000257\n",
      "  Batch 1700/3750 | Loss: 1.2462 | LR: 0.000257\n",
      "  Batch 1800/3750 | Loss: 1.2461 | LR: 0.000256\n",
      "  Batch 1900/3750 | Loss: 1.2468 | LR: 0.000256\n",
      "  Batch 2000/3750 | Loss: 1.2445 | LR: 0.000255\n",
      "  Batch 2100/3750 | Loss: 1.2440 | LR: 0.000255\n",
      "  Batch 2200/3750 | Loss: 1.2421 | LR: 0.000254\n",
      "  Batch 2300/3750 | Loss: 1.2404 | LR: 0.000254\n",
      "  Batch 2400/3750 | Loss: 1.2411 | LR: 0.000253\n",
      "  Batch 2500/3750 | Loss: 1.2400 | LR: 0.000253\n",
      "  Batch 2600/3750 | Loss: 1.2399 | LR: 0.000252\n",
      "  Batch 2700/3750 | Loss: 1.2414 | LR: 0.000252\n",
      "  Batch 2800/3750 | Loss: 1.2413 | LR: 0.000251\n",
      "  Batch 2900/3750 | Loss: 1.2403 | LR: 0.000251\n",
      "  Batch 3000/3750 | Loss: 1.2396 | LR: 0.000250\n",
      "  Batch 3100/3750 | Loss: 1.2389 | LR: 0.000250\n",
      "  Batch 3200/3750 | Loss: 1.2372 | LR: 0.000249\n",
      "  Batch 3300/3750 | Loss: 1.2363 | LR: 0.000249\n",
      "  Batch 3400/3750 | Loss: 1.2354 | LR: 0.000248\n",
      "  Batch 3500/3750 | Loss: 1.2352 | LR: 0.000248\n",
      "  Batch 3600/3750 | Loss: 1.2350 | LR: 0.000247\n",
      "  Batch 3700/3750 | Loss: 1.2340 | LR: 0.000247\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 7 Summary:\n",
      "     Train Loss: 1.2346\n",
      "     Val Loss:   1.1912\n",
      "     LR:         0.000246\n",
      "     ‚úî New best model saved! (Val Loss: 1.1912)\n",
      "\n",
      "============================================================\n",
      "Epoch 8/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.2026 | LR: 0.000246\n",
      "  Batch 200/3750 | Loss: 1.2288 | LR: 0.000245\n",
      "  Batch 300/3750 | Loss: 1.2131 | LR: 0.000245\n",
      "  Batch 400/3750 | Loss: 1.2070 | LR: 0.000244\n",
      "  Batch 500/3750 | Loss: 1.1999 | LR: 0.000244\n",
      "  Batch 600/3750 | Loss: 1.2014 | LR: 0.000243\n",
      "  Batch 700/3750 | Loss: 1.1994 | LR: 0.000243\n",
      "  Batch 800/3750 | Loss: 1.1996 | LR: 0.000242\n",
      "  Batch 900/3750 | Loss: 1.2011 | LR: 0.000242\n",
      "  Batch 1000/3750 | Loss: 1.2031 | LR: 0.000241\n",
      "  Batch 1100/3750 | Loss: 1.2001 | LR: 0.000240\n",
      "  Batch 1200/3750 | Loss: 1.2025 | LR: 0.000240\n",
      "  Batch 1300/3750 | Loss: 1.1980 | LR: 0.000239\n",
      "  Batch 1400/3750 | Loss: 1.1980 | LR: 0.000239\n",
      "  Batch 1500/3750 | Loss: 1.2008 | LR: 0.000238\n",
      "  Batch 1600/3750 | Loss: 1.2017 | LR: 0.000238\n",
      "  Batch 1700/3750 | Loss: 1.2023 | LR: 0.000237\n",
      "  Batch 1800/3750 | Loss: 1.2025 | LR: 0.000236\n",
      "  Batch 1900/3750 | Loss: 1.2014 | LR: 0.000236\n",
      "  Batch 2000/3750 | Loss: 1.1999 | LR: 0.000235\n",
      "  Batch 2100/3750 | Loss: 1.1988 | LR: 0.000235\n",
      "  Batch 2200/3750 | Loss: 1.2005 | LR: 0.000234\n",
      "  Batch 2300/3750 | Loss: 1.2001 | LR: 0.000234\n",
      "  Batch 2400/3750 | Loss: 1.1985 | LR: 0.000233\n",
      "  Batch 2500/3750 | Loss: 1.1999 | LR: 0.000232\n",
      "  Batch 2600/3750 | Loss: 1.1990 | LR: 0.000232\n",
      "  Batch 2700/3750 | Loss: 1.1991 | LR: 0.000231\n",
      "  Batch 2800/3750 | Loss: 1.2009 | LR: 0.000231\n",
      "  Batch 2900/3750 | Loss: 1.1992 | LR: 0.000230\n",
      "  Batch 3000/3750 | Loss: 1.1985 | LR: 0.000229\n",
      "  Batch 3100/3750 | Loss: 1.1978 | LR: 0.000229\n",
      "  Batch 3200/3750 | Loss: 1.1990 | LR: 0.000228\n",
      "  Batch 3300/3750 | Loss: 1.1986 | LR: 0.000228\n",
      "  Batch 3400/3750 | Loss: 1.1977 | LR: 0.000227\n",
      "  Batch 3500/3750 | Loss: 1.1977 | LR: 0.000227\n",
      "  Batch 3600/3750 | Loss: 1.1976 | LR: 0.000226\n",
      "  Batch 3700/3750 | Loss: 1.1977 | LR: 0.000225\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 8 Summary:\n",
      "     Train Loss: 1.1978\n",
      "     Val Loss:   1.1616\n",
      "     LR:         0.000225\n",
      "     ‚úî New best model saved! (Val Loss: 1.1616)\n",
      "\n",
      "============================================================\n",
      "Epoch 9/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.1509 | LR: 0.000224\n",
      "  Batch 200/3750 | Loss: 1.1643 | LR: 0.000224\n",
      "  Batch 300/3750 | Loss: 1.1643 | LR: 0.000223\n",
      "  Batch 400/3750 | Loss: 1.1724 | LR: 0.000223\n",
      "  Batch 500/3750 | Loss: 1.1685 | LR: 0.000222\n",
      "  Batch 600/3750 | Loss: 1.1722 | LR: 0.000221\n",
      "  Batch 700/3750 | Loss: 1.1720 | LR: 0.000221\n",
      "  Batch 800/3750 | Loss: 1.1655 | LR: 0.000220\n",
      "  Batch 900/3750 | Loss: 1.1659 | LR: 0.000219\n",
      "  Batch 1000/3750 | Loss: 1.1657 | LR: 0.000219\n",
      "  Batch 1100/3750 | Loss: 1.1698 | LR: 0.000218\n",
      "  Batch 1200/3750 | Loss: 1.1665 | LR: 0.000218\n",
      "  Batch 1300/3750 | Loss: 1.1657 | LR: 0.000217\n",
      "  Batch 1400/3750 | Loss: 1.1672 | LR: 0.000216\n",
      "  Batch 1500/3750 | Loss: 1.1667 | LR: 0.000216\n",
      "  Batch 1600/3750 | Loss: 1.1669 | LR: 0.000215\n",
      "  Batch 1700/3750 | Loss: 1.1680 | LR: 0.000214\n",
      "  Batch 1800/3750 | Loss: 1.1676 | LR: 0.000214\n",
      "  Batch 1900/3750 | Loss: 1.1688 | LR: 0.000213\n",
      "  Batch 2000/3750 | Loss: 1.1679 | LR: 0.000213\n",
      "  Batch 2100/3750 | Loss: 1.1651 | LR: 0.000212\n",
      "  Batch 2200/3750 | Loss: 1.1632 | LR: 0.000211\n",
      "  Batch 2300/3750 | Loss: 1.1643 | LR: 0.000211\n",
      "  Batch 2400/3750 | Loss: 1.1634 | LR: 0.000210\n",
      "  Batch 2500/3750 | Loss: 1.1634 | LR: 0.000209\n",
      "  Batch 2600/3750 | Loss: 1.1621 | LR: 0.000209\n",
      "  Batch 2700/3750 | Loss: 1.1614 | LR: 0.000208\n",
      "  Batch 2800/3750 | Loss: 1.1611 | LR: 0.000207\n",
      "  Batch 2900/3750 | Loss: 1.1611 | LR: 0.000207\n",
      "  Batch 3000/3750 | Loss: 1.1595 | LR: 0.000206\n",
      "  Batch 3100/3750 | Loss: 1.1586 | LR: 0.000206\n",
      "  Batch 3200/3750 | Loss: 1.1578 | LR: 0.000205\n",
      "  Batch 3300/3750 | Loss: 1.1564 | LR: 0.000204\n",
      "  Batch 3400/3750 | Loss: 1.1592 | LR: 0.000204\n",
      "  Batch 3500/3750 | Loss: 1.1588 | LR: 0.000203\n",
      "  Batch 3600/3750 | Loss: 1.1580 | LR: 0.000202\n",
      "  Batch 3700/3750 | Loss: 1.1586 | LR: 0.000202\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 9 Summary:\n",
      "     Train Loss: 1.1580\n",
      "     Val Loss:   1.1330\n",
      "     LR:         0.000201\n",
      "     ‚úî New best model saved! (Val Loss: 1.1330)\n",
      "\n",
      "============================================================\n",
      "Epoch 10/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.1150 | LR: 0.000201\n",
      "  Batch 200/3750 | Loss: 1.1433 | LR: 0.000200\n",
      "  Batch 300/3750 | Loss: 1.1411 | LR: 0.000199\n",
      "  Batch 400/3750 | Loss: 1.1424 | LR: 0.000199\n",
      "  Batch 500/3750 | Loss: 1.1315 | LR: 0.000198\n",
      "  Batch 600/3750 | Loss: 1.1364 | LR: 0.000197\n",
      "  Batch 700/3750 | Loss: 1.1378 | LR: 0.000197\n",
      "  Batch 800/3750 | Loss: 1.1386 | LR: 0.000196\n",
      "  Batch 900/3750 | Loss: 1.1367 | LR: 0.000195\n",
      "  Batch 1000/3750 | Loss: 1.1334 | LR: 0.000195\n",
      "  Batch 1100/3750 | Loss: 1.1349 | LR: 0.000194\n",
      "  Batch 1200/3750 | Loss: 1.1329 | LR: 0.000193\n",
      "  Batch 1300/3750 | Loss: 1.1346 | LR: 0.000193\n",
      "  Batch 1400/3750 | Loss: 1.1342 | LR: 0.000192\n",
      "  Batch 1500/3750 | Loss: 1.1319 | LR: 0.000191\n",
      "  Batch 1600/3750 | Loss: 1.1310 | LR: 0.000191\n",
      "  Batch 1700/3750 | Loss: 1.1301 | LR: 0.000190\n",
      "  Batch 1800/3750 | Loss: 1.1325 | LR: 0.000189\n",
      "  Batch 1900/3750 | Loss: 1.1307 | LR: 0.000189\n",
      "  Batch 2000/3750 | Loss: 1.1303 | LR: 0.000188\n",
      "  Batch 2100/3750 | Loss: 1.1308 | LR: 0.000187\n",
      "  Batch 2200/3750 | Loss: 1.1317 | LR: 0.000187\n",
      "  Batch 2300/3750 | Loss: 1.1307 | LR: 0.000186\n",
      "  Batch 2400/3750 | Loss: 1.1315 | LR: 0.000185\n",
      "  Batch 2500/3750 | Loss: 1.1296 | LR: 0.000185\n",
      "  Batch 2600/3750 | Loss: 1.1298 | LR: 0.000184\n",
      "  Batch 2700/3750 | Loss: 1.1307 | LR: 0.000183\n",
      "  Batch 2800/3750 | Loss: 1.1313 | LR: 0.000183\n",
      "  Batch 2900/3750 | Loss: 1.1320 | LR: 0.000182\n",
      "  Batch 3000/3750 | Loss: 1.1315 | LR: 0.000181\n",
      "  Batch 3100/3750 | Loss: 1.1307 | LR: 0.000181\n",
      "  Batch 3200/3750 | Loss: 1.1308 | LR: 0.000180\n",
      "  Batch 3300/3750 | Loss: 1.1312 | LR: 0.000179\n",
      "  Batch 3400/3750 | Loss: 1.1308 | LR: 0.000178\n",
      "  Batch 3500/3750 | Loss: 1.1311 | LR: 0.000178\n",
      "  Batch 3600/3750 | Loss: 1.1318 | LR: 0.000177\n",
      "  Batch 3700/3750 | Loss: 1.1319 | LR: 0.000176\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 10 Summary:\n",
      "     Train Loss: 1.1319\n",
      "     Val Loss:   1.1128\n",
      "     LR:         0.000176\n",
      "     ‚úî New best model saved! (Val Loss: 1.1128)\n",
      "\n",
      "============================================================\n",
      "Epoch 11/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.1287 | LR: 0.000175\n",
      "  Batch 200/3750 | Loss: 1.1346 | LR: 0.000175\n",
      "  Batch 300/3750 | Loss: 1.1162 | LR: 0.000174\n",
      "  Batch 400/3750 | Loss: 1.1151 | LR: 0.000173\n",
      "  Batch 500/3750 | Loss: 1.1104 | LR: 0.000173\n",
      "  Batch 600/3750 | Loss: 1.1071 | LR: 0.000172\n",
      "  Batch 700/3750 | Loss: 1.1065 | LR: 0.000171\n",
      "  Batch 800/3750 | Loss: 1.1035 | LR: 0.000171\n",
      "  Batch 900/3750 | Loss: 1.1112 | LR: 0.000170\n",
      "  Batch 1000/3750 | Loss: 1.1081 | LR: 0.000169\n",
      "  Batch 1100/3750 | Loss: 1.1087 | LR: 0.000168\n",
      "  Batch 1200/3750 | Loss: 1.1086 | LR: 0.000168\n",
      "  Batch 1300/3750 | Loss: 1.1114 | LR: 0.000167\n",
      "  Batch 1400/3750 | Loss: 1.1101 | LR: 0.000166\n",
      "  Batch 1500/3750 | Loss: 1.1082 | LR: 0.000166\n",
      "  Batch 1600/3750 | Loss: 1.1083 | LR: 0.000165\n",
      "  Batch 1700/3750 | Loss: 1.1079 | LR: 0.000164\n",
      "  Batch 1800/3750 | Loss: 1.1079 | LR: 0.000164\n",
      "  Batch 1900/3750 | Loss: 1.1050 | LR: 0.000163\n",
      "  Batch 2000/3750 | Loss: 1.1048 | LR: 0.000162\n",
      "  Batch 2100/3750 | Loss: 1.1038 | LR: 0.000162\n",
      "  Batch 2200/3750 | Loss: 1.1040 | LR: 0.000161\n",
      "  Batch 2300/3750 | Loss: 1.1040 | LR: 0.000160\n",
      "  Batch 2400/3750 | Loss: 1.1052 | LR: 0.000159\n",
      "  Batch 2500/3750 | Loss: 1.1043 | LR: 0.000159\n",
      "  Batch 2600/3750 | Loss: 1.1047 | LR: 0.000158\n",
      "  Batch 2700/3750 | Loss: 1.1053 | LR: 0.000157\n",
      "  Batch 2800/3750 | Loss: 1.1038 | LR: 0.000157\n",
      "  Batch 2900/3750 | Loss: 1.1029 | LR: 0.000156\n",
      "  Batch 3000/3750 | Loss: 1.1019 | LR: 0.000155\n",
      "  Batch 3100/3750 | Loss: 1.1012 | LR: 0.000155\n",
      "  Batch 3200/3750 | Loss: 1.1012 | LR: 0.000154\n",
      "  Batch 3300/3750 | Loss: 1.1020 | LR: 0.000153\n",
      "  Batch 3400/3750 | Loss: 1.1033 | LR: 0.000152\n",
      "  Batch 3500/3750 | Loss: 1.1035 | LR: 0.000152\n",
      "  Batch 3600/3750 | Loss: 1.1042 | LR: 0.000151\n",
      "  Batch 3700/3750 | Loss: 1.1038 | LR: 0.000150\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 11 Summary:\n",
      "     Train Loss: 1.1043\n",
      "     Val Loss:   1.0958\n",
      "     LR:         0.000150\n",
      "     ‚úî New best model saved! (Val Loss: 1.0958)\n",
      "\n",
      "============================================================\n",
      "Epoch 12/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.1112 | LR: 0.000149\n",
      "  Batch 200/3750 | Loss: 1.1042 | LR: 0.000149\n",
      "  Batch 300/3750 | Loss: 1.1161 | LR: 0.000148\n",
      "  Batch 400/3750 | Loss: 1.1069 | LR: 0.000147\n",
      "  Batch 500/3750 | Loss: 1.1021 | LR: 0.000147\n",
      "  Batch 600/3750 | Loss: 1.1000 | LR: 0.000146\n",
      "  Batch 700/3750 | Loss: 1.0944 | LR: 0.000145\n",
      "  Batch 800/3750 | Loss: 1.0975 | LR: 0.000144\n",
      "  Batch 900/3750 | Loss: 1.0944 | LR: 0.000144\n",
      "  Batch 1000/3750 | Loss: 1.0916 | LR: 0.000143\n",
      "  Batch 1100/3750 | Loss: 1.0907 | LR: 0.000142\n",
      "  Batch 1200/3750 | Loss: 1.0882 | LR: 0.000142\n",
      "  Batch 1300/3750 | Loss: 1.0887 | LR: 0.000141\n",
      "  Batch 1400/3750 | Loss: 1.0853 | LR: 0.000140\n",
      "  Batch 1500/3750 | Loss: 1.0816 | LR: 0.000140\n",
      "  Batch 1600/3750 | Loss: 1.0823 | LR: 0.000139\n",
      "  Batch 1700/3750 | Loss: 1.0832 | LR: 0.000138\n",
      "  Batch 1800/3750 | Loss: 1.0826 | LR: 0.000137\n",
      "  Batch 1900/3750 | Loss: 1.0840 | LR: 0.000137\n",
      "  Batch 2000/3750 | Loss: 1.0832 | LR: 0.000136\n",
      "  Batch 2100/3750 | Loss: 1.0848 | LR: 0.000135\n",
      "  Batch 2200/3750 | Loss: 1.0865 | LR: 0.000135\n",
      "  Batch 2300/3750 | Loss: 1.0865 | LR: 0.000134\n",
      "  Batch 2400/3750 | Loss: 1.0870 | LR: 0.000133\n",
      "  Batch 2500/3750 | Loss: 1.0847 | LR: 0.000133\n",
      "  Batch 2600/3750 | Loss: 1.0852 | LR: 0.000132\n",
      "  Batch 2700/3750 | Loss: 1.0863 | LR: 0.000131\n",
      "  Batch 2800/3750 | Loss: 1.0864 | LR: 0.000131\n",
      "  Batch 2900/3750 | Loss: 1.0841 | LR: 0.000130\n",
      "  Batch 3000/3750 | Loss: 1.0838 | LR: 0.000129\n",
      "  Batch 3100/3750 | Loss: 1.0848 | LR: 0.000128\n",
      "  Batch 3200/3750 | Loss: 1.0830 | LR: 0.000128\n",
      "  Batch 3300/3750 | Loss: 1.0835 | LR: 0.000127\n",
      "  Batch 3400/3750 | Loss: 1.0845 | LR: 0.000126\n",
      "  Batch 3500/3750 | Loss: 1.0838 | LR: 0.000126\n",
      "  Batch 3600/3750 | Loss: 1.0841 | LR: 0.000125\n",
      "  Batch 3700/3750 | Loss: 1.0846 | LR: 0.000124\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 12 Summary:\n",
      "     Train Loss: 1.0844\n",
      "     Val Loss:   1.0822\n",
      "     LR:         0.000124\n",
      "     ‚úî New best model saved! (Val Loss: 1.0822)\n",
      "\n",
      "============================================================\n",
      "Epoch 13/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.0790 | LR: 0.000123\n",
      "  Batch 200/3750 | Loss: 1.0928 | LR: 0.000123\n",
      "  Batch 300/3750 | Loss: 1.0676 | LR: 0.000122\n",
      "  Batch 400/3750 | Loss: 1.0699 | LR: 0.000121\n",
      "  Batch 500/3750 | Loss: 1.0752 | LR: 0.000121\n",
      "  Batch 600/3750 | Loss: 1.0719 | LR: 0.000120\n",
      "  Batch 700/3750 | Loss: 1.0651 | LR: 0.000119\n",
      "  Batch 800/3750 | Loss: 1.0666 | LR: 0.000118\n",
      "  Batch 900/3750 | Loss: 1.0679 | LR: 0.000118\n",
      "  Batch 1000/3750 | Loss: 1.0694 | LR: 0.000117\n",
      "  Batch 1100/3750 | Loss: 1.0694 | LR: 0.000116\n",
      "  Batch 1200/3750 | Loss: 1.0678 | LR: 0.000116\n",
      "  Batch 1300/3750 | Loss: 1.0686 | LR: 0.000115\n",
      "  Batch 1400/3750 | Loss: 1.0736 | LR: 0.000114\n",
      "  Batch 1500/3750 | Loss: 1.0749 | LR: 0.000114\n",
      "  Batch 1600/3750 | Loss: 1.0729 | LR: 0.000113\n",
      "  Batch 1700/3750 | Loss: 1.0705 | LR: 0.000112\n",
      "  Batch 1800/3750 | Loss: 1.0709 | LR: 0.000112\n",
      "  Batch 1900/3750 | Loss: 1.0722 | LR: 0.000111\n",
      "  Batch 2000/3750 | Loss: 1.0719 | LR: 0.000110\n",
      "  Batch 2100/3750 | Loss: 1.0710 | LR: 0.000110\n",
      "  Batch 2200/3750 | Loss: 1.0719 | LR: 0.000109\n",
      "  Batch 2300/3750 | Loss: 1.0725 | LR: 0.000108\n",
      "  Batch 2400/3750 | Loss: 1.0735 | LR: 0.000108\n",
      "  Batch 2500/3750 | Loss: 1.0731 | LR: 0.000107\n",
      "  Batch 2600/3750 | Loss: 1.0723 | LR: 0.000106\n",
      "  Batch 2700/3750 | Loss: 1.0724 | LR: 0.000106\n",
      "  Batch 2800/3750 | Loss: 1.0718 | LR: 0.000105\n",
      "  Batch 2900/3750 | Loss: 1.0704 | LR: 0.000104\n",
      "  Batch 3000/3750 | Loss: 1.0691 | LR: 0.000104\n",
      "  Batch 3100/3750 | Loss: 1.0689 | LR: 0.000103\n",
      "  Batch 3200/3750 | Loss: 1.0698 | LR: 0.000102\n",
      "  Batch 3300/3750 | Loss: 1.0700 | LR: 0.000102\n",
      "  Batch 3400/3750 | Loss: 1.0696 | LR: 0.000101\n",
      "  Batch 3500/3750 | Loss: 1.0698 | LR: 0.000100\n",
      "  Batch 3600/3750 | Loss: 1.0705 | LR: 0.000100\n",
      "  Batch 3700/3750 | Loss: 1.0706 | LR: 0.000099\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 13 Summary:\n",
      "     Train Loss: 1.0709\n",
      "     Val Loss:   1.0686\n",
      "     LR:         0.000099\n",
      "     ‚úî New best model saved! (Val Loss: 1.0686)\n",
      "\n",
      "============================================================\n",
      "Epoch 14/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.0672 | LR: 0.000098\n",
      "  Batch 200/3750 | Loss: 1.0352 | LR: 0.000097\n",
      "  Batch 300/3750 | Loss: 1.0326 | LR: 0.000097\n",
      "  Batch 400/3750 | Loss: 1.0256 | LR: 0.000096\n",
      "  Batch 500/3750 | Loss: 1.0341 | LR: 0.000095\n",
      "  Batch 600/3750 | Loss: 1.0314 | LR: 0.000095\n",
      "  Batch 700/3750 | Loss: 1.0351 | LR: 0.000094\n",
      "  Batch 800/3750 | Loss: 1.0374 | LR: 0.000093\n",
      "  Batch 900/3750 | Loss: 1.0321 | LR: 0.000093\n",
      "  Batch 1000/3750 | Loss: 1.0342 | LR: 0.000092\n",
      "  Batch 1100/3750 | Loss: 1.0394 | LR: 0.000092\n",
      "  Batch 1200/3750 | Loss: 1.0419 | LR: 0.000091\n",
      "  Batch 1300/3750 | Loss: 1.0423 | LR: 0.000090\n",
      "  Batch 1400/3750 | Loss: 1.0449 | LR: 0.000090\n",
      "  Batch 1500/3750 | Loss: 1.0434 | LR: 0.000089\n",
      "  Batch 1600/3750 | Loss: 1.0439 | LR: 0.000088\n",
      "  Batch 1700/3750 | Loss: 1.0463 | LR: 0.000088\n",
      "  Batch 1800/3750 | Loss: 1.0463 | LR: 0.000087\n",
      "  Batch 1900/3750 | Loss: 1.0463 | LR: 0.000086\n",
      "  Batch 2000/3750 | Loss: 1.0473 | LR: 0.000086\n",
      "  Batch 2100/3750 | Loss: 1.0459 | LR: 0.000085\n",
      "  Batch 2200/3750 | Loss: 1.0452 | LR: 0.000085\n",
      "  Batch 2300/3750 | Loss: 1.0464 | LR: 0.000084\n",
      "  Batch 2400/3750 | Loss: 1.0480 | LR: 0.000083\n",
      "  Batch 2500/3750 | Loss: 1.0487 | LR: 0.000083\n",
      "  Batch 2600/3750 | Loss: 1.0499 | LR: 0.000082\n",
      "  Batch 2700/3750 | Loss: 1.0507 | LR: 0.000081\n",
      "  Batch 2800/3750 | Loss: 1.0503 | LR: 0.000081\n",
      "  Batch 2900/3750 | Loss: 1.0497 | LR: 0.000080\n",
      "  Batch 3000/3750 | Loss: 1.0510 | LR: 0.000080\n",
      "  Batch 3100/3750 | Loss: 1.0513 | LR: 0.000079\n",
      "  Batch 3200/3750 | Loss: 1.0523 | LR: 0.000078\n",
      "  Batch 3300/3750 | Loss: 1.0528 | LR: 0.000078\n",
      "  Batch 3400/3750 | Loss: 1.0537 | LR: 0.000077\n",
      "  Batch 3500/3750 | Loss: 1.0531 | LR: 0.000077\n",
      "  Batch 3600/3750 | Loss: 1.0526 | LR: 0.000076\n",
      "  Batch 3700/3750 | Loss: 1.0512 | LR: 0.000075\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 14 Summary:\n",
      "     Train Loss: 1.0504\n",
      "     Val Loss:   1.0612\n",
      "     LR:         0.000075\n",
      "     ‚úî New best model saved! (Val Loss: 1.0612)\n",
      "\n",
      "============================================================\n",
      "Epoch 15/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.0847 | LR: 0.000074\n",
      "  Batch 200/3750 | Loss: 1.0465 | LR: 0.000074\n",
      "  Batch 300/3750 | Loss: 1.0439 | LR: 0.000073\n",
      "  Batch 400/3750 | Loss: 1.0414 | LR: 0.000073\n",
      "  Batch 500/3750 | Loss: 1.0472 | LR: 0.000072\n",
      "  Batch 600/3750 | Loss: 1.0394 | LR: 0.000071\n",
      "  Batch 700/3750 | Loss: 1.0329 | LR: 0.000071\n",
      "  Batch 800/3750 | Loss: 1.0321 | LR: 0.000070\n",
      "  Batch 900/3750 | Loss: 1.0303 | LR: 0.000070\n",
      "  Batch 1000/3750 | Loss: 1.0283 | LR: 0.000069\n",
      "  Batch 1100/3750 | Loss: 1.0279 | LR: 0.000068\n",
      "  Batch 1200/3750 | Loss: 1.0282 | LR: 0.000068\n",
      "  Batch 1300/3750 | Loss: 1.0299 | LR: 0.000067\n",
      "  Batch 1400/3750 | Loss: 1.0280 | LR: 0.000067\n",
      "  Batch 1500/3750 | Loss: 1.0256 | LR: 0.000066\n",
      "  Batch 1600/3750 | Loss: 1.0264 | LR: 0.000066\n",
      "  Batch 1700/3750 | Loss: 1.0268 | LR: 0.000065\n",
      "  Batch 1800/3750 | Loss: 1.0281 | LR: 0.000064\n",
      "  Batch 1900/3750 | Loss: 1.0285 | LR: 0.000064\n",
      "  Batch 2000/3750 | Loss: 1.0313 | LR: 0.000063\n",
      "  Batch 2100/3750 | Loss: 1.0315 | LR: 0.000063\n",
      "  Batch 2200/3750 | Loss: 1.0326 | LR: 0.000062\n",
      "  Batch 2300/3750 | Loss: 1.0318 | LR: 0.000062\n",
      "  Batch 2400/3750 | Loss: 1.0330 | LR: 0.000061\n",
      "  Batch 2500/3750 | Loss: 1.0334 | LR: 0.000060\n",
      "  Batch 2600/3750 | Loss: 1.0339 | LR: 0.000060\n",
      "  Batch 2700/3750 | Loss: 1.0338 | LR: 0.000059\n",
      "  Batch 2800/3750 | Loss: 1.0346 | LR: 0.000059\n",
      "  Batch 2900/3750 | Loss: 1.0334 | LR: 0.000058\n",
      "  Batch 3000/3750 | Loss: 1.0327 | LR: 0.000058\n",
      "  Batch 3100/3750 | Loss: 1.0318 | LR: 0.000057\n",
      "  Batch 3200/3750 | Loss: 1.0313 | LR: 0.000057\n",
      "  Batch 3300/3750 | Loss: 1.0323 | LR: 0.000056\n",
      "  Batch 3400/3750 | Loss: 1.0315 | LR: 0.000055\n",
      "  Batch 3500/3750 | Loss: 1.0315 | LR: 0.000055\n",
      "  Batch 3600/3750 | Loss: 1.0313 | LR: 0.000054\n",
      "  Batch 3700/3750 | Loss: 1.0317 | LR: 0.000054\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 15 Summary:\n",
      "     Train Loss: 1.0320\n",
      "     Val Loss:   1.0555\n",
      "     LR:         0.000054\n",
      "     ‚úî New best model saved! (Val Loss: 1.0555)\n",
      "\n",
      "============================================================\n",
      "Epoch 16/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.0740 | LR: 0.000053\n",
      "  Batch 200/3750 | Loss: 1.0484 | LR: 0.000053\n",
      "  Batch 300/3750 | Loss: 1.0535 | LR: 0.000052\n",
      "  Batch 400/3750 | Loss: 1.0462 | LR: 0.000051\n",
      "  Batch 500/3750 | Loss: 1.0472 | LR: 0.000051\n",
      "  Batch 600/3750 | Loss: 1.0414 | LR: 0.000050\n",
      "  Batch 700/3750 | Loss: 1.0414 | LR: 0.000050\n",
      "  Batch 800/3750 | Loss: 1.0390 | LR: 0.000049\n",
      "  Batch 900/3750 | Loss: 1.0384 | LR: 0.000049\n",
      "  Batch 1000/3750 | Loss: 1.0393 | LR: 0.000048\n",
      "  Batch 1100/3750 | Loss: 1.0351 | LR: 0.000048\n",
      "  Batch 1200/3750 | Loss: 1.0375 | LR: 0.000047\n",
      "  Batch 1300/3750 | Loss: 1.0368 | LR: 0.000047\n",
      "  Batch 1400/3750 | Loss: 1.0366 | LR: 0.000046\n",
      "  Batch 1500/3750 | Loss: 1.0350 | LR: 0.000046\n",
      "  Batch 1600/3750 | Loss: 1.0332 | LR: 0.000045\n",
      "  Batch 1700/3750 | Loss: 1.0321 | LR: 0.000045\n",
      "  Batch 1800/3750 | Loss: 1.0317 | LR: 0.000044\n",
      "  Batch 1900/3750 | Loss: 1.0302 | LR: 0.000044\n",
      "  Batch 2000/3750 | Loss: 1.0313 | LR: 0.000043\n",
      "  Batch 2100/3750 | Loss: 1.0300 | LR: 0.000043\n",
      "  Batch 2200/3750 | Loss: 1.0286 | LR: 0.000042\n",
      "  Batch 2300/3750 | Loss: 1.0289 | LR: 0.000042\n",
      "  Batch 2400/3750 | Loss: 1.0296 | LR: 0.000041\n",
      "  Batch 2500/3750 | Loss: 1.0290 | LR: 0.000041\n",
      "  Batch 2600/3750 | Loss: 1.0287 | LR: 0.000040\n",
      "  Batch 2700/3750 | Loss: 1.0297 | LR: 0.000040\n",
      "  Batch 2800/3750 | Loss: 1.0294 | LR: 0.000039\n",
      "  Batch 2900/3750 | Loss: 1.0298 | LR: 0.000039\n",
      "  Batch 3000/3750 | Loss: 1.0288 | LR: 0.000039\n",
      "  Batch 3100/3750 | Loss: 1.0289 | LR: 0.000038\n",
      "  Batch 3200/3750 | Loss: 1.0277 | LR: 0.000038\n",
      "  Batch 3300/3750 | Loss: 1.0277 | LR: 0.000037\n",
      "  Batch 3400/3750 | Loss: 1.0270 | LR: 0.000037\n",
      "  Batch 3500/3750 | Loss: 1.0273 | LR: 0.000036\n",
      "  Batch 3600/3750 | Loss: 1.0267 | LR: 0.000036\n",
      "  Batch 3700/3750 | Loss: 1.0254 | LR: 0.000035\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 16 Summary:\n",
      "     Train Loss: 1.0249\n",
      "     Val Loss:   1.0505\n",
      "     LR:         0.000035\n",
      "     ‚úî New best model saved! (Val Loss: 1.0505)\n",
      "\n",
      "============================================================\n",
      "Epoch 17/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.0273 | LR: 0.000035\n",
      "  Batch 200/3750 | Loss: 1.0208 | LR: 0.000034\n",
      "  Batch 300/3750 | Loss: 0.9986 | LR: 0.000034\n",
      "  Batch 400/3750 | Loss: 1.0062 | LR: 0.000033\n",
      "  Batch 500/3750 | Loss: 1.0078 | LR: 0.000033\n",
      "  Batch 600/3750 | Loss: 1.0019 | LR: 0.000032\n",
      "  Batch 700/3750 | Loss: 1.0063 | LR: 0.000032\n",
      "  Batch 800/3750 | Loss: 1.0095 | LR: 0.000032\n",
      "  Batch 900/3750 | Loss: 1.0107 | LR: 0.000031\n",
      "  Batch 1000/3750 | Loss: 1.0113 | LR: 0.000031\n",
      "  Batch 1100/3750 | Loss: 1.0126 | LR: 0.000030\n",
      "  Batch 1200/3750 | Loss: 1.0138 | LR: 0.000030\n",
      "  Batch 1300/3750 | Loss: 1.0140 | LR: 0.000030\n",
      "  Batch 1400/3750 | Loss: 1.0153 | LR: 0.000030\n",
      "  Batch 1500/3750 | Loss: 1.0149 | LR: 0.000030\n",
      "  Batch 1600/3750 | Loss: 1.0136 | LR: 0.000030\n",
      "  Batch 1700/3750 | Loss: 1.0141 | LR: 0.000030\n",
      "  Batch 1800/3750 | Loss: 1.0170 | LR: 0.000030\n",
      "  Batch 1900/3750 | Loss: 1.0197 | LR: 0.000030\n",
      "  Batch 2000/3750 | Loss: 1.0174 | LR: 0.000030\n",
      "  Batch 2100/3750 | Loss: 1.0169 | LR: 0.000030\n",
      "  Batch 2200/3750 | Loss: 1.0182 | LR: 0.000030\n",
      "  Batch 2300/3750 | Loss: 1.0193 | LR: 0.000030\n",
      "  Batch 2400/3750 | Loss: 1.0193 | LR: 0.000030\n",
      "  Batch 2500/3750 | Loss: 1.0201 | LR: 0.000030\n",
      "  Batch 2600/3750 | Loss: 1.0200 | LR: 0.000030\n",
      "  Batch 2700/3750 | Loss: 1.0201 | LR: 0.000030\n",
      "  Batch 2800/3750 | Loss: 1.0199 | LR: 0.000030\n",
      "  Batch 2900/3750 | Loss: 1.0194 | LR: 0.000030\n",
      "  Batch 3000/3750 | Loss: 1.0191 | LR: 0.000030\n",
      "  Batch 3100/3750 | Loss: 1.0199 | LR: 0.000030\n",
      "  Batch 3200/3750 | Loss: 1.0193 | LR: 0.000030\n",
      "  Batch 3300/3750 | Loss: 1.0193 | LR: 0.000030\n",
      "  Batch 3400/3750 | Loss: 1.0187 | LR: 0.000030\n",
      "  Batch 3500/3750 | Loss: 1.0180 | LR: 0.000030\n",
      "  Batch 3600/3750 | Loss: 1.0173 | LR: 0.000030\n",
      "  Batch 3700/3750 | Loss: 1.0164 | LR: 0.000030\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 17 Summary:\n",
      "     Train Loss: 1.0166\n",
      "     Val Loss:   1.0488\n",
      "     LR:         0.000030\n",
      "     ‚úî New best model saved! (Val Loss: 1.0488)\n",
      "\n",
      "============================================================\n",
      "Epoch 18/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.0274 | LR: 0.000030\n",
      "  Batch 200/3750 | Loss: 1.0173 | LR: 0.000030\n",
      "  Batch 300/3750 | Loss: 1.0144 | LR: 0.000030\n",
      "  Batch 400/3750 | Loss: 1.0167 | LR: 0.000030\n",
      "  Batch 500/3750 | Loss: 1.0161 | LR: 0.000030\n",
      "  Batch 600/3750 | Loss: 1.0167 | LR: 0.000030\n",
      "  Batch 700/3750 | Loss: 1.0182 | LR: 0.000030\n",
      "  Batch 800/3750 | Loss: 1.0154 | LR: 0.000030\n",
      "  Batch 900/3750 | Loss: 1.0145 | LR: 0.000030\n",
      "  Batch 1000/3750 | Loss: 1.0175 | LR: 0.000030\n",
      "  Batch 1100/3750 | Loss: 1.0177 | LR: 0.000030\n",
      "  Batch 1200/3750 | Loss: 1.0130 | LR: 0.000030\n",
      "  Batch 1300/3750 | Loss: 1.0133 | LR: 0.000030\n",
      "  Batch 1400/3750 | Loss: 1.0109 | LR: 0.000030\n",
      "  Batch 1500/3750 | Loss: 1.0106 | LR: 0.000030\n",
      "  Batch 1600/3750 | Loss: 1.0087 | LR: 0.000030\n",
      "  Batch 1700/3750 | Loss: 1.0070 | LR: 0.000030\n",
      "  Batch 1800/3750 | Loss: 1.0088 | LR: 0.000030\n",
      "  Batch 1900/3750 | Loss: 1.0104 | LR: 0.000030\n",
      "  Batch 2000/3750 | Loss: 1.0107 | LR: 0.000030\n",
      "  Batch 2100/3750 | Loss: 1.0113 | LR: 0.000030\n",
      "  Batch 2200/3750 | Loss: 1.0101 | LR: 0.000030\n",
      "  Batch 2300/3750 | Loss: 1.0103 | LR: 0.000030\n",
      "  Batch 2400/3750 | Loss: 1.0110 | LR: 0.000030\n",
      "  Batch 2500/3750 | Loss: 1.0113 | LR: 0.000030\n",
      "  Batch 2600/3750 | Loss: 1.0108 | LR: 0.000030\n",
      "  Batch 2700/3750 | Loss: 1.0107 | LR: 0.000030\n",
      "  Batch 2800/3750 | Loss: 1.0117 | LR: 0.000030\n",
      "  Batch 2900/3750 | Loss: 1.0119 | LR: 0.000030\n",
      "  Batch 3000/3750 | Loss: 1.0113 | LR: 0.000030\n",
      "  Batch 3100/3750 | Loss: 1.0107 | LR: 0.000030\n",
      "  Batch 3200/3750 | Loss: 1.0105 | LR: 0.000030\n",
      "  Batch 3300/3750 | Loss: 1.0104 | LR: 0.000030\n",
      "  Batch 3400/3750 | Loss: 1.0108 | LR: 0.000030\n",
      "  Batch 3500/3750 | Loss: 1.0107 | LR: 0.000030\n",
      "  Batch 3600/3750 | Loss: 1.0107 | LR: 0.000030\n",
      "  Batch 3700/3750 | Loss: 1.0102 | LR: 0.000030\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 18 Summary:\n",
      "     Train Loss: 1.0107\n",
      "     Val Loss:   1.0449\n",
      "     LR:         0.000030\n",
      "     ‚úî New best model saved! (Val Loss: 1.0449)\n",
      "\n",
      "============================================================\n",
      "Epoch 19/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 0.9986 | LR: 0.000030\n",
      "  Batch 200/3750 | Loss: 1.0096 | LR: 0.000030\n",
      "  Batch 300/3750 | Loss: 1.0098 | LR: 0.000030\n",
      "  Batch 400/3750 | Loss: 1.0180 | LR: 0.000030\n",
      "  Batch 500/3750 | Loss: 1.0096 | LR: 0.000030\n",
      "  Batch 600/3750 | Loss: 1.0103 | LR: 0.000030\n",
      "  Batch 700/3750 | Loss: 1.0075 | LR: 0.000030\n",
      "  Batch 800/3750 | Loss: 1.0114 | LR: 0.000030\n",
      "  Batch 900/3750 | Loss: 1.0111 | LR: 0.000030\n",
      "  Batch 1000/3750 | Loss: 1.0077 | LR: 0.000030\n",
      "  Batch 1100/3750 | Loss: 1.0076 | LR: 0.000030\n",
      "  Batch 1200/3750 | Loss: 1.0121 | LR: 0.000030\n",
      "  Batch 1300/3750 | Loss: 1.0110 | LR: 0.000030\n",
      "  Batch 1400/3750 | Loss: 1.0100 | LR: 0.000030\n",
      "  Batch 1500/3750 | Loss: 1.0092 | LR: 0.000030\n",
      "  Batch 1600/3750 | Loss: 1.0074 | LR: 0.000030\n",
      "  Batch 1700/3750 | Loss: 1.0074 | LR: 0.000030\n",
      "  Batch 1800/3750 | Loss: 1.0091 | LR: 0.000030\n",
      "  Batch 1900/3750 | Loss: 1.0072 | LR: 0.000030\n",
      "  Batch 2000/3750 | Loss: 1.0081 | LR: 0.000030\n",
      "  Batch 2100/3750 | Loss: 1.0063 | LR: 0.000030\n",
      "  Batch 2200/3750 | Loss: 1.0079 | LR: 0.000030\n",
      "  Batch 2300/3750 | Loss: 1.0059 | LR: 0.000030\n",
      "  Batch 2400/3750 | Loss: 1.0060 | LR: 0.000030\n",
      "  Batch 2500/3750 | Loss: 1.0065 | LR: 0.000030\n",
      "  Batch 2600/3750 | Loss: 1.0069 | LR: 0.000030\n",
      "  Batch 2700/3750 | Loss: 1.0064 | LR: 0.000030\n",
      "  Batch 2800/3750 | Loss: 1.0073 | LR: 0.000030\n",
      "  Batch 2900/3750 | Loss: 1.0066 | LR: 0.000030\n",
      "  Batch 3000/3750 | Loss: 1.0073 | LR: 0.000030\n",
      "  Batch 3100/3750 | Loss: 1.0076 | LR: 0.000030\n",
      "  Batch 3200/3750 | Loss: 1.0064 | LR: 0.000030\n",
      "  Batch 3300/3750 | Loss: 1.0070 | LR: 0.000030\n",
      "  Batch 3400/3750 | Loss: 1.0067 | LR: 0.000030\n",
      "  Batch 3500/3750 | Loss: 1.0060 | LR: 0.000030\n",
      "  Batch 3600/3750 | Loss: 1.0057 | LR: 0.000030\n",
      "  Batch 3700/3750 | Loss: 1.0064 | LR: 0.000030\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 19 Summary:\n",
      "     Train Loss: 1.0061\n",
      "     Val Loss:   1.0455\n",
      "     LR:         0.000030\n",
      "     ‚ö† EarlyStopping counter: 1/5\n",
      "\n",
      "============================================================\n",
      "Epoch 20/20\n",
      "============================================================\n",
      "  Batch 100/3750 | Loss: 1.0240 | LR: 0.000030\n",
      "  Batch 200/3750 | Loss: 1.0285 | LR: 0.000030\n",
      "  Batch 300/3750 | Loss: 1.0224 | LR: 0.000030\n",
      "  Batch 400/3750 | Loss: 1.0176 | LR: 0.000030\n",
      "  Batch 500/3750 | Loss: 1.0168 | LR: 0.000030\n",
      "  Batch 600/3750 | Loss: 1.0168 | LR: 0.000030\n",
      "  Batch 700/3750 | Loss: 1.0168 | LR: 0.000030\n",
      "  Batch 800/3750 | Loss: 1.0181 | LR: 0.000030\n",
      "  Batch 900/3750 | Loss: 1.0145 | LR: 0.000030\n",
      "  Batch 1000/3750 | Loss: 1.0131 | LR: 0.000030\n",
      "  Batch 1100/3750 | Loss: 1.0158 | LR: 0.000030\n",
      "  Batch 1200/3750 | Loss: 1.0123 | LR: 0.000030\n",
      "  Batch 1300/3750 | Loss: 1.0147 | LR: 0.000030\n",
      "  Batch 1400/3750 | Loss: 1.0167 | LR: 0.000030\n",
      "  Batch 1500/3750 | Loss: 1.0160 | LR: 0.000030\n",
      "  Batch 1600/3750 | Loss: 1.0151 | LR: 0.000030\n",
      "  Batch 1700/3750 | Loss: 1.0170 | LR: 0.000030\n",
      "  Batch 1800/3750 | Loss: 1.0167 | LR: 0.000030\n",
      "  Batch 1900/3750 | Loss: 1.0165 | LR: 0.000030\n",
      "  Batch 2000/3750 | Loss: 1.0164 | LR: 0.000030\n",
      "  Batch 2100/3750 | Loss: 1.0153 | LR: 0.000030\n",
      "  Batch 2200/3750 | Loss: 1.0165 | LR: 0.000030\n",
      "  Batch 2300/3750 | Loss: 1.0152 | LR: 0.000030\n",
      "  Batch 2400/3750 | Loss: 1.0163 | LR: 0.000030\n",
      "  Batch 2500/3750 | Loss: 1.0139 | LR: 0.000030\n",
      "  Batch 2600/3750 | Loss: 1.0150 | LR: 0.000030\n",
      "  Batch 2700/3750 | Loss: 1.0129 | LR: 0.000030\n",
      "  Batch 2800/3750 | Loss: 1.0130 | LR: 0.000030\n",
      "  Batch 2900/3750 | Loss: 1.0118 | LR: 0.000030\n",
      "  Batch 3000/3750 | Loss: 1.0110 | LR: 0.000030\n",
      "  Batch 3100/3750 | Loss: 1.0105 | LR: 0.000030\n",
      "  Batch 3200/3750 | Loss: 1.0106 | LR: 0.000030\n",
      "  Batch 3300/3750 | Loss: 1.0113 | LR: 0.000030\n",
      "  Batch 3400/3750 | Loss: 1.0092 | LR: 0.000030\n",
      "  Batch 3500/3750 | Loss: 1.0092 | LR: 0.000030\n",
      "  Batch 3600/3750 | Loss: 1.0094 | LR: 0.000030\n",
      "  Batch 3700/3750 | Loss: 1.0103 | LR: 0.000030\n",
      "\n",
      "  Running validation...\n",
      "\n",
      "  üìä Epoch 20 Summary:\n",
      "     Train Loss: 1.0104\n",
      "     Val Loss:   1.0431\n",
      "     LR:         0.000030\n",
      "     ‚úî New best model saved! (Val Loss: 1.0431)\n",
      "\n",
      "\n",
      "üìà Training History:\n",
      "Best Val Loss: 1.0431 at epoch 20\n",
      "\n",
      "==================================================\n",
      "EVALUATION\n",
      "==================================================\n",
      "\n",
      "EN: When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy . &quot;\n",
      "GT: Khi t√¥i c√≤n nh·ªè , T√¥i nghƒ© r·∫±ng B·∫ØcTri·ªÅu Ti√™n l√† ƒë·∫•t n∆∞·ªõc t·ªët nh·∫•t tr√™n th·∫ø gi·ªõi v√† t√¥i th∆∞·ªùng h√°t b√†i &quot; Ch√∫ng ta ch·∫≥ng c√≥ g√¨ ph·∫£i ghen t·ªã . &quot;\n",
      "PR: khi t√¥i c√≤n nh·ªè , t√¥i nghƒ© qu·ªëc gia c·ªßa t√¥i l√† ng∆∞·ªùi gi·ªèi nh·∫•t tr√™n h√†nh t, v√† t√¥i l·ªõn l√™n h√°t m·ªôt b√†i h√°t t√™n l√† &quot; kh√¥ng c√≥ g√¨ ƒë·ªÉ envy . &quot;\n",
      "BLEU: 0.11390778025531027\n",
      "\n",
      "EN: And I was very proud .\n",
      "GT: T√¥i ƒë√£ r·∫•t t·ª± h√†o v·ªÅ ƒë·∫•t n∆∞·ªõc t√¥i .\n",
      "PR: v√† t√¥i r·∫•t t·ª± h√†o .\n",
      "BLEU: 0.11786767588753087\n",
      "\n",
      "EN: In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .\n",
      "GT: ·ªû tr∆∞·ªùng , ch√∫ng t√¥i d√†nh r·∫•t nhi·ªÅu th·ªùi gian ƒë·ªÉ h·ªçc v·ªÅ cu·ªôc ƒë·ªùi c·ªßa ch·ªß t·ªãch Kim II- Sung , nh∆∞ng l·∫°i kh√¥ng h·ªçc nhi·ªÅu v·ªÅ th·∫ø gi·ªõi b√™n ngo√†i , ngo·∫°i tr·ª´ vi·ªác Hoa K·ª≥ , H√†n Qu·ªëc v√† Nh·∫≠t B·∫£n l√† k·∫ª th√π c·ªßa ch√∫ng t√¥i .\n",
      "PR: ·ªü tr∆∞·ªùng h·ªçc , ch√∫ng t√¥i ƒë√£ d√†nh r·∫•t nhi·ªÅu th·ªùi gian nghi√™n c·ª©u l·ªãch s·ª≠ c·ªßa kilm kilem , nh∆∞ng ch√∫ng t√¥i ch∆∞a bao gi·ªù h·ªçc ƒë∆∞·ª£c nhi·ªÅu v·ªÅ th·∫ø gi·ªõi b√™n ngo√†i , ngo·∫°i tr·ª´ m·ªπ , nam h√†n , nh·∫≠t b·∫£n l√† k·∫ª th√π .\n",
      "BLEU: 0.3049267204801421\n",
      "\n",
      "EN: Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .\n",
      "GT: M·∫∑c d√π t√¥i ƒë√£ t·ª´ng t·ª± h·ªèi kh√¥ng bi·∫øt th·∫ø gi·ªõi b√™n ngo√†i kia nh∆∞ th·∫ø n√†o , nh∆∞ng t√¥i v·∫´n nghƒ© r·∫±ng m√¨nh s·∫Ω s·ªëng c·∫£ cu·ªôc ƒë·ªùi ·ªü B·∫ØcTri·ªÅu Ti√™n , cho t·ªõi khi t·∫•t c·∫£ m·ªçi th·ª© ƒë·ªôt nhi√™n thay ƒë·ªïi .\n",
      "PR: m·∫∑c d√π t√¥i th∆∞·ªùng t·ª± h·ªèi v·ªÅ th·∫ø gi·ªõi b√™n ngo√†i , t√¥i nghƒ© r·∫±ng t√¥i s·∫Ω d√†nh c·∫£ cu·ªôc ƒë·ªùi m√¨nh ·ªü b·∫Øc trt, cho ƒë·∫øn khi m·ªçi th·ª© ƒë√£ thay ƒë·ªïi .\n",
      "BLEU: 0.12956071485670367\n",
      "\n",
      "EN: When I was seven years old , I saw my first public execution , but I thought my life in North Korea was normal .\n",
      "GT: Khi t√¥i l√™n 7 , t√¥i ch·ª©ng ki·∫øn c·∫£nh ng∆∞·ªùi ta x·ª≠ b·∫Øn c√¥ng khai l·∫ßn ƒë·∫ßu ti√™n trong ƒë·ªùi , nh∆∞ng t√¥i v·∫´n nghƒ© cu·ªôc s·ªëng c·ªßa m√¨nh ·ªü ƒë√¢y l√† ho√†n to√†n b√¨nh th∆∞·ªùng .\n",
      "PR: khi t√¥i 7 tu·ªïi , t√¥i th·∫•y v·ªã tr√≠ c√¥ng ch√∫ng ƒë·∫ßu tc·ªßa m√¨nh , nh∆∞ng t√¥i nghƒ© cu·ªôc s·ªëng ·ªü b·∫Øc h√†n qu·ªëc l√† b√¨nh th∆∞·ªùng .\n",
      "BLEU: 0.06790777593753387\n",
      "\n",
      "EN: My family was not poor , and myself , I had never experienced hunger .\n",
      "GT: Gia ƒë√¨nh c·ªßa t√¥i kh√¥ng ngh√®o , v√† b·∫£n th√¢n t√¥i th√¨ ch∆∞a t·ª´ng ph·∫£i ch·ªãu ƒë√≥i .\n",
      "PR: gia ƒë√¨nh t√¥i kh√¥ng ngh√®o , v√† t√¥i ch∆∞a bao gi·ªù tr·∫£i nghi·ªám ƒë√≥i .\n",
      "BLEU: 0.2532709877036079\n",
      "\n",
      "EN: But one day , in 1995 , my mom brought home a letter from a coworker &apos;s sister .\n",
      "GT: Nh∆∞ng v√†o m·ªôt ng√†y c·ªßa nƒÉm 1995 , m·∫π t√¥i mang v·ªÅ nh√† m·ªôt l√° th∆∞ t·ª´ m·ªôt ng∆∞·ªùi ch·ªã em c√πng ch·ªó l√†m v·ªõi m·∫π .\n",
      "PR: nh∆∞ng m·ªôt ng√†y , v√†o nƒÉm 1995 , m·∫π t√¥i mang v·ªÅ nh√† t·ª´ m·ªôt ng∆∞·ªùi ch·ªã em g√°i .\n",
      "BLEU: 0.4063154670705585\n",
      "\n",
      "EN: It read , &quot; When you read this , all five family members will not exist in this world , because we haven &apos;t eaten for the past two weeks .\n",
      "GT: Trong ƒë√≥ c√≥ vi·∫øt : Khi ch·ªã ƒë·ªçc ƒë∆∞·ª£c nh·ªØng d√≤ng n√†y th√¨ c·∫£ gia ƒë√¨nh 5 ng∆∞·ªùi c·ªßa em ƒë√£ kh√¥ng c√≤n tr√™n c√µi ƒë·ªùi n√†y n·ªØa , b·ªüi v√¨ c·∫£ nh√† em ƒë√£ kh√¥ng c√≥ g√¨ ƒë·ªÉ ƒÉn trong hai tu·∫ßn .\n",
      "PR: n√≥ ƒë·ªçc , &quot; khi b·∫°n ƒë·ªçc n√≥ , t·∫•t c·∫£ 5 th√†nh vi√™n trong th·∫ø gi·ªõi n√†y , b·ªüi v√¨ ch√∫ng t√¥i ch∆∞a ƒÉn trong v√≤ng 2 tu·∫ßn qua .\n",
      "BLEU: 0.03006043411427879\n",
      "\n",
      "EN: We are lying on the floor together , and our bodies are so weak we are ready to die . &quot;\n",
      "GT: T·∫•t c·∫£ c√πng n·∫±m tr√™n s√†n , v√† c∆° th·ªÉ ch√∫ng t√¥i y·∫øu ƒë·∫øn c√≥ th·ªÉ c·∫£m th·∫•y nh∆∞ c√°i ch·∫øt ƒëang ƒë·∫øn r·∫•t g·∫ßn .\n",
      "PR: ch√∫ng ta ƒëang n·∫±m tr√™n s√†n nh√† , v√† c∆° th·ªÉ ch√∫ng ta ƒë√£ s·∫µn s√†ng ch·∫øt . &quot;\n",
      "BLEU: 0.18989192536146585\n",
      "\n",
      "EN: I was so shocked .\n",
      "GT: T√¥i ƒë√£ b·ªã s·ªëc .\n",
      "PR: t√¥i ƒë√£ r·∫•t s·ªëc .\n",
      "BLEU: 0.12574334296829354\n",
      "\n",
      "AVERAGE BLEU = 0.1374168123477097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = TransformerModel(\n",
    "    src_vocab=len(tok_src.word2idx),\n",
    "    trg_vocab=len(tok_trg.word2idx),\n",
    "    d_model=256,\n",
    "    nhead=4,\n",
    "    num_layers=3,\n",
    "    pad_idx=0\n",
    ")\n",
    "\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Train model with 20 epochs, warmup, and early stopping\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*50)\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, device, \n",
    "    epochs=20,           # Increased to 20 epochs\n",
    "    lr=3e-4,\n",
    "    patience=5,          # Early stopping patience\n",
    "    warmup_epochs=2      # Warmup for first 2 epochs\n",
    ")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "evaluate_test_set(model, test_en, test_vi, tok_src, tok_trg, device, n=50)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8761788,
     "sourceId": 13767258,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 505429,
     "modelInstanceId": 489996,
     "sourceId": 649510,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10003.651204,
   "end_time": "2025-12-14T20:14:45.789086",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-14T17:28:02.137882",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
