{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5464a98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T17:37:10.666470Z",
     "iopub.status.busy": "2025-11-17T17:37:10.666098Z",
     "iopub.status.idle": "2025-11-17T17:37:24.007688Z",
     "shell.execute_reply": "2025-11-17T17:37:24.006938Z"
    },
    "papermill": {
     "duration": 13.347632,
     "end_time": "2025-11-17T17:37:24.009391",
     "exception": false,
     "start_time": "2025-11-17T17:37:10.661759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import math\n",
    "import time\n",
    "from collections import Counter\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# 1. Tokenizer & Vocab\n",
    "# -------------------------\n",
    "class Tokenizer:\n",
    "    def __init__(self, texts, min_freq=2):\n",
    "        self.word2idx = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n",
    "        self.idx2word = {0:\"<pad>\", 1:\"<sos>\", 2:\"<eos>\", 3:\"<unk>\"}\n",
    "        self.build_vocab(texts, min_freq)\n",
    "    \n",
    "    def build_vocab(self, texts, min_freq):\n",
    "        counter = Counter()\n",
    "        for line in texts:\n",
    "            tokens = line.strip().lower().split()\n",
    "            counter.update(tokens)\n",
    "        for word, freq in counter.items():\n",
    "            if freq >= min_freq and word not in self.word2idx:\n",
    "                idx = len(self.word2idx)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = text.strip().lower().split()\n",
    "        return [self.word2idx.get(tok, self.word2idx[\"<unk>\"]) for tok in tokens]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return \" \".join([self.idx2word.get(i, \"<unk>\") for i in ids])\n",
    "\n",
    "# -------------------------\n",
    "# 2. Dataset\n",
    "# -------------------------\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_texts, trg_texts, src_tokenizer, trg_tokenizer):\n",
    "        self.src_texts = src_texts\n",
    "        self.trg_texts = trg_texts\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src = [self.src_tokenizer.word2idx[\"<sos>\"]] + self.src_tokenizer.encode(self.src_texts[idx]) + [self.src_tokenizer.word2idx[\"<eos>\"]]\n",
    "        trg = [self.trg_tokenizer.word2idx[\"<sos>\"]] + self.trg_tokenizer.encode(self.trg_texts[idx]) + [self.trg_tokenizer.word2idx[\"<eos>\"]]\n",
    "        return torch.tensor(src, dtype=torch.long), torch.tensor(trg, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch, pad_idx=0):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=pad_idx)\n",
    "    trg_batch = nn.utils.rnn.pad_sequence(trg_batch, batch_first=True, padding_value=pad_idx)\n",
    "    return src_batch, trg_batch\n",
    "\n",
    "# -------------------------\n",
    "# 3. Positional Encoding\n",
    "# -------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Transformer Model\n",
    "# -------------------------\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, d_model=256, nhead=4, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.src_embed = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.trg_embed = nn.Embedding(trg_vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers,\n",
    "                                          dim_feedforward=dim_feedforward,\n",
    "                                          dropout=dropout,\n",
    "                                          batch_first=True)\n",
    "        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        src_emb = self.pos_enc(self.src_embed(src))\n",
    "        trg_emb = self.pos_enc(self.trg_embed(trg))\n",
    "        out = self.transformer(src_emb, trg_emb, \n",
    "                               src_key_padding_mask=(src==0), \n",
    "                               tgt_key_padding_mask=(trg==0), \n",
    "                               memory_key_padding_mask=(src==0))\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Train function\n",
    "# -------------------------\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, pad_idx, device, epochs=10, save_path=\"best_model.pt\"):\n",
    "    model.to(device)\n",
    "    scaler = torch.amp.GradScaler(device=device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "        for src, trg in train_loader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast(device_type=device.type):\n",
    "                output = model(src, trg[:, :-1])\n",
    "                loss = criterion(output.reshape(-1, output.size(-1)), trg[:, 1:].reshape(-1))\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for src, trg in val_loader:\n",
    "                src, trg = src.to(device), trg.to(device)\n",
    "                output = model(src, trg[:, :-1])\n",
    "                loss = criterion(output.reshape(-1, output.size(-1)), trg[:, 1:].reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {time.time()-start_time:.2f}s\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"Model saved!\")\n",
    "\n",
    "# -------------------------\n",
    "# 6. Translate single sentence\n",
    "# -------------------------\n",
    "def translate_sentence(model, src_sentence, src_tokenizer, trg_tokenizer, device, max_len=50):\n",
    "    model.eval()\n",
    "    src_ids = [src_tokenizer.word2idx[\"<sos>\"]] + src_tokenizer.encode(src_sentence) + [src_tokenizer.word2idx[\"<eos>\"]]\n",
    "    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    trg_ids = [trg_tokenizer.word2idx[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.tensor(trg_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, trg_tensor)\n",
    "        next_token = output[0, -1].argmax().item()\n",
    "        trg_ids.append(next_token)\n",
    "        if next_token == trg_tokenizer.word2idx[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    return trg_tokenizer.decode(trg_ids[1:-1])\n",
    "\n",
    "# -------------------------\n",
    "# 7. Evaluate on test set\n",
    "# -------------------------\n",
    "def evaluate_test_set(model, test_src, test_trg, src_tokenizer, trg_tokenizer, device, max_len=50, num_samples=None):\n",
    "    model.eval()\n",
    "    total_bleu = 0\n",
    "    n = len(test_src) if num_samples is None else min(len(test_src), num_samples)\n",
    "    for src_sentence, trg_sentence in zip(test_src[:n], test_trg[:n]):\n",
    "        pred_sentence = translate_sentence(model, src_sentence, src_tokenizer, trg_tokenizer, device, max_len)\n",
    "        bleu = sentence_bleu([trg_sentence.strip().lower().split()], pred_sentence.strip().lower().split())\n",
    "        total_bleu += bleu\n",
    "        print(f\"Source: {src_sentence.strip()}\")\n",
    "        print(f\"Target: {trg_sentence.strip()}\")\n",
    "        print(f\"Predicted: {pred_sentence}\")\n",
    "        print(f\"BLEU: {bleu:.4f}\\n\")\n",
    "    avg_bleu = total_bleu / n\n",
    "    print(f\"Average BLEU score on test set: {avg_bleu:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 8. Load data & prepare loaders\n",
    "# -------------------------\n",
    "with open(\"/kaggle/input/en-vi-ds/data/train.en\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_en = f.readlines()\n",
    "with open(\"/kaggle/input/en-vi-ds/data/train.vi\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_vi = f.readlines()\n",
    "with open(\"/kaggle/input/en-vi-ds/data/tst2013.en\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_en = f.readlines()\n",
    "with open(\"/kaggle/input/en-vi-ds/data/tst2013.vi\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_vi = f.readlines()\n",
    "\n",
    "src_tokenizer = Tokenizer(train_en)\n",
    "trg_tokenizer = Tokenizer(train_vi)\n",
    "\n",
    "dataset = TranslationDataset(train_en, train_vi, src_tokenizer, trg_tokenizer)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda b: collate_fn(b, pad_idx=0))\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=lambda b: collate_fn(b, pad_idx=0))\n",
    "\n",
    "# -------------------------\n",
    "# 9. Train model\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(len(src_tokenizer.word2idx), len(trg_tokenizer.word2idx))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adb3b5a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T17:37:24.014920Z",
     "iopub.status.busy": "2025-11-17T17:37:24.014089Z",
     "iopub.status.idle": "2025-11-17T18:17:38.363428Z",
     "shell.execute_reply": "2025-11-17T18:17:38.362371Z"
    },
    "papermill": {
     "duration": 2414.355597,
     "end_time": "2025-11-17T18:17:38.367233",
     "exception": false,
     "start_time": "2025-11-17T17:37:24.011636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | Train Loss: 2.5535 | Val Loss: 0.7299 | Time: 202.64s\n",
      "Model saved!\n",
      "Epoch 2/12 | Train Loss: 0.5657 | Val Loss: 0.3120 | Time: 202.60s\n",
      "Model saved!\n",
      "Epoch 3/12 | Train Loss: 0.2891 | Val Loss: 0.1931 | Time: 202.62s\n",
      "Model saved!\n",
      "Epoch 4/12 | Train Loss: 0.1778 | Val Loss: 0.1366 | Time: 203.16s\n",
      "Model saved!\n",
      "Epoch 5/12 | Train Loss: 0.1177 | Val Loss: 0.1083 | Time: 201.94s\n",
      "Model saved!\n",
      "Epoch 6/12 | Train Loss: 0.0816 | Val Loss: 0.0905 | Time: 199.86s\n",
      "Model saved!\n",
      "Epoch 7/12 | Train Loss: 0.0593 | Val Loss: 0.0794 | Time: 197.88s\n",
      "Model saved!\n",
      "Epoch 8/12 | Train Loss: 0.0442 | Val Loss: 0.0692 | Time: 198.72s\n",
      "Model saved!\n",
      "Epoch 9/12 | Train Loss: 0.0343 | Val Loss: 0.0637 | Time: 199.60s\n",
      "Model saved!\n",
      "Epoch 10/12 | Train Loss: 0.0270 | Val Loss: 0.0598 | Time: 199.36s\n",
      "Model saved!\n",
      "Epoch 11/12 | Train Loss: 0.0221 | Val Loss: 0.0548 | Time: 201.27s\n",
      "Model saved!\n",
      "Epoch 12/12 | Train Loss: 0.0178 | Val Loss: 0.0524 | Time: 202.66s\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, pad_idx=0, device=device, epochs=12, save_path=\"best_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a328ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:17:38.373590Z",
     "iopub.status.busy": "2025-11-17T18:17:38.373307Z",
     "iopub.status.idle": "2025-11-17T18:17:38.807131Z",
     "shell.execute_reply": "2025-11-17T18:17:38.806023Z"
    },
    "papermill": {
     "duration": 0.439161,
     "end_time": "2025-11-17T18:17:38.808843",
     "exception": false,
     "start_time": "2025-11-17T18:17:38.369682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy . &quot;\n",
      "Target: Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài &quot; Chúng ta chẳng có gì phải ghen tị . &quot;\n",
      "Predicted: 7.200\n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: And I was very proud .\n",
      "Target: Tôi đã rất tự hào về đất nước tôi .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .\n",
      "Target: Ở trường , chúng tôi dành rất nhiều thời gian để học về cuộc đời của chủ tịch Kim II- Sung , nhưng lại không học nhiều về thế giới bên ngoài , ngoại trừ việc Hoa Kỳ , Hàn Quốc và Nhật Bản là kẻ thù của chúng tôi .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .\n",
      "Target: Mặc dù tôi đã từng tự hỏi không biết thế giới bên ngoài kia như thế nào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộc đời ở BắcTriều Tiên , cho tới khi tất cả mọi thứ đột nhiên thay đổi .\n",
      "Predicted: 7.200\n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: When I was seven years old , I saw my first public execution , but I thought my life in North Korea was normal .\n",
      "Target: Khi tôi lên 7 , tôi chứng kiến cảnh người ta xử bắn công khai lần đầu tiên trong đời , nhưng tôi vẫn nghĩ cuộc sống của mình ở đây là hoàn toàn bình thường .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: My family was not poor , and myself , I had never experienced hunger .\n",
      "Target: Gia đình của tôi không nghèo , và bản thân tôi thì chưa từng phải chịu đói .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: But one day , in 1995 , my mom brought home a letter from a coworker &apos;s sister .\n",
      "Target: Nhưng vào một ngày của năm 1995 , mẹ tôi mang về nhà một lá thư từ một người chị em cùng chỗ làm với mẹ .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: It read , &quot; When you read this , all five family members will not exist in this world , because we haven &apos;t eaten for the past two weeks .\n",
      "Target: Trong đó có viết : Khi chị đọc được những dòng này thì cả gia đình 5 người của em đã không còn trên cõi đời này nữa , bởi vì cả nhà em đã không có gì để ăn trong hai tuần .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: We are lying on the floor together , and our bodies are so weak we are ready to die . &quot;\n",
      "Target: Tất cả cùng nằm trên sàn , và cơ thể chúng tôi yếu đến có thể cảm thấy như cái chết đang đến rất gần .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: I was so shocked .\n",
      "Target: Tôi đã bị sốc .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: This was the first time I heard that people in my country were suffering .\n",
      "Target: Vì đó là lần đầu tiên tôi biết rằng đồng bào của tôi đang phải chịu đựng như vậy .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: Soon after , when I was walking past a train station , I saw something terrible that I can &apos;t erase from my memory .\n",
      "Target: Không lâu sau đó , khi tôi đi qua một nhà ga , tôi nhìn thấy một cảnh tượng kinh hoàng mà tôi không bao giờ có thể quên\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: A lifeless woman was lying on the ground , while an emaciated child in her arms just stared helplessly at his mother &apos;s face .\n",
      "Target: Trên nền nhà ga là xác chết của một người đàn bà hai tay vẫn đang ôm một đứa bé hốc hác và đứa bé chỉ biết nhìn chằm chằm vào khuôn mặt của mẹ nó .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: But nobody helped them , because they were so focused on taking care of themselves and their families .\n",
      "Target: Nhưng không có ai giúp họ , bởi vì tất cả đều đang phải lo cho chính mình và cả gia đình .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: A huge famine hit North Korea in the mid-1990s .\n",
      "Target: Vào giữa những năm 90 , Bắc Triều Tiên trải qua một nạn đói trầm trọng .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: Ultimately , more than a million North Koreans died during the famine , and many only survived by eating grass , bugs and tree bark .\n",
      "Target: Nó khiến hơn một triệu người Triều Tiên chết trong nạn đói , và nhiều người chỉ sống sót phải ăn cỏ , sâu bọ và vỏ cây .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: Power outages also became more and more frequent , so everything around me was completely dark at night except for the sea of lights in China , just across the river from my home .\n",
      "Target: Việc cúp điện ngày càng xảy ra thường xuyên , vì thế mọi thứ xung quanh tôi đều chìm vào bóng tối khi đêm đến ngoại trừ ánh sáng đèn từ phía Trung Quốc chỉ cách nhà tôi một con sông .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: I always wondered why they had lights but we didn &apos;t .\n",
      "Target: Tôi lúc nào cũng tự hỏi là tại sao họ lại có điện còn chúng tôi thì không .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: This is a satellite picture showing North Korea at night compared to neighbors .\n",
      "Target: Đây là một bức ảnh từ vệ tinh chụp Bắc Triều Tiên vào ban đêm trong tương quan với các nước xung quanh .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: This is the Amrok River , which serves as a part of the border between North Korea and China .\n",
      "Target: Đây là sông Áp Lục nó là biên giới tự nhiên giữa Bắc Triều Tiên và Trung Quốc .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: As you can see , the river can be very narrow at certain points , allowing North Koreans to secretly cross .\n",
      "Target: Có thể thấy là lòng sông có đoạn rất hẹp vì thế một số người Bắc Triều Tiên bí mật vượt sang Trung Quốc .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: But many die .\n",
      "Target: Nhưng rất nhiều người đã chết .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: Sometimes , I saw dead bodies floating down the river .\n",
      "Target: Và tôi đã nhìn thấy xác họ nổi trên sông .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: I can &apos;t reveal many details &#91; about &#93; how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .\n",
      "Target: Tôi không thể nói cụ thể về việc mình đã trốn khỏi Bắc Triều Tiên như thế nào chỉ có thể nói rằng trong những năm tháng khốn khó vì nạn đói ấy tôi được gửi sang Trung Quốc để sống với một người họ hàng xa .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: But I only thought that I would be separated from my family for a short time .\n",
      "Target: Lúc đó , tôi chỉ nghĩ rằng mình sẽ phải xa gia đình một thời gian ngắn .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: I could have never imagined that it would take 14 years to live together .\n",
      "Target: chứ không bao giờ tôi có thể tưởng tượng rằng tôi sẽ phải xa họ những 14 năm ròng .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: In China , it was hard living as a young girl without my family .\n",
      "Target: Ở Trung Quốc , cuộc sống của một cô bé bị cách ly khỏi gia đình như tôi rất khó khăn .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: I had no idea what life was going to be like as a North Korean refugee , but I soon learned it &apos;s not only extremely difficult , it &apos;s also very dangerous , since North Korean refugees are considered in China as illegal migrants .\n",
      "Target: Tôi đã không tưởng được những gì xảy đến với cuộc sống của một người tị nạn từ Bắc Triều Tiên thì sẽ như thế nào , nhưng tôi sớm nhận ra rằng nó không những rất khó khăn , mà còn vô cùng nguy hiểm , vì những người tị nạn từ Bắc Triều Tiên vào Trung Quốc đều bị coi là dân nhập cư trái phép .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: So I was living in constant fear that my identity could be revealed , and I would be repatriated to a horrible fate back in North Korea .\n",
      "Target: Tôi luôn sống trong một nỗi sợ thường trực rằng danh tính của tôi sẽ bị phát hiện , và tôi sẽ bị trả về với cuộc sống cũ ở Bắc Triều Tiên .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: One day , my worst nightmare came true , when I was caught by the Chinese police and brought to the police station for interrogation .\n",
      "Target: Một ngày , cơn ác mộng đó đã thành sự thật , tôi đã bị cảnh sát Trung Quốc bắt và đưa đến đồn cảnh sát để chất vấn .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: Someone had accused me of being North Korean , so they tested my Chinese language abilities and asked me tons of questions .\n",
      "Target: Có ai đó đã báo với họ rằng tôi là người Bắc Triều Tiên , vì thế họ đã kiểm tra khả năng tiếng Trung của tôi và hỏi tôi rất nhiều câu hỏi .\n",
      "Predicted: 7.200\n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: I was so scared , I thought my heart was going to explode .\n",
      "Target: Tôi đã vô cùng sợ hãi , và có cảm giác như tim mình sắp nổ tung .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: If anything seemed unnatural , I could be imprisoned and repatriated .\n",
      "Target: Vì nếu như họ thấy có điều gì không tự nhiên , tôi sẽ bị tống vào tù và rồi bị trả về nước .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: I thought my life was over , but I managed to control all the emotions inside me and answer the questions .\n",
      "Target: Tôi nghĩ cuộc đời mình đến đây là chấm dứt , nhưng tôi vẫn cố gắng điều khiển những cảm xúc của mình và trả lời những câu hỏi của họ .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: After they finished questioning me , one official said to another , &quot; This was a false report .\n",
      "Target: Sau khi hỏi xong , một trong hai cảnh sát nói với người kia , Đây là một vụ chỉ điểm sai .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: She &apos;s not North Korean . &quot;\n",
      "Target: Nó không phải là người Bắc Triều Tiên . &quot;\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: And they let me go . It was a miracle .\n",
      "Target: Và họ thả tôi ra . Đó quả là một phép màu .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: Some North Koreans in China seek asylum in foreign embassies , but many can be caught by the Chinese police and repatriated .\n",
      "Target: Một số người Bắc Triều Tiên ở Trung Quốc đã đến những đại sứ quán của nước ngoài để xin tị nạn , nhưng rất nhiều trong số đó đã bị bắt bởi cảnh sát Trung Quốc và bị trả về nước .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: These girls were so lucky .\n",
      "Target: những cô gái này đã rất may mắn .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: Even though they were caught , they were eventually released after heavy international pressure .\n",
      "Target: vì mặc dù đã bị bắt , nhưng cuối cùng học cũng được thả ra nhờ vào sức ép từ cộng đồng quốc tế .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: These North Koreans were not so lucky .\n",
      "Target: Nhưng những người Bắc Triều Tiên này thì không được may mắn như vậy .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: Every year , countless North Koreans are caught in China and repatriated to North Korea , where they can be tortured , imprisoned or publicly executed .\n",
      "Target: Hàng năm , có vô số người Bắc Triều Tiên bị bắt ở Trung Quốc và bị trả về nước , nơi mà họ bị tra tấn , bị giam cầm hoặc bị xử tử công khai .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: Even though I was really fortunate to get out , many other North Koreans have not been so lucky .\n",
      "Target: Trong khi tôi rất may mắn vì đã được thả ra thì rất nhiều đồng bào của tôi lại không được như vậy .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: It &apos;s tragic that North Koreans have to hide their identities and struggle so hard just to survive .\n",
      "Target: Việc người Bắc Triều Tiên phải che dấu danh tính của mình và đấu tranh để tồn tại quả là một bi kịch .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: Even after learning a new language and getting a job , their whole world can be turned upside down in an instant .\n",
      "Target: Kể cả khi đã học tiếng Trung và tìm được một công việc , thì cuộc sống của họ cũng có thể bị đảo lộn hoàn toàn chỉ trong một khoảng khắc .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: That &apos;s why , after 10 years of hiding my identity , I decided to risk going to South Korea , and I started a new life yet again .\n",
      "Target: Đó là lý do tại sao sau 10 năm che dấu danh tính thật tôi quyết định liều mình đi đến Hàn Quốc , để bắt đầu một cuộc sống mới một lần nữa .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: Settling down in South Korea was a lot more challenging than I had expected .\n",
      "Target: Việc ổn định cuộc sống ở đây khó khăn hơn nhiều so với tôi tưởng tượng .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: English was so important in South Korea , so I had to start learning my third language .\n",
      "Target: Vì ở Hàn Quốc , tiếng Anh có vị trí vô cùng quan trọng , nên tôi đã bắt đầu học tiếng Anh , ngôn ngữ thứ ba của tôi .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: Also , I realized there was a wide gap between North and South .\n",
      "Target: Tôi cũng nhận ra một khoảng cách rất lớn giữa người Nam và Bắc Triều Tiên .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Source: We are all Korean , but inside , we have become very different due to 67 years of division .\n",
      "Target: Chúng tôi đều là người Triều Tiên , nhưng đã trở nên rất khác nhau do hậu quả của 67 năm bị chia cắt .\n",
      "Predicted: \n",
      "BLEU: 0.0000\n",
      "\n",
      "Average BLEU score on test set: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 10. Load best model & evaluate on test\n",
    "# -------------------------\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "evaluate_test_set(model, test_en, test_vi, src_tokenizer, trg_tokenizer, device, num_samples=50)  # đánh giá 50 câu đầu để nhanh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8829d3f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:17:38.816078Z",
     "iopub.status.busy": "2025-11-17T18:17:38.815440Z",
     "iopub.status.idle": "2025-11-17T18:17:38.826283Z",
     "shell.execute_reply": "2025-11-17T18:17:38.825588Z"
    },
    "papermill": {
     "duration": 0.015698,
     "end_time": "2025-11-17T18:17:38.827611",
     "exception": false,
     "start_time": "2025-11-17T18:17:38.811913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I love machine learning.\"\n",
    "pred = translate_sentence(model, sentence, src_tokenizer, trg_tokenizer, device)\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3a4788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T18:17:38.834476Z",
     "iopub.status.busy": "2025-11-17T18:17:38.834194Z",
     "iopub.status.idle": "2025-11-17T18:17:38.861055Z",
     "shell.execute_reply": "2025-11-17T18:17:38.859968Z"
    },
    "papermill": {
     "duration": 0.031958,
     "end_time": "2025-11-17T18:17:38.862494",
     "exception": false,
     "start_time": "2025-11-17T18:17:38.830536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I love programming in Python.\n",
      "Predicted translation: \n",
      "\n",
      "Input: The weather today is really nice.\n",
      "Predicted translation: \n",
      "\n",
      "Input: Machine learning is amazing.\n",
      "Predicted translation: 7.200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 11. Kiểm tra với câu mới\n",
    "# -------------------------\n",
    "example_sentences = [\n",
    "    \"I love programming in Python.\",\n",
    "    \"The weather today is really nice.\",\n",
    "    \"Machine learning is amazing.\"\n",
    "]\n",
    "\n",
    "for sent in example_sentences:\n",
    "    translation = translate_sentence(model, sent, src_tokenizer, trg_tokenizer, device)\n",
    "    print(f\"Input: {sent}\")\n",
    "    print(f\"Predicted translation: {translation}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8761788,
     "sourceId": 13767258,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2433.907239,
   "end_time": "2025-11-17T18:17:40.687818",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-17T17:37:06.780579",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
